---
title: "Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach"
author: "Ifeanyi Edochie"
format: html
editor: visual
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false
#| error: false

# Chunk setup
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

pacman::p_load(
  sf, 
  data.table, 
  tidyverse, 
  car, 
  msae,
  sae, 
  survey, 
  spdep,
  knitr, 
  MASS, 
  caret,
  purrr,
  pins,
  gt,
  lmtest, 
  scales,
  viridis, 
  patchwork,
  dplyr, 
  tictoc,
  gt,
  matrixcalc
  )

# Loading locally-developed
list.files("R", pattern = "*.R$", full.names = TRUE, ignore.case = TRUE) |>
  walk(~ suppressMessages(source(.x)))

# Raw data root
root_raw <- "./data/raw"
root_temp <- "./data/temp"
root_clean <- "./data/clean-example"

# Data-storage boards
bd_raw <- root_raw |> file.path("api") |> board_folder(versioned = T)
bd_aux <- root_temp |> board_folder(versioned = T)
bd_clean <- root_clean |> board_folder(versioned = T)
bd_out <- "output/res-50-mfh" |> board_folder(versioned = T)
```

# Stable FH Estimators over T time periods: The Multivariate Fay Herriot Modelling Approach

This section describes procedures that yield stable small area estimators for each of $D$ areas over $T$ subsequent time instants. Area populations, the samples and the data might change between time periods. Accordingly, we denote $U_t$ the overall population at time $t$, which is partitioned into $D$ areas $U_{1t}, ... ,U_{Dt}$, of respective population sizes $N_{1t}$

An overview of the MFH model estimation process is as follows:

-   Step 0: The data preparation phase in which we prepare the 3 data objects needed for the small area estimation under the Multivariate Fay Herriot Model

-   Step 1: Compute the selected direct area estimators for each target area $d = 1, ..., D$ for each time $t = 1, ..., T$ and estimators of their corresponding sampling variances and covariances.

-   Step 2: Prepare variables (from administrative data or other sources of data representative at the target area level) at the level of the target area for each time instant in the MFH model. We present a simple approach which performs model selection in a pooled linear regression model without time effects.

-   Step 3: Fit the MFH models to test for homoskedastic area-time effects $({\mu_d}_1, ... , {\mu_d}_T)$ are homoskedastic or not. If we reject the homoskedasticity of variances, implement the MFH3 model. Otherwise, we proceed with the MFH2 model. For the purposes of this training, we assume a homoskedastic model for simplicity.

-   Step 4: Check the selected model assumptions, including linearity, normality of predicted area effects and standardized model residuals, and the presence of the outlying areas.

-   Step 5: In case of systematic model departures such as isolated departures because of outlying areas, some adjustments might need to be implemented before returning to Step 2 to recompute the MFH model.

-   Step 6: If model assumptions hold, using the above direct estimates and estimated sampling variances and covariances, and the selected auxiliary variables, compute MFH estimators $\hat{\delta}_{dt}^{MFH},\quad d = 1,...,D$ and $t = 1, ..., T$ and their corresponding estimated MSEs.

We will show below the use of the `eblupMFH2()` and `eblupMFH3()` from the R package msae [@permatasari2022package] compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are:

`eblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

`eblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

## MFH Estimation of Poverty Rates for T time periods

In this example, we use a synthetic data set adapted from R package `sae` called `incomedata`. The original data contains information for $n = 17,119$ fictitious individuals residing across $D = 52$ Spanish provinces. The variables include the name of the province of residence (`provlab`), province code (`prov`), as well as several correlates of income. We have added two additional income vectors corresponding to two additional years of data.

We will show how to estimate a poverty map for each year by using the Multivariate Fay Herriot modelling approach. This approach allows us to take advantage of the temporal correlation between poverty rates i.e. an individuals income in year $t$ is likely correlated with their income in year $t+1$.

The rest of this tutorial shows how to prepare MFH models using a random 10% sample of the `incomedata` to estimate the poverty rates.

## Step 0: The Data Preparation Phase

The MFH estimation process relies on 3 types of data:

(1) The data containing the outcome variable from which direct estimates will be computed. This is a usually a survey dataset for each year of data including outcome variable (such as income or welfare aggregates), weights, cluster identifiers (i.e. if available, psu or enumeration areas) at the unit level i.e. individual or household. In this example, we call this: `survey_dt`

(2) A dataset containing for the right hand side (RHS) variables i.e. indicator estimates representative at the level of the target area for each year. This is often obtained from administrative data sources or geospatial data such as remotely sensed high resolution data. The final RHS dataset ought to include an area id, year and variables of interest. In this example, we call this `rhs_dt`

(3) Finally, a shapefile which spatially links each target area to their boundaries on a map. This will contain the area id as well as the geometry object which when visualized will show the shape of the areas in which poverty rates will be estimated. In this example, we call this `shp_dt`

```{r dta-load}
#| echo: true
survey_dt <- bd_clean |> pin_read("pov_direct")
rhs_dt <- bd_clean |> pin_read("sae_data")
shp_dt <- bd_clean |> pin_read("geometries")
```

Below is what our datasets look like. These are generally the variables

```{r}
survey_dt |> glimpse()
```

`survey_dt` contains our target area identifiers (`provlab`, `prov`), the weight variable `weight`, the cluster id i.e. enumeration area or psu, `ea_id`, the year, `year`, the income variable `income` and the poverty line `poverty`. In this example, `survey_dt` of class `data.frame` is at the level of the individual. It could also be at the level of the household if poverty lines are evaluated at that level.

```{r}
rhs_dt |> glimpse()
```

\``rhs_dt` is an object of class `data.frame` created at the level of the target area. It should contain the same target area identifiers as in `survey_dt` and the year variable `year`.

```{r}
shp_dt |> glimpse()
```

`shp_dt` is an object of class `sf`, `data.frame` created at the level of the target area. This is the shapefile for the area of interest for which the poverty map will be estimated. It should contain the same target area ID found in `survey_dt` as well as `rhs_dt`.

A quick summary table on the data needs for the MFH model:

```{r}
# Create the table content
mfh_table <- tibble::tibble(
  Dataset = c("`survey_dt`", "`rhs_dt`", "`shp_dt`"),
  `Unit of Observation` = c(
    "Individual (or Household)",
    "Target Area (e.g. Province)",
    "Target Area (Spatial)"
  ),
  `Required Variables` = c(
    "`target area identifiers`, `weights`, `cluster identifier`, year, `income/welfare variable`, `poverty line`",
    "`target area identifiers`, `year`, `covariates` (e.g. `gen`, `educ1`, `schyrs`, etc.)",
    "`target area identifiers`, `geometries` (e.g. `geometry` column from `sf`)"
  )
)

# Build the gt table
mfh_table %>%
  gt() %>%
  tab_header(
    title = md("**Data Input Checklist for the Multivariate Fay-Herriot Model**"),
    subtitle = md("*Datasets, levels, and Required variables*")
  ) %>%
  cols_label(
    Dataset = "Dataset Name",
    `Unit of Observation` = "Unit of Observation",
    `Required Variables` = "Required Variables"
  ) %>%
  tab_options(
    table.font.names = "Arial",
    heading.title.font.size = 16,
    heading.subtitle.font.size = 12,
    table.font.size = 12,
    column_labels.font.weight = "bold",
    data_row.padding = px(4),
    table.border.top.width = px(2),
    table.border.bottom.width = px(2),
    heading.align = "left"
  ) %>%
  fmt_markdown(columns = everything())
```

For ease of use of this tutorial, make sure the variable names match as described across the datasets i.e. use the same target area variables and year variable name across all 3 datasets.

## Step 1: Direct Estimation of Poverty & Variance-Covarance Matrix

We will use the direct HT estimators that use the survey weights in `weight` variable. First, we calculate the total sample size, the number of provinces, the sample sizes for each province and extract the population sizes for each province/target area from the `sizeprov` file. For those using the household/individual level survey data, this may be obtained from the sum of the household or individual weights as appropriate.

### Simple Direct Estimation

The goal is simply to apply the poverty line to the

```{r dir-est-setup}
#| echo: true

### a little bit of housekeeping to ensure ease of access 
area_vars <- c("prov", "provlab") ### both variables are at the same level. if the levels vary, you would need to combine both variables for effect use

cluster_var <- "ea_id"
weight_var <- "weight"
year_var <- "year"
outcome_var <- "income"
povline_var <- "povline"

candidate_vars <- colnames(rhs_dt)[!colnames(rhs_dt) %in% c(area_vars, year_var)]

### quickly compute the sample size for each province
sampsize_dt <- survey_dt |>
  group_by(!!!syms(area_vars), !!sym(year_var)) |>
  summarize(N = n(), .groups = "drop")

## the poverty line for each is already included within the data. 
## Lets compute the direct estimates for each year of data and create a list of data.frames (equal in length to the number of years) 
## containing the direct estimate, the standard errors and the coefficient of variation. 

direct_dt <- 
  survey_dt |>
  mutate(pov_indicator = ifelse(income < povline, 1, 0)) |>
  group_by(!!!syms(area_vars), !!sym(year_var)) |>
  summarise(direct_povrate = weighted.mean(x = pov_indicator, 
                                           w = !!sym(weight_var),
                                           na.rm = TRUE),
            .groups = "drop")
  
```

Here is what the results look like:

```{r, echo = TRUE}
direct_dt
```

### Computing the Variance-Covariance Matrix for the Sampling Error of the Direct Estimate

Next, we quickly estimate the sample variance and covariance for the direct estimator using the survey R package as below. We apply the `compute_vcov()` function which we have created as a wrapper on the `survey::svymean()` and `survey::svyvar()` functions to estimate the variance covariance matrix.

```{r dir-est-pov}
#| echo: true
survey_dt <- 
  survey_dt |>
  mutate(pov_indicator = ifelse(!!sym(outcome_var) < !!sym(povline_var), 1, 0))

### we need to reconstruct the variables from long to wide for this
### this wide format will also be useful for estimating the MFH model as well 
### later on

widesurvey_dt <- 
  survey_dt |>
  pivot_wider(
    id_cols = c(!!!syms(area_vars), !!sym(cluster_var), !!sym(weight_var)),
    names_from = !!sym(year_var),
    values_from = c(!!sym(outcome_var), !!sym(povline_var), "pov_indicator"),
    names_glue = "{.value}{year}",
    values_fn = first
  )

#### now we are ready to compute the variance covariance matrix
### notice that we have to specify the new column names for the pov_indicator variables as they recreated in widesurvey_dt
var_dt <- 
  compute_vcov(dt = widesurvey_dt,
               domain = area_vars[[1]],
               ids = cluster_var,
               weights = weight_var,
               yvars = paste0("pov_indicator", unique(survey_dt[[year_var]]))) 

var_dt <- 
  var_dt |>
  rename(!!sym(area_vars[[1]]) := "domain") ### quick rename the domain variable to match our area_vars

### lets merge in our sample sizes
var_dt <-
  var_dt |>
  merge(sampsize_dt %>%
          dplyr::select(!!sym(area_vars[1]), "N") |>
          unique(), 
        by = area_vars[[1]])
```

It is noteworthy that the `var_dt` object is estimated at the level of the target area. Hence, for reasons that will be made self-evident later, the `var_dt` is not a matrix for each area rather the matrix is stored rowwise.

#### Handling Low Sample Sizes: Variance Smoothing

A quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by [@you2023application]. Please see the code and Roxygen comments below explaining the use of the `varsmoothie_king()` function which computes smoothed variances.

The goal now is to use the above `varsmoothie_king()` function to add additional columns of smoothed variances into our `var_dt` object.

```{r dir-est-var-smooth}
#| echo: true

varcols <- grep("^v_", names(var_dt), value = TRUE) ## the column names for the variance covariance matrix

var_dt <-
  lapply(X = varcols,
         FUN = function(x){

           z <- varsmoothie_king(domain = var_dt[[area_vars[1]]],
                                 direct_var = var_dt[[x]],
                                 sampsize = var_dt[["N"]]) |>
             as.data.table() |>
             setnames(old = "var_smooth", new = paste0("vs", x)) |>
             as_tibble()
           

           return(z)

         }) %>%
  Reduce(f = "merge",
         x = .) %>%
  merge(x = var_dt,
        y = .,
        by.x = area_vars[[1]],
        by.y = "Domain") |>
  as_tibble()

```

Now, you can replace the zero/near zero sample size area MSEs with their smoothed variances.

## Step 2: Variable Preparation and Model Selection

### Data Preparation for Model Selection & MFH estimation

Thus far, we have careful set up the types of data we require for the MFH model. One final step of variable preparation is necessary to use the `eblupUFH` and `eblupMFH` functions. This requires that we reshape the set of candidate variables dataset i.e. the `rhs_dt` object into wide format. We will also have to reshape our direct estimates and merge this into `rhs_dt` as well as `var_dt`. All the data we have created thus far needs to be placed together ultimately for the EBLUP estimation.

```{r}
#| echo: true

## reshaping the rhs_dt from long to wide
widerhs_dt <- 
  rhs_dt |>
  pivot_wider(
    id_cols = dplyr::all_of(area_vars),
    names_from = dplyr::all_of(year_var),
    values_from = dplyr::all_of(candidate_vars),
    names_glue = "{.value}{year}",
    values_fn = \(x) dplyr::first(x)  # or `first`, or something more explicit if needed
  )

## now let us reshape and merge in the poverty rates

mfh_dt <- 
  direct_dt |>
  pivot_wider(
    id_cols = dplyr::all_of(area_vars),
    names_from = dplyr::all_of(year_var),
    values_from = "direct_povrate",
    names_glue = "{.value}{year}",
    values_fn = \(x) dplyr::first(x)  # or `first`, or something more explicit if neede
  ) |>
  merge(y = widerhs_dt,
        by = area_vars[[1]]) |>
  merge(y = var_dt,
        by = area_vars[[1]])


```

Here is what the data looks like:

```{r}
#| echo: true
mfh_dt |> glimpse()
```

### Variable Selection Process

Next, we apply a simple variable selection process which employs the stepwise regression algorithm using the AIC selection criteria as in described by [@yamashita2007stepwise]. The function `step_wrapper()` implemented below is a wrapper to the `stepAIC()` function carries all the perfunctory cleaning necessary use the `stepAIC()` function. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations and remove perfectly or near collinear variables and combinations using the variance inflation method.

```{r var-selection}
#| echo: true
candidate_vars <- 
  expand.grid(var = candidate_vars,
              year = unique(survey_dt[[year_var]])) |>
  transform(name = paste0(var, year)) |>
  dplyr::select(name) |>
  unlist() |>
  unname()
  
  
## extract the year identifiers in each variance covariance name
varyear_list <- stringr::str_extract_all(varcols, "\\d{4}")

## Keep only those where both years are the same i.e. the variances
variance_cols <- varcols[lengths(varyear_list) == 2 & 
                           sapply(varyear_list, function(x) x[1] == x[2])]

## replace the variances-covariances that are zero or near zero with their smoothed counterparts
mfh_dt <- 
  mfh_dt |>
  mutate(across(
    starts_with("v_"),
    ~ if_else(abs(.x) <= 1e-4, get(paste0("vsv", str_remove(cur_column(), "^v"))), .x),
    .names = "{.col}"
  ))

fh_step <-
  lapply(X = paste0("direct_povrate", unique(survey_dt[[year_var]])),
         FUN = function(x){

           model_obj <-
           step_wrapper(dt = mfh_dt,
                        xvars = candidate_vars,
                        y = x,
                        cor_thresh = 0.7,
                        k = log(nrow(mfh_dt))) ### using log(n) to force BIC selection

           xx <- names(model_obj$coefficients)[!grepl("(Intercept)",
                                                      names(model_obj$coefficients))]
           return(xx)

         })


# mfh_formula <- 
#   mapply(x = paste0("direct_povrate", unique(survey_dt[[year_var]])),
#          y = variance_cols,
#          FUN = function(x, y){
#            
#            fh_step <- step_wrapper_fh(dt = mfh_dt,
#                                       xvars = candidate_vars,
#                                       y = x,
#                                       cor_thresh = 0.8,
#                                       criteria = "BIC",
#                                       vardir = y,
#                                       transformation = "no")
#            
#            return(fh_step$fixed)
# 
#          },
#          SIMPLIFY = FALSE)


mfh_formula <-
  mapply(FUN = function(x, rhs){

    y <- as.formula(paste0(x, " ~ ", paste(rhs, collapse = " + ")))

    return(y)

  },
  x = paste0("direct_povrate", unique(survey_dt[[year_var]])),
  rhs = fh_step |> map(~.x[1:3]),
  SIMPLIFY = FALSE)

### here is what the 3 equations look like
mfh_formula

```

## Step 3: Fitting the Multivariate Fay Herriot Model

Next, we show how to use the `msae` R package to estimate the Empirical Best Linear Unbiased Predictor (EBLUP) for the poverty map using the `eblupMFH2()` which allow for time series fay herriot estimation under homoskedastic assumptions. For completeness, we also briefly perform the previous described direct estimation in step 1, using the `eblupUFH()` function as well as the `eblupMFH1()` for the fay herriot model.

```{r mvfh-est}
#| echo: true
#| eval: false
mfh_dt1 <- mfh_dt |> mutate(across(contains("v_pov_"), ~ 0.1)) |> as_tibble()

# univariate FH
model0_obj <- eblupUFH(mfh_formula, vardir = varcols, data = mfh_dt)
bd_out |> pin_write(model0_obj, type = "rds")

# multivariate FH 1
tic(msg = "eblupMFH1")
model1_obj <- eblupMFH1(mfh_formula, vardir = varcols, data = mfh_dt, MAXITER = 10000, PRECISION = 0.01)
bd_out |> pin_write(model1_obj, type = "rds")
toc()

# multivariate FH 2
tic(msg = "eblupMFH2")
model2_obj <- eblupMFH2(mfh_formula, vardir = varcols, data = mfh_dt, MAXITER = 1e10, PRECISION = 0.01)
bd_out |> pin_write(model2_obj, type = "rds")
toc()
```

```{r mvfh-load}
model0_obj <- bd_out |> pin_read("model0_obj")
model1_obj <- bd_out |> pin_read("model1_obj")
model2_obj <- bd_out |> pin_read("model2_obj")
```

## Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers

We now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas.

### Linearity Test

To assess whether a linear regression model may be incorrectly specified — for instance, due to omitted variables or incorrect functional form — we can use Ramsey’s Regression Equation Specification Error Test (RESET). This test examines whether adding nonlinear combinations (typically powers) of the model’s fitted values significantly improves the model. However, the outcome of interest now is the model MSEs. A significant test result (low p-value) suggests the model is mis-specified and may benefit from additional or transformed predictors.

We implement Ramsey’s RESET test in R using the resettest() function from the lmtest package:

```{r lin-test}
#| echo: true
### lets create a dataframe with the errors and estimated poverty rates

eblup_dt <- model2_obj$eblup
mse_dt <- model2_obj$MSE

colnames(eblup_dt) <- paste0("eblup_", colnames(eblup_dt))
colnames(mse_dt) <- paste0("mse_", colnames(mse_dt))

reset_dt <- bind_cols(eblup_dt, mse_dt) |> as_tibble()
  
### lets perform the reset test on the pairs of variables as appropriate i.e. poverty = B0 + B1*MSE

test_list <- 
  lapply(unique(survey_dt[[year_var]]), function(x) {
    
    # Subset only the columns for year x
    dt <- reset_dt |>
      dplyr::select(matches(paste0(x, "$")))  # Select columns ending with current year
    
    yvar <- colnames(dt)[grepl("^eblup_", colnames(dt))]
    xvar <- colnames(dt)[grepl("^mse_", colnames(dt))]
    
    # Create formula with squared and cubed terms using I()
    form <- as.formula(paste0(
      xvar, " ~ ", 
      yvar, " + I(", yvar, "^2)"
    ))
    
    model_obj <- lm(form, data = dt)
    
    return(model_obj)
    
  })

reset_dt2 <- 
  reset_dt |> 
  mutate(id = row_number()) |> 
  pivot_longer(c(contains("eblup"), contains("mse"))) |> 
  mutate(
    year = str_extract(name, "\\d{4}") |> as.numeric(),
    var =  str_extract(name, "eblup|mse")
  ) |> 
  dplyr::select(-name) |> 
  pivot_wider(names_from = var, values_from = value )
  
reset_dt2 |> 
  ggplot() + 
  aes(x = eblup, y = mse) + 
  geom_point() + 
  facet_wrap(. ~ year) + 
  geom_smooth() + 
  theme_bw()

```

Here is what the results look like:

```{r}
#| echo: true
test_list %>% lapply(X = ., FUN = summary)
```

The results particularly for the final 2 years indicate that we might need to retransform the variables in the model as there are potentially non linear relationships which our model is not capturing. Perhaps creating higher order polynomials and other variable transformations of our right hand side variables reduce the error rates within the model.

### Evaluating the Normality Assumption

#### The Shapiro Wilks Test

We use the shapiro wilks test of normality using the `shapiro.test()` function in base R. The Shapiro-Wilk test assesses whether a sample of data is drawn from a normally distributed population. It does so by comparing the order statistics (i.e., sorted values) of the sample to the expected values under a normal distribution. Specifically, the test statistic $W$ is a ratio of the squared correlation between the observed sample quantiles and the corresponding normal quantiles.

First, we perform the shapiro wilks normality test on the model errors, $\varepsilon$. We show both the normality distribution histogram as well as the qqplots as below:

```{r sw-test}
#| echo: true


### first lets replace the negative values with 0
eblup_dt[eblup_dt < 0] <- 0

### evaluating the normality assumption

#### first lets create a residual table by looking at the difference between actual and predicted poverty rates
resid_dt <- mfh_dt[,paste0("direct_povrate", unique(survey_dt[[year_var]]))] - eblup_dt

### perform the shapiro test

shapiro_obj <- apply(resid_dt, 2, shapiro.test)

summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

resid_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "Residual") %>%
  ggplot(aes(x = Residual)) + 
  geom_histogram(bins = 10, fill = "steelblue", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Residual Histograms by Time Period")


### here's how to create qqplots
resid_dt %>%
  pivot_longer(cols = everything(),
               names_to = "Time",
               values_to = "Residual") %>%
  ggplot(aes(sample = Residual)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Time, scales = "free") +
  theme_minimal() +
  labs(title = "QQ Plots of Residuals by Time Period")

```

Likewise, we test the normality of the random effect variable

```{r}
#| echo: true

#### For the random effects
raneff_dt <- as.data.frame(model2_obj$randomEffect)

### lets run the shapiro wilks tests again
shapiro_obj <- apply(raneff_dt, 2, shapiro.test)


summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

raneff_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "RandEff") %>%
  ggplot(aes(x = RandEff)) + 
  geom_histogram(bins = 10, fill = "darkorange", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Random Effects Histograms by Time Period")


```

In both cases, we compare the p-value to the 0.05 level of significance. The results suggest that in most cases we have to reject the null hypothesis of normally distributed model errors and random effects. This doesn't affect the validity of our poverty estimates for the Multivariate Fay Herriot model. However, subsequent analysis that will assume a normal distribution of the model errors cannot be performed. One good example of this is the statistical significance test for changes in poverty rates over time. For the purposes of this tutorial, we will carry on to show how to perform this under the assumption of normally distributed model errors. However, if the test for normality fails, this test cannot be carried out.

Next, we will show the benefits of small area estimation over direct estimation

### Comparing Direct Estimation to Multivariate Model Outputs

```{r, warning=FALSE, message = FALSE, error = FALSE}
#| echo: true

model2mse_dt <- 
  model2_obj$MSE |>
  mutate(!!sym(area_vars[[1]]) := 1:n()) |>
  pivot_longer(
    cols = starts_with("direct_povrate"),
    names_to = "year",
    names_pattern = "direct_povrate(\\d+)",  # Extract just the digits
    values_to = "modelMSE"
  ) |>
  mutate(year = as.integer(year))

model2pov_dt <- 
  model2_obj$eblup |>
  mutate(!!sym(area_vars[[1]]) := 1:n()) |>
  pivot_longer(
    cols = starts_with("direct_povrate"),
    names_to = "year",
    names_pattern = "direct_povrate(\\d+)",  # Extract just the digits
    values_to = "modelpov"
  ) |>
  mutate(year = as.integer(year))

model2pov_dt <- merge(model2mse_dt, model2pov_dt)


model2pov_dt <- 
  model2pov_dt |>
  mutate(modelCV = sqrt(modelMSE) / modelpov)


model2pov_dt <- merge(model2pov_dt, 
                      direct_dt, 
                      by = c(area_vars[[1]], 
                             year_var))

### compute direct CVs and include in `model2pov_dt`


## we have to reshape the direct estimates data in mfh_dt from wide to long
## and then merge with model2pov_dt 
model2pov_dt <- 
mfh_dt |>
  dplyr::select(starts_with(variance_cols),  # add this!
                all_of(area_vars[[1]])) |>
  pivot_longer(
    cols = -all_of(area_vars[[1]]),
    names_to = "indicator_year",
    values_to = "value"
  ) |>
  mutate(
    type = case_when(
      str_starts(indicator_year, "v_pov_indicator") ~ "variance"
    ),
    year = str_extract(indicator_year, "\\d{4}")
  ) |>
  dplyr::select(-indicator_year) |>
  pivot_wider(
    names_from = type,
    values_from = value
  ) |>
  mutate(year = as.integer(year)) |>
  merge(model2pov_dt,
        by = c(area_vars[[1]], year_var),
        all = TRUE) |>
  mutate(direct_CV = sqrt(variance) / direct_povrate) |>
  merge(sampsize_dt %>%
          dplyr::select(!!sym(area_vars[1]), "N") |>
          unique(), 
        by = area_vars[[1]])


```

Now lets plot the direct against the model CVs to get a sense for the gains made as a result of applying the MFH modelling approach over direct estimation. We will also show the sample sizes for each year to show that large sample size reduce the gains from small area estimation.

```{r}
#| echo: true
#| fig-width: 15
#| fig-height: 20

numeric_cols <- sapply(model2pov_dt, is.numeric)
model2pov_dt_clean <- model2pov_dt[complete.cases(model2pov_dt[, numeric_cols]) &
                                     apply(model2pov_dt[, numeric_cols], 1, function(row)
                                       all(is.finite(row))), ]
  

# Make year a factor using year_var
model2pov_dt_clean[[year_var]] <- as.factor(model2pov_dt_clean[[year_var]])

# Compute max values for scaling
max_n <- max(model2pov_dt_clean$N, na.rm = TRUE)
max_cv <- max(model2pov_dt_clean$direct_CV, na.rm = TRUE)

# Create the plot
ggplot(model2pov_dt_clean, aes(x = .data[[year_var]])) +
  geom_line(aes(y = direct_CV, group = 1, color = "Direct CV"), size = 1) +
  geom_line(aes(y = modelCV, group = 1, color = "Model CV"), size = 1) +
  geom_point(aes(y = N / max_n * max_cv, color = "Sample Size (N)"),
             shape = 1, size = 1.5) +
  facet_wrap(vars(.data[[area_vars[[1]]]]), scales = "free_y") +
  scale_y_continuous(
    name = "Coefficient of Variation (CV)",
    sec.axis = sec_axis(~ . * max_n / max_cv, name = "Sample Size (N)")
  ) +
  scale_color_manual(values = c("Direct CV" = "darkorange",
                                "Model CV" = "steelblue",
                                "Sample Size (N)" = "grey40")) +
  labs(x = "Year", color = "") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.title.y.right = element_text(color = "grey40"),
    axis.title.y.left = element_text(color = "black"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

The other advantage of MFH estimation is the smoothing of the model estimates over the unit variate model. We can show this below as well:

```{r}
#| echo: true
#| fig-width: 15
#| fig-height: 20


model2pov_dt[[year_var]] <- as.factor(model2pov_dt[[year_var]])

## first lets merge in the results of the eblupUFH model

ufh_dt <- 
  model0_obj$eblup |>
  mutate(!!sym(area_vars[[1]]) := 1:n()) |>
  pivot_longer(
    cols = starts_with("direct_povrate"),
    names_to = "year",
    names_pattern = "direct_povrate(\\d+)",  # Extract just the digits
    values_to = "ufh_povrate"
  )

model2pov_dt <- merge(model2pov_dt,
                      ufh_dt,
                      by = c(area_vars[[1]], year_var))

## now we make a similar plot but comparing the UFH vs MFH model estimates to check for the smoothness of the estimation 

# Compute max values for scaling
max_n <- max(model2pov_dt$N, na.rm = TRUE)


ggplot(model2pov_dt, aes(x = .data[[year_var]])) +
  geom_line(aes(y = ufh_povrate, group = 1, color = "UFH Poverty"), size = 1) +
  geom_line(aes(y = modelpov, group = 1, color = "MFH Poverty"), size = 1) +
  geom_point(aes(y = N / max_n, color = "Sample Size (N)"),
             shape = 1, size = 1.5) +
  facet_wrap(vars(.data[[area_vars[[1]]]]), scales = "free_y") +
  scale_y_continuous(
    name = "Poverty Rates",
    sec.axis = sec_axis(~ . * max_n, name = "Sample Size (N)")
  ) +
  scale_color_manual(values = c("UFH Poverty" = "darkorange",
                                "MFH Poverty" = "steelblue",
                                "Sample Size (N)" = "grey40")) +
  labs(x = "Year", color = "") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.title.y.right = element_text(color = "grey40"),
    axis.title.y.left = element_text(color = "black"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )




```

It can be seen from the plots that the provinces in with the highest variance in UFH poverty estimates tend to provide smoother MFH estimates particularly in the provinces with smaller samples. This shows the power of the MFH approach in taking advantage of the across year correlation of poverty.

Next we will assume our model errors were normally distributed in order to show how to statistically test to see if poverty has changed between given years.

### Did the poverty rates change over time?

Next we can call the `pbmcpeMFH2()` function, which returns the EBLUP, as well as, the MSEs of the EBLUPs for each time point, and the MCPEs for each pair of time points based on the MFH model 2 as follows:

```{r pbmcpeMFH2-est}
#| echo: true
#| eval: false
set.seed(123)
tic(pbmcpeMFH2)
mcpemfh2_obj <- 
  pbmcpeMFH2(formula = mfh_formula,
           vardir = varcols,
           nB = 50,
           data = mfh_dt, 
           MAXITER = 1e10, 
           PRECISION = 1e-2)
bd_clean |> pin_write(mcpemfh2_obj, type = "rds")
toc()
```

```{r pbmcpeMFH2-load}
mcpemfh2_obj <- bd_clean |> pin_read("mcpemfh2_obj")
```

```{r}

#' A function to compare and plot differences from the mcpe object
#' 
compare_mfh2 <- function(period_list = c(1,2),
                         mcpe_obj = mcpemfh2_obj,
                         alpha = 0.05){
  
  col_chr <- paste0("(", period_list[1], ",", period_list[2], ")")

  df <- 
    tibble(diff = mcpe_obj$eblup[,period_list[2]] - mcpe_obj$eblup[,period_list[1]],
           mse = mcpe_obj$mse[,period_list[1]] + mcpe_obj$mse[,period_list[2]] - 2*mcpe_obj$mcpe[,col_chr],
           alpha = rep(alpha, nrow(mcpe_obj$eblup)),
           zq = qnorm(alpha / 2, lower.tail = F)) |>
    mutate(lb = diff - zq * sqrt(mse),
           ub = diff + zq * sqrt(mse)) |>
    mutate(significant = ifelse(lb > 0 | ub < 0, 
                                "Significant", 
                                "Not Significant")) |>
    mutate(index = row_number())
  
  p1 <- 
  ggplot(df, aes(x = index, y = diff)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_errorbar(aes(ymin = lb, ymax = ub, color = significant), width = 0.2, linewidth = 1) +
  geom_point(color = "black", size = 2) +
  scale_color_manual(values = c("Significant" = "red", "Not Significant" = "gray40")) +
  labs(
    x = "Area (Index)",
    y = "Difference between time periods",
    color = "Significance",
    title = paste0("Change in Poverty Rates based on MCPE between period ", 
                   period_list[1], 
                   " and ", 
                   period_list[2])
  ) +
  theme_minimal()
    
  
  return(list(df = df,
              plot = p1))
  
}
```

The above function takes the difference between any two time periods and prepares a table of differences for each area include the MCPE estimated error rates as well as lower and upper bounds given a specified confidence level. See the function implemented to compare periods 1 and 2 (years 2012 and 2013)

```{r}
#| echo: true
comp12_obj <- compare_mfh2()
comp23_obj <- compare_mfh2(period_list = c(2, 3))
comp13_obj <- compare_mfh2(period_list = c(1, 3))
```

See the plots below

```{r}
comp12_obj$plot
comp23_obj$plot
comp13_obj$plot
```

See the data created

-   Comparing the years 2012 and 2013

```{r}
#| echo: true
comp12_obj$df %>% head() %>% kable()
```

-   Comparing the years 2013 and 2014

```{r}
#| echo: true
comp23_obj$df %>% head() %>% kable()
```

-   Comparing the years 2012 and 2014

```{r}
#| echo: true
comp13_obj$df %>% head() %>% kable()
```

### Presenting Results

Now that we have estimated the MFH models and run some diagnostics, we might be interested in the following:

-   presenting some poverty maps

```{r}
#| echo: true
#| fig-width: 15
#| fig-height: 8

lapply(X = unique(survey_dt[[year_var]]),
       FUN = function(x){
         
         shp_dt |>
           merge(model2pov_dt %>% 
                   filter(!!sym(year_var) == x) |>
                   dplyr::select(!!sym(area_vars[[1]]), "modelpov"),
                 by = area_vars[[1]]) |>
           ggplot() + 
           geom_sf(aes(fill = modelpov), color = NA) +
            scale_fill_viridis(
              name = "Poverty Map",
              option = "magma",
            ) +
            theme_minimal() +
            labs(
              title = paste0("Spatial Distribution of Poverty ", x),
              caption = "Data source: Author Calculation"
            ) +
            theme(
              legend.position = "bottom",
              axis.text = element_blank(),
              axis.ticks = element_blank(),
              panel.grid = element_blank()
            )
         
       }) |>
  Reduce(f = "+")
  
  

```

-   quantifying poverty change by mapping the growth rates of poverty rates for all the time periods

```{r}
#| echo: true

### lets work with our shapefile
longpov_dt <- 
  model2_obj$eblup |>
  mutate(!!sym(area_vars[[1]]) := 1:n()) |>
  pivot_longer(
    cols = starts_with("direct_povrate"),
    names_to = "year",
    names_pattern = "direct_povrate(\\d+)",  # Extract just the digits
    values_to = "modelpov"
  )

### lets perform a regression for each area of poverty rates against year 
### and then we divide the slope variable by the average poverty rate

shp_dt$growth_rate <- 
longpov_dt |>
  mutate(!!sym(year_var) := as.integer(!!sym(year_var))) |>
  group_split(!!sym(area_vars[[1]])) %>%
  lapply(X = .,
         FUN = function(x){
           
           y <- lm(paste0("modelpov ~ ", year_var[[1]]), data = x)
           
           y <- coef(y)[2]
           
           delta <- y / (mean(x$modelpov, na.rm = TRUE))
           
           return(delta)
           
         }) |>
  unlist() |>
  unname()

```

Lets plot this like with a heatmap on a shapefile:

```{r}
#| echo: true

# Cap growth rates at 1 just to get rid of the outlier
shp_dt$growth_rate_capped <- pmin(shp_dt$growth_rate, 1)

ggplot(shp_dt) +
  geom_sf(aes(fill = growth_rate_capped), color = NA) +
  scale_fill_viridis(
    name = "Growth Rate (capped at 1)",
    limits = c(min(shp_dt$growth_rate_capped), 0.5),
    oob = squish,
    option = "magma",
  ) +
  theme_minimal() +
  labs(
    title = "Spatial Distribution of Poverty Growth Rates",
    subtitle = "Growth rates capped at 1 to reduce outlier effect",
    caption = "Data source: Author Calculation"
  ) +
  theme(
    legend.position = "bottom",
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

```
