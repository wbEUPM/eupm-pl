---
title: "The Multivariate Fay Herriot"
author: "Ifeanyi Edochie"
format: html
editor: source
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false
#| error: false

# Chunk setup
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

pacman::p_load(
  sf, 
  data.table, 
  car, 
  msae,
  sae, 
  survey, 
  spdep,
  knitr, 
  MASS, 
  caret,
  purrr,
  pins,
  gt,
  lmtest, 
  scales,
  viridis, 
  patchwork,
  dplyr, 
  tictoc,
  gt,
  matrixcalc,
  tidyverse,
  modelsummary,
  flextable,
  correlation,
  datawizard
  )

# Loading locally-developed
list.files("R", pattern = "*.R$", full.names = TRUE, ignore.case = TRUE) |>
  walk(~ suppressMessages(source(.x)))

# Raw data root
root_raw <- "./data/raw"
root_temp <- "./data/temp"
root_clean <- "./data/clean"

# Data-storage boards
bd_raw <- root_raw |> file.path("api") |> board_folder(versioned = T)
bd_aux <- root_temp |> board_folder(versioned = T)
bd_clean <- root_clean |> board_folder(versioned = T)
bd_out <- "output/res-50-mfh" |> board_folder(versioned = T)
```

## Overview

This section describes procedures that yield stable small area estimators for each of $D$ areas over $T$ subsequent time instants. Area populations, the samples, and the data might change between time periods. Accordingly, we denote $U_t$ the overall population at time $t$, which is partitioned into $D$ areas $U_{1t}, ..., U_{Dt}$, of respective population sizes $N_{1t}$.

An overview of the MFH model estimation process is as follows:

-   Step 1: The data preparation: direct poverty estimates, their corresponding sampling variances and covariances at the targeted area-level, and the area-level geometries. Usually completed in a different script.

-   Step 2: Preparing variables (from administrative data or other sources) representative at the target area level for each time instant.

    -   Selecting a handful of relevant covariates following one of the selection processes.

-   Step 3: Fit the MFH models across years of available data.

    -   Test for homoskedastic area-time effects $({\mu_d}_1, ..., {\mu_d}_T)$. If we reject the homoskedasticity of variances, implement the MFH3 model. Otherwise, we proceed with the MFH2 model.

    **Note**: For the purposes of this training, we assume a homoskedastic model for simplicity.

-   Step 4: Check the selected assumptions, including: linearity, normality of predicted area effects and standardized residuals, and the presence of the outlying areas.

-   Step 5: In case of systematic model departures such as isolated departures because of outlying areas, some adjustments might need to be implemented before returning to Step 2 to recompute the MFH model.

-   Step 6: If model assumptions hold, using the above direct estimates and estimated sampling variances and covariances, and the selected auxiliary variables, compute MFH estimators $\hat{\delta}_{dt}^{MFH},\quad d = 1,...,D$ and $t = 1, ..., T$ and their corresponding estimated MSEs.

Below, we demonstrate the use of:

-   eblupUFH(): univariate Fay-Herriot model;

-   eblupMFH1(): multivariate Fay-Herriot model (model 1) that assumes no autocorrelation and homoskedasticity;

-   eblupMFH2(): autoregressive multivariate Fay-Herriot model (model 2) that assumes autocorrelation but homoskedasticity;

-   eblupMFH3(): heteroskedastic and autoregressive multivariate Fay-Herriot model (model 3).

All of it is implemented in the msae R package [@permatasari2022package], see ?eblupMFH1 for help.

### Data organization

To ensure transparent data organization, we adopt a standard structure of the poverty estimates data produced in our analysis. Specifically, each estimation method produces data set of the following structure (@tbl-data-str).

```{r echo=FALSE}
#| label: tbl-data-str
#| tbl-cap: Structure of the data resulting form MFH and other poverty estimates
tribble(
  ~Variable, ~Description,
  "id", "Region ID",
  "year", "Year",
  "estimate", "Estimate type, e.g. UFH, MFH1, MHF2, MFH3, direct, etc.",
  "type", "Poverty measure type (dependent variabel), e.g. AROP, AROPE, AROP anchored, AROP real.",
  "pov", "Estimates of poverty",
  "SE", "Standard errors of the direct estimates",
  "vardir", "Variance (SE^2)",
  "CV", "Coefficeint of variation (SE/pov)",
   "var_SRS", "",
  # "deff", "",
  # "n_eff", "",
  "v_pov20XX", "Covariance between poverty in current year versus year 20XX (multiple columns)"
) |> 
  flextable() |> FitFlextableToPage()
```

For this example, we use actual data for Poland for 2011, 2019-2023.

-   Direct estimates are prepared based on the confidential SILC data in the script `10-direct-estimates.qmd`.

-   Covariances between poverty rates across years prepared based on the confidential SILC data along with the direct estimates in `10-direct-estimates.qmd`.

-   Right-hand-side variables, are prepared by the NSO using the [API](https://api.stat.gov.pl/Home/BdlApi) to their sub-national data base accessible through the R package [`bdl`](https://cran.r-project.org/package=bdl), see the script `20-other-variables.qmd`.

-   Sub-national geometries are available with the [`bdl`](https://cran.r-project.org/package=bdl) package and are prepared in `05-geometries.qmd`.

**Note:** since we have multiple different types of poverty estimates, e.g. AROP, AROPE, AROP anchored, etc. we recommend creating separate scripts for preparing variable-specific poverty maps. This tutorial focuses exclusively on AROP poverty estimate.

## Step 1: Data Preparation

The MFH estimation process relies on: direct estimates, right hand side variables, subnational geometries, and poverty covaraince across time.

### Direct estimates

Direct estimates of poverty and its variance, `pov_direct`. Its is structure as outlined in @tbl-data-str. Below we presents the structure of this data set.

```{r dta-load-direct}
pov_direct <- bd_clean |> pin_read("pov_direct") |> 
  rename(id = subcode) |> filter(type == "arop") |> 
  mutate(estimate = "Direct")
```

```{r dta-str-pov-direct}
pov_direct |> glimpse()
```

### Covariances between poverty rates

Covariances are prepared together with the direct estimates, however, because they are created for each pair of years-specific poverty estimates at the regional level, their format differ from. In addition, we have covariance that are estimated based on the fully balanced data and based on the pairs of years, where only balanced panels for each pair of years is used. Column `vcoc_type` distinguishes between these two type of covariances.

To be able to use covariances in our analysis, we need to:

-   Transform data into the wide format, where:

    -   Each row correspond to a single region

    -   Each column shows poverty covariance between year A and year B.

-   Assume that any pair of years that does not have a covariance estimate actually means that the covariance is zero.

-   Replace any negative covariance with zero, or apply covariance smoothing to it if needed.

```{r dta-load-covariance}
pov_covar <-
  bd_clean |> pin_read("pov_vcov") |> 
  rename(id = subcode) |> 
  mutate(type = str_remove(from, "\\d{4}"),
         across(c(from, to), ~ str_extract(., "\\d{4}"))) |> 
  filter(type == "arop", variable == "covariance", from != to) |>
  pivot_wider(names_from = c(from, to), values_from = value, names_prefix = "cov") |>
  (\(x) {
    expand.grid(id = unique(x$id), vcov_type = unique(x$vcov_type)) |> 
      left_join(x, by = join_by(id, vcov_type)) |> 
      as_tibble()
  })() |> 
  mutate(across(contains("cov"), ~ifelse(is.na(.), 0, .))) |> 
  select(-variable) |> 
  arrange(id, vcov_type)
```

Resulting covariance data has the following structure:

```{r dta-str-vcov}
#| echo: true
pov_covar |> glimpse()
```

### Right hand side variables

Right hand side (RHS) variables i.e. indicators representative at the level of the target area for each area consist of two components:

-   Data present in the wide format, where each column represents an individual variable and row and observation for one area and one year. Stecifically columns are:

    -   `id` regional identifier used to match RHS variables with regions and poverty estiamtes

    -   `year` year for which observations are used.

    -   `x1`, `x2`, `x3`, ..., `xn` are the RHS variables described in the metadata.

-   Metadata table describes briefly each variable and provides a short, self-explaining name, used in regression tables.

```{r dta-load-rhs}
rhs_dta_0 <- 
  suppressMessages(file.path(root_raw, "data-other", "Auxiliary (potential) variables.xlsx") |>
      readxl::read_excel()) 

new_names <- 
  rhs_dta_0 |> slice(1) |> t() |> as.data.frame() |>
  rownames_to_column() |> as_tibble() |> 
  mutate(rowname = ifelse(str_detect(rowname, "\\.{2,}"), NA_character_, rowname)) |> 
  fill(rowname) |> 
  mutate(rowname = str_to_lower(rowname),
         rowname = ifelse(!is.na(V1), str_c(rowname,"__", V1), rowname)) |> 
  pull(rowname)

rhs_dta <- 
  rhs_dta_0 |> 
  slice(-1) |> 
  (\(x){colnames(x) <- new_names; x})() |> 
  select(-no., -subregion) |> 
  pivot_longer(
    cols = matches("\\d{4}"), 
    values_transform = \(x) suppressWarnings(as.numeric(x))
    ) |> 
  filter(!is.na(value)) |> 
  separate(name, into = c("var", "year")) |> 
  mutate(year = as.numeric(year)) |> 
  pivot_wider(names_from = "var", values_from = "value")


rhs_meta <-
  file.path(root_raw, "data-other", "Auxiliary (potential) variables.xlsx") |>
  readxl::read_excel(sheet = 2) |> 
  select(var = Variable,
         category = `Category of variable`,
         name = `Variable (in English)`) |> 
  mutate(var = str_to_lower(var))
```

```{r dta-str-rhs}
#| echo: true
rhs_dta |> select(1:10) |> glimpse()
rhs_meta |> glimpse()
```

### Geo-spatial boundaries

Geo-spatial boundaries of each target area are loaded. We use NUTS2 and NUTS3 boundaries for plotting. Column `id` is used to map polygons to areas.

```{r dta-load-gis}
#| echo: false
geoms <-  bd_clean |> pin_read("geometries")
geom_nuts3 <- geoms$level4 |> rename(subcode = id)
geom_nuts2 <- geoms$level2
```

Lastly, we define a set of key variables names used in the code below as well as the range of years, for which we estimate our poverty estimates.

```{r set-var-names}
#| echo: true

area_var <- c("id") 
year_var <- "year"
directpov <- "pov"
directvar <- "vardir"
year_set <- c(2019:2023)
```

## Step 2: Variables preparation and selection

At this step we prepared in the exact form that enables its use in `eblupUFH` and `eblupMFH` functions. This includes:

-   Dependent variable preparation in the wide format with year-specific poverty rates as columns and regions as rows. `id` column is used as a regional identifier.

-   RHS variables preparation and reshaping:

    -   Computing economically sound ratios, constructing complex variables and performing linear transformation

    -   Reshaping data to the wide format with variables names and years in columns and regions identified by `id` variable in rows.

-   RHS variables selection that maximize the predictive power.

### Dependent variable preparation

```{r}
#| echo: false
mfh_dt0 <- 
  pov_direct |>
  select(id, year, all_of(c(year_var, directpov, directvar))) |> 
  pivot_wider(
    id_cols = all_of(area_var),
    names_from = all_of(year_var),
    values_from = all_of(c(directpov, directvar)),
    names_glue = "{.value}{year}",
    values_fn = ~ dplyr::first(na.omit(.x))
  ) 
```

Wide-format reshaped direct poverty estimates have the following structure:

```{r}
mfh_dt0 |> glimpse()
```

### RHS variables preparation

While computing the new variables, we need to make sure that we:

-   name them uniquely and appropriately;

-   record relevant metadata for them.

@tbl-rhs-descr-stats provides the summary statistics for the RHS variables after we computed additional ones.

```{r var-preparation}
rhs_dta0 <- 
  rhs_dta |> 
  mutate(
    x101 = x4 / x3, # Share of Working Age pop in the region,
    x102 = x6 / (x3 / x42),
    x103 = x7 / (x3 / x42),
    x104 = x8 / (x3 / x42),
    x105 = x9 / (x3/10000),
    x106 = x12 / (x3 / x42),
    x118 = x12 / (x3 / x42),
    x107 = x14 / (x3 / x42),
    x108 = x23 / (x3/10000),
    x109 = x41 /  (x3/10000),
    x110 = x40 /  (x3/x42),
    x111 = x22 /  (x3/x42),
    x112 = x17 /  (x12),
    x113 = x18 /  (x12),
    x114 = x19 /  (x12),
    x115 = x13 /  (x12),
    x116 = x1 /  (x3 / 1000),
    x117 = x2 /  (x3 / 1000),
    x119 = x29 /  (x4 / 1000),
    x120 = x30 /  (x4 / 1000)
  ) |> 
  select(#-x1, -x2, -x3, 
         -x6, -x7, -x8, -x12, -x13,
         -x17, -x18, -x19, -x31, -x33)

rhs_meta_extra <- 
  tribble(
    ~var, ~name,
    "x101", "Workage pop share",
    "x102", "Sahre fam. poverty ben.",
    "x103", "Sahre fam. homelessness ben.",
    "x104", "Sahre fam. unemployment ben.",
    "x105", "Market places per 10000 person",
    "x106", "Dwelling appartment per family",
    "x107", "Dwelling alowance per family",
    "x108", "Pharmacies per 10000 person",
    "x109", "Care facil. resid. per 10000 person",
    "x110", "Childrent 0-3 per family",
    "x111", "Share of families with Large Fam Card",
    "x112", "Water, share dwel.",
    "x113", "Heating, share dwel.",
    "x114", "Gas, share dwel.",
    "x115", "Rooms per apt.",
    "x116", "Birth rate per 1000",
    "x117", "Deth rate per 1000",
    "x118", "Dwelling rooms per family",
    "x119", "Immigrants per 1000 working pop",
    "x120", "Emmigrants per 1000 working pop"
  )

rhs_meta_0 <- rhs_meta |> bind_rows(rhs_meta_extra)

# # Identify the columns of interest
# x_vars <- mfh_dt0 |> select(matches("^X\\d+y\\d+")) |> names()
# x_vars_numeric <- x_vars[sapply(mfh_dt0[x_vars], is.numeric)]
# 
# # Function to min-max scale
# min_max_scale <- function(x) {
#   rng <- range(x, na.rm = TRUE)
#   (x - rng[1]) / (rng[2] - rng[1])
# }
# 
# # Mutate log and scaled versions
# mfh_dt <-
#   mfh_dt0 %>%
#   mutate(
#     # Add log-transformed columns
#     across(all_of(x_vars_numeric), .fns = ~ log1p(.), .names = "log{.col}"),
#     
#     # Add min-max scaled columns
#     across(all_of(x_vars_numeric), .fns = ~ min_max_scale(.), .names = "scaled{.col}")
#   )
```

```{r tbl-rhs-descr-stats}
#| tbl-cap: Descriptive statistics of the RHS variables
rhs_dta0 |>
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  mutate(var_order = str_extract(var, "\\d{1,4}") |> as.numeric()) |>
  arrange(var_order) |>
  left_join(rhs_meta_0, by = join_by(var)) |>
  mutate(name = str_c(var, " ", name) |> as_factor()) |>
  mutate(year = as.character(year)) |>
  (\(x) bind_rows(x |> mutate(year = "All years"), x))() |>
  filter(year == "All years") |>
  mutate(year = as_factor(year)) |>
  datasummary(
    formula = name  ~ value * (Mean + SD + Min + P50 + Max),
    output = "flextable",
    data = _
  ) |>
  FitFlextableToPage()
```

After all variables are computed, we perform logarithm transformation of all contentious variables, where maximum is above 10, or difference between min and maximum exceeds 10 times.

```{r}
var_to_log <- 
  rhs_dta0 |>
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  group_by(year, var) |> 
  summarise(min = min(value), max = max(value)) |> 
  filter(min * 10 < max & max > 10 | max > 100) |> pull(var) |> unique()

rhs_dta1 <- 
  rhs_dta0 |>
  group_by(year) |>
  mutate(across(all_of(var_to_log), ~ log(.))) |> 
  ungroup()
```

Descriptive statistics after transformation is:

```{r tbl-rhs-descr-stats}
#| tbl-cap: Descriptive statistics of the RHS variables after transformation
rhs_dta1 |> mutate(type = "log") |> 
  bind_rows(rhs_dta0 |> mutate(type = "level")) |> 
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  mutate(var_order = str_extract(var, "\\d{1,4}") |> as.numeric()) |>
  arrange(var_order) |>
  left_join(rhs_meta_0, by = join_by(var)) |>
  mutate(name = str_c(var, " ", name) |> as_factor()) |>
  mutate(year = as.character(year)) |>
  (\(x) bind_rows(x |> mutate(year = "All years"), x))() |>
  filter(year == "All years") |>
  mutate(year = as_factor(year)) |> #glimpse()
  datasummary(
    formula = name  ~ value * (Mean ) * type,
    output = "flextable",
    data = _
  ) |>
  FitFlextableToPage()
```

### Exploratory correlation analysis

Before conducting the variable selection, we explore the correlations between our transformed variables and the poverty rates.

```{r tbl-cor}
#| tbl-cap: Correlatoin between poverty rates and key variables
stars.pval <- function(x){
  stars <- c("***", "**", "*", "")
  var <- c(0, 0.01, 0.05, 0.10, 1)
  i <- findInterval(x, var, left.open = T, rightmost.closed = T)
  stars[i]
}

dta_cor <-
  rhs_dta1 |> mutate(type = "log") |> 
  bind_rows(rhs_dta0 |> mutate(type = "level")) |> 
  mutate(year = year + 1) |> 
  left_join(pov_direct |> filter(type == "arop") |> select(id, year, pov)) |> 
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  filter(!is.na(pov)) |> 
  mutate(var_order = str_extract(var, "\\d{1,4}") |> as.numeric()) |>
  arrange(var_order) |>
  left_join(rhs_meta_0, by = join_by(var)) |>
  mutate(name = str_c(var, " ", name) |> as_factor()) |>
  mutate(year = as.character(year)) |>
  (\(x) bind_rows(x |> mutate(year = "All years"), x))() |>
  filter(year == "All years") |>
  mutate(year = as_factor(year)) |> 
  # filter(str_detect(var, "x10")) |> 
  select(id, year, type, pov, value, name) |> 
  pivot_wider(names_from = name, values_from = value) |> 
  datawizard::data_group(type) |>
  correlation::correlation() |> 
  as_tibble() |> 
  filter(Parameter1 == "pov") |> 
  mutate(Parameter2 = as_factor(Parameter2) |> fct_reorder(abs(r), .desc = T)) |> 
  mutate(stats = str_c(number(r, 0.001), "", stars.pval(p))) |> 
  select(Group, Variable = Parameter2, stats) |>
  pivot_wider(names_from = Group, values_from = stats) |> 
  arrange(Variable)

dta_cor |> 
  flextable() |>
  FitFlextableToPage()

```

### Variable Selection

Next, we apply a simple variable selection process which employs the stepwise regression algorithm using the AIC selection criteria as in described by [@yamashita2007stepwise]. The function `step_wrapper()` implemented below is a wrapper to the `stepAIC()` function carries all the perfunctory cleaning necessary use the `stepAIC()` function. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations and remove perfectly or near collinear variables and combinations using the variance inflation method.

```{r var-selection}
#| echo: false
dta_full <-
  rhs_dta1 |> 
  mutate(year = year + 1) |> 
  left_join(pov_direct |> filter(type == "arop") |> 
              select(all_of(c(area_var, year_var, directpov, directvar)))) |> 
  filter(!is.na(pov)) |> 
  select(-where(~any(is.na(.))))

dta_full_wide <- 
  dta_full |> 
  pivot_wider(
    names_from = year, 
    values_from = c(contains("x"), all_of(c(directpov, directvar))), 
    names_sep = "_") |> 
  select(id, contains(directpov), contains(directvar), everything())

candidate_vars <- dta_full |> select(contains("x")) |> names()

fh_step <-
  set_names(year_set, year_set) |>
  imap( ~ {
    dta_local <- dta_full |> filter(year == .x) 
    model_obj <-
      step_wrapper(
        dt = dta_local,
        xvars = candidate_vars,
        y = directpov,
        cor_thresh = 0.7,
        k = log(nrow(dta_local)),
        trace = 0
      ) ### using log(n) to force BIC selection
    out <- names(model_obj$coefficients)
    out[!str_detect(out, "Intercept")]
  }) 
# |> 
#   map(~{
#     .x[1:(min(2, length(.x)))]
#   })


# Set of formulas for the Multivariate FH
mfh_formula <-
  fh_step |> 
  imap(~{
    c(str_c(directpov, "_", .y),
      " ~ ",
      str_c(.x, "_", .y) |> str_c(collapse = " + ")) |>
      str_c(collapse = "") |> as.formula()
  })
  
# Single formula for MVFH
model_pooled <-
  step_wrapper(
    dt = dta_full |> select(pov, contains("x1")),
    xvars = candidate_vars,
    y = directpov,
        cor_thresh = 0.7,
        k = log(nrow(dta_full)),
    trace = 0
  ) ### using log(n) to force BIC selection

coef_pooled <- names(model_pooled$coefficients)
coef_pooled <- coef_pooled[!str_detect(coef_pooled, "Intercept")]
form_pooled <- c(directpov,  " ~ ", str_c(coef_pooled, collapse = " + ")) |> str_c(collapse = "") |> 
  as.formula()
```

```{r tbl-varsel-mfh-1}
#| tbl-cap: Multiple linear regressions on slected variables
### here is what the 3 equations look like
mfh_formula |> 
  map(~lm(.x, data = dta_full_wide)) |> 
  modelsummary(
    output = "flextable", 
    estimate = "{estimate}{stars} ({std.error})", 
    statistic = NULL,
    coef_rename = function(x) {
      # browser()
      x |>
        str_remove_all("_\\d{4}") |> 
        map_chr( ~ {
          match_coef <- rhs_meta_0 |> filter(var == .x) |> pull(name)
          if (is.null(match_coef) | length(match_coef) == 0) {
            return(.x)
          } else {
            return(match_coef)
          }
        })
    }
  ) |> 
  autofit() |> 
  width(j = 1, width = 3)
```

```{r tbl-varsel-mfh-2}
#| tbl-cap: Multiple linear regressions on the same set of variables
set_names(year_set, year_set)  |> 
  imap(~lm(form_pooled, data = dta_full |> filter(year == .x))) |> 
  modelsummary(
    output = "flextable", 
    estimate = "{estimate}{stars} ({std.error})", 
    statistic = NULL,
    coef_rename = function(x) {
      # browser()
      x |>
        str_remove_all("_\\d{4}") |> 
        map_chr( ~ {
          match_coef <- rhs_meta_0 |> filter(var == .x) |> pull(name)
          if (is.null(match_coef) | length(match_coef) == 0) {
            return(.x)
          } else {
            return(match_coef)
          }
        })
    }
  ) |> 
  autofit() |> 
  width(j = 1, width = 3)
```

Notes: Significance levels are: `***` p-value \< 0.001, `***` p-value \< 0.01, `*` p-value \< 0.05, `+` p-value \< 0.1

## Step 3: Fitting the Multivariate Fay Herriot Model

```{r fn-run-eblups}
# Run eblup estimates for UVFH and MVFH
est_ufh_mfh1 <- function(dta, formula, 
                         iter = 500, precision = 0.0001,
                         verbose = FALSE) {
  
  var_cols <- dta |> select(contains("vardir"), contains("cov")) |> names()
  print(formula)
  cat("eblupUFH --- \n")
  tic(msg = "eblupUFH")
  model0_obj <- try({
    eblupUFH(
      formula = formula,
      vardir = var_cols,
      data = dta,
      MAXITER = iter,
      PRECISION = precision
    )
  }, silent = T
  )
  toc()
  
  cat("eblupMFH1 --- \n")
  tic(msg = "eblupMFH1")
  if (verbose) {
    model1_obj <- try({
      eblupMFH1_v2(
        formula = formula,
        vardir = var_cols,
        data = dta,
        MAXITER = iter,
        PRECISION = precision
      )
    }, silent = T
    )
  } else {
    model1_obj <- try({
      eblupMFH1(
        formula = formula,
        vardir = var_cols,
        data = dta,
        MAXITER = iter,
        PRECISION = precision
      )
    }, silent = T
    )
  }
  toc()
  
  
  cat("eblupMFH2 --- \n")
  tic(msg = "eblupMFH2")
  if (verbose) {
    model1_obj <- try({
      eblupMFH2_v2(
        formula = formula,
        vardir = var_cols,
        data = dta,
        MAXITER = iter,
        PRECISION = precision
      )
    }, silent = T
    )
  } else {
    model2_obj <- try({
      eblupMFH2(
        formula = formula,
        vardir = var_cols,
        data = dta,
        MAXITER = iter,
        PRECISION = precision
      )
    }, silent = T
    )
  }
  toc()
  
  # Results 
  tibble(model = c("UFH", "MFH1", "MFH2"),
         eblups = list(model0_obj, model1_obj, model2_obj)
         ) |> 
    mutate(results = map(eblups, ~{
      if (!"try-error" %in% class(.x)) {
        out <- ext_mfh_dta(.x,
                           select(dta_mhf, id),
                           type = "arop",
                           estimate = "UFH (MFH)")
      } else {
        out <- tibble()
      }
    }))
}

# Extract EBLUP statistics
ext_mfh_dta <- function(fit, ids, type = "arop", estimate = "UFH") {
  # ids <- mfh_dt |> select(id)
  eblup_dt <- fit$eblup |> as_tibble()
  mse_dt <- fit$MSE |> as_tibble() |> rename_with( ~ str_replace(., "pov", "vardir"))
  # browser()
  bind_cols(ids, eblup_dt, mse_dt) |>
    pivot_longer(c(contains("pov"), contains("vardir"))) |>
    mutate(year = str_extract(name, "\\d{4}") |> as.integer(),
           name = str_remove(name, "\\d{4}")) |>
    pivot_wider(values_from = value, names_from = name) |>
    rename_with(.fn = ~ str_remove(., "_"), .cols = c(contains("pov"), contains("vardir"))) |> 
    mutate(
      CV  = sqrt(vardir) / pov,
      SD = sqrt(vardir),
      type = type,
      estimate = estimate
    )
}

```

### MFH with zero covariance

As a starting point, we run UFH and MFH1 (only 1) with the zero covariance between years.

```{r mvfh-zero-cov-est, eval=FALSE}
dta_cov <- pov_covar |> filter(vcov_type == "paired") |> select(-vcov_type, -type)
dta_mhf <- dta_full_wide |> left_join(dta_cov, by = join_by(id))

eblup_zero_cov <-
  dta_mhf |> mutate(across(contains("cov"), ~ 0)) |> 
  est_ufh_mfh1(dta = _, formula = mfh_formula, verbose = T) |>
  mutate(model = str_c(model, " (0 cov)"))

bd_out |> pin_write(eblup_zero_cov, type = "rds")

eblup_const_cov <-
  dta_mhf |> 
  mutate(
    across(contains("cov"), ~ . / 3),
    across(all_of(c(
      #"cov2019_2020", "cov2020_2021", "cov2021_2022", "cov2022_2023",
      "cov2019_2021", "cov2019_2022", "cov2019_2023",
       "cov2020_2022", "cov2020_2023", "cov2021_2023"
      )), ~ 0)
    ) |> 
  est_ufh_mfh1(dta = _, formula = mfh_formula, verbose = T, precision = 0.001) |>
  mutate(model = str_c(model, " (const. cov. 1y)"))

bd_out |> pin_write(eblup_const_cov, type = "rds")
```

```{r}
# eblup_zero_cov <- bd_out |> pin_read("eblup_zero_cov")
# eblup_zero_cov$eblups[[1]]$eblup |> all.equal(eblup_zero_cov$eblups[[2]]$eblup)
# 
# eblup_const_cov <- bd_out |> pin_read("eblup_const_cov")
# eblup_zero_cov$eblups[[1]]$eblup |> all.equal(eblup_const_cov$eblups[[1]]$eblup)
# 
# 
# eblup_zero_cov$eblups[[2]]$eblup |> all.equal(eblup_const_cov$eblups[[2]]$eblup)


```


```{r mfh-two-years-pairs, eval=FALSE}
dta_cov <- pov_covar |> filter(vcov_type == "paired") |> select(-vcov_type, -type)
dta_mhf <- dta_full_wide |> left_join(dta_cov, by = join_by(id))

# one_comb <- c(2020, 2021)

est_mfh_pair <- function(one_comb) {
  
  mfh_formula_local <- mfh_formula[names(mfh_formula) %in% one_comb]
  
  # Vcov combinations
  var_cols_1 <-
    one_comb |>
    map( ~ {
      dta_mhf |> select(contains(as.character(.x))) |>
        select(contains("vardir"), contains("cov")) |>
        names()
    }) |>
    unlist()
  
  var_cols_2 <- var_cols_1[str_detect(var_cols_1, "vardir")]
  all_comb <- combn(one_comb, m = 2, simplify = T)
  all_comb <- str_c(all_comb[1, ], "_", all_comb[2, ])
  var_cols_3  <- var_cols_1[str_detect(var_cols_1, str_c(all_comb, collapse = "|"))] |>
    unique()
  
  # vcov specification
  var_cols_local <- c(var_cols_2, var_cols_3)
  
  # Univariate FH
  cat("UVFH\n")
  model0_obj <- try(eblupUFH(mfh_formula_local, vardir = var_cols_local, data = dta_mhf))
  
  # MFH 1
  cat("MFH1\n")
  model1_obj <- try(eblupMFH1_v2(
    mfh_formula_local,
    vardir = var_cols_local,
    data = dta_mhf,
    MAXITER = 1000,
    PRECISION = 0.01
  ))
  
  # MFH 2
  cat("MFH2\n")
  # model2_obj <- try({eblupMFH2_v2(
  #   mfh_formula_local,
  #   vardir = var_cols_local,
  #   data = dta_mhf,
  #   MAXITER = 1000,
  #   PRECISION = 0.001
  # )}, silent = T)
  
  tibble(
    year_min = min(one_comb),
    year_max = max(one_comb),
    model = c("UFH", "MFH1"), # "MFH2"),
    eblups = list(model0_obj, model1_obj) #, model2_obj)
  )
}

# est_mfh_pair(c(2019, 2020, 2021))
eblup_pairs_2y <-
  list(c(2019, 2020), 
       c(2020, 2021),
       c(2021, 2022),
       c(2022, 2023)) |>
  map( ~ {est_mfh_pair(.x)}) |> 
  bind_rows() |> 
  mutate(results =
           map(eblups, ~{
             # browser()
             if (!"try-error" %in% class(.x)) {
               out <- ext_mfh_dta(.x,
                                  select(dta_mhf, id),
                                  type = "arop",
                                  estimate = "UFH (MFH)")
             } else {
               out <- tibble()
             }
           })) |> 
  mutate(estimate = str_c(model, " (", year_min, "-", year_max, ")"))


bd_out |> pin_write(eblup_pairs_2y, type = "rds")

```


Saving results of the models

```{r res-save}
# model0_obj
mod_dta <-
  bd_out |> pin_read("eblup_const_cov") |> 
  bind_rows(bd_out |> pin_read("eblup_zero_cov")) |> 
  bind_rows(bd_out |> pin_read("eblup_pairs_2y")) |>
  mutate(model = ifelse(!is.na(estimate), estimate, model)) |>
  rename(est2 = estimate) |>
  unnest(results) |> 
  bind_rows(pov_direct |> mutate(model = "Direct")) |> 
  mutate(estimate = model) |> 
  filter(str_detect(estimate, "Direct|MFH|UFH.{1,2}0")) |> 
  mutate(estimate = as_factor(estimate))
  
# mod_dta |> count(estimate)
# bd_clean |> pin_write(pov_mfh, name = "pov_mfh", type = "rds")
```

## Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers

We now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas.

### SE vs Poverty

```{r}
mod_dta |> filter(year %in% 2022) |>
  ggplot() + 
  aes(y = SD, x = pov, colour = estimate) + 
  geom_point(size = 3, aes(shape = estimate)) +
  # geom_smooth(se = FALSE) +
  facet_wrap(. ~ year, scales = "free") +
  # scale_shape_manual(values= c(21, 24, 23, 22, 3, 4, 5, 7, 8, 10)[1:nlevels(mod_dta$estimate)]) + 
  scale_y_continuous("SE") + 
  coord_cartesian(ylim=c(0, 0.075)) +
  coord_cartesian(xlim=c(0, 0.4)) +
  ylim(c(0, 0.075)) +
  # xlim(c(0, 0.4)) +
  theme_bw()
```

### CV vs Poverty

```{r}
mod_dta |> #filter(year %in% 2023) |>
  ggplot() + 
  aes(y = CV, x = pov, colour = estimate) + 
  geom_point(size = 1) +
  geom_smooth(se = FALSE) +
  facet_wrap(. ~ year, scales = "free") +
  # scale_shape_manual(values= c(21, 24, 23, 22, 3, 4, 5, 7, 8, 10)[1:nlevels(mod_dta$estimate)]) + 
  scale_y_continuous("CV") + 
  coord_cartesian(ylim=c(0, 0.075)) +
  coord_cartesian(xlim=c(0, 0.4)) +
  ylim(c(0, 0.75)) +
  # xlim(c(0, 0.4)) +
  theme_bw()
```

### Direct vs MFH

#### Poverty rate

```{r}
library(glue)
pov_mfh_dir <- 
  pov_mfh |> 
  left_join(pov_direct |>
              select(
                id,
                year,
                pov_direct = pov,
                SD_direct = SD,
                CV_direct = CV
              ),
            by = join_by(id, year)
            ) |> 
  left_join(shp_dt) |> 
  st_as_sf()

pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "pov_direct",
    y_var = "pov",
    colour_var = "estimate",
    x_title_glue = "Poverty rate in direct estimates",
    y_title_glue = "MFH poverty rate",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```

#### CV

```{r}
pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "CV_direct",
    y_var = "CV",
    colour_var = "estimate",
    x_title_glue = "CV in direct estimates",
    y_title_glue = "MFH CV",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```

#### SE

```{r}
pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "SD_direct",
    y_var = "SD",
    colour_var = "estimate",
    x_title_glue = "SE in direct estimates",
    y_title_glue = "MFH SE",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```

### Linearity Test

To assess whether a linear regression model may be incorrectly specified — for instance, due to omitted variables or incorrect functional form — we can use Ramsey’s Regression Equation Specification Error Test (RESET). This test examines whether adding nonlinear combinations (typically powers) of the model’s fitted values significantly improves the model. However, the outcome of interest now is the model MSEs. A significant test result (low p-value) suggests the model is mis-specified and may benefit from additional or transformed predictors.

We implement Ramsey’s RESET test in R using the resettest() function from the lmtest package:

```{r lin-test}
#| echo: false
### lets create a dataframe with the errors and estimated poverty rates

pov_mfh_dir |> 
  mutate(error = pov_direct - pov) |>
  fct_plot_scatter(
    x_var = "pov_direct",
    y_var = "error",
    colour_var = "estimate",
    x_title_glue = "Predicted poverty rate (MFH)",
    y_title_glue = "Error terms",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_number(0.0001)
  ) +
  facet_wrap(. ~ year, scales = "free")
  
eblup_dt <- model2_obj$eblup
mse_dt <- model2_obj$MSE

colnames(eblup_dt) <- paste0("eblup_", colnames(eblup_dt))
colnames(mse_dt) <- paste0("mse_", colnames(mse_dt))

reset_dt <- bind_cols(eblup_dt, mse_dt) |> as_tibble()

### lets perform the reset test on the pairs of variables as appropriate i.e. poverty = B0 + B1*MSE

test_list <-
  lapply(year_set, function(x) {

    # Subset only the columns for year x
    dt <- reset_dt |>
      dplyr::select(matches(paste0(x, "$")))  # Select columns ending with current year

    yvar <- colnames(dt)[grepl("^eblup_", colnames(dt))]
    xvar <- colnames(dt)[grepl("^mse_", colnames(dt))]

    # Create formula with squared and cubed terms using I()
    form <- as.formula(paste0(
      xvar, " ~ ",
      yvar, " + I(", yvar, "^2)"
    ))

    model_obj <- lm(form, data = dt)

    return(model_obj)

  })
# 
# reset_dt2 <- 
#   reset_dt |> 
#   mutate(id = row_number()) |> 
#   pivot_longer(c(contains("eblup"), contains("mse"))) |> 
#   mutate(
#     year = str_extract(name, "\\d{4}") |> as.numeric(),
#     var =  str_extract(name, "eblup|mse")
#   ) |> 
#   dplyr::select(-name) |> 
#   pivot_wider(names_from = var, values_from = value )
#   
# reset_dt2 |> 
#   ggplot() + 
#   aes(x = eblup, y = mse) + 
#   geom_point() + 
#   facet_wrap(. ~ year) + 
#   geom_smooth() + 
#   theme_bw()

```

Here is what the results look like:

```{r}
#| echo: false
test_list %>% lapply(X = ., FUN = summary)
```

The results particularly for the final 2 years indicate that we might need to retransform the variables in the model as there are potentially non linear relationships which our model is not capturing. Perhaps creating higher order polynomials and other variable transformations of our right hand side variables reduce the error rates within the model.

### Evaluating the Normality Assumption

#### The Shapiro Wilks Test

We use the shapiro wilks test of normality using the `shapiro.test()` function in base R. The Shapiro-Wilk test assesses whether a sample of data is drawn from a normally distributed population. It does so by comparing the order statistics (i.e., sorted values) of the sample to the expected values under a normal distribution. Specifically, the test statistic $W$ is a ratio of the squared correlation between the observed sample quantiles and the corresponding normal quantiles.

First, we perform the shapiro wilks normality test on the model errors, $\varepsilon$. We show both the normality distribution histogram as well as the qqplots as below:

```{r sw-test}
#| echo: false


### first lets replace the negative values with 0
eblup_dt[eblup_dt < 0] <- 0

### evaluating the normality assumption

#### first lets create a residual table by looking at the difference between actual and predicted poverty rates
resid_dt <- mfh_dt[,paste0("pov", year_set)] - eblup_dt

### perform the shapiro test

shapiro_obj <- apply(resid_dt, 2, shapiro.test)

summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

resid_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "Residual") %>%
  ggplot(aes(x = Residual)) + 
  geom_histogram(bins = 10, fill = "steelblue", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Residual Histograms by Time Period")


### here's how to create qqplots
resid_dt %>%
  pivot_longer(cols = everything(),
               names_to = "Time",
               values_to = "Residual") %>%
  ggplot(aes(sample = Residual)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Time, scales = "free") +
  theme_minimal() +
  labs(title = "QQ Plots of Residuals by Time Period")

```

Likewise, we test the normality of the random effect variable

```{r}
#| echo: true

#### For the random effects
raneff_dt <- as.data.frame(model2_obj$randomEffect)

### lets run the shapiro wilks tests again
shapiro_obj <- apply(raneff_dt, 2, shapiro.test)


summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

raneff_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "RandEff") %>%
  ggplot(aes(x = RandEff)) + 
  geom_histogram(bins = 10, fill = "darkorange", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Random Effects Histograms by Time Period")
```

In both cases, we compare the p-value to the 0.05 level of significance. The results suggest that in most cases we have to reject the null hypothesis of normally distributed model errors and random effects. This doesn't affect the validity of our poverty estimates for the Multivariate Fay Herriot model. However, subsequent analysis that will assume a normal distribution of the model errors cannot be performed. One good example of this is the statistical significance test for changes in poverty rates over time. For the purposes of this tutorial, we will carry on to show how to perform this under the assumption of normally distributed model errors. However, if the test for normality fails, this test cannot be carried out.

Next, we will show the benefits of small area estimation over direct estimation

### Comparing Direct Estimation to Multivariate Model Outputs

```{r, warning=FALSE, message = FALSE, error = FALSE}
#| echo: true

pov_mfh_dir_long <- 
  pov_mfh |> 
  bind_rows(pov_direct |>mutate(estimate = "Direct")) |> 
  left_join(shp_dt) |> 
  st_as_sf()

pov_cv_year_dta <-
  pov_mfh_dir_long |> 
  mutate(name = str_remove(name, "PODREGION") |> str_trim()) |>  
  st_drop_geometry() |> 
  arrange(year) |> 
  mutate(
    group_id = ifelse(year == 2011, 1, 2) |> str_c(estimate),
    year = as_factor(year)) 

pov_cv_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "CV",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "Coefficient of variance",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_number(0.001),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-cv-years-regions.png", 
       plot = pov_cv_year, width = 15, height = 12, dpi = 200, scale = 1)


pov_pov_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "pov",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "Poverty rate",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_percent(),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-pov-years-regions.png", 
       plot = pov_pov_year, width = 15, height = 12, dpi = 200, scale = 1)

pov_se_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "SD",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "SE",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_number(0.001),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-se-years-regions.png", 
       plot = pov_se_year, width = 15, height = 12, dpi = 200, scale = 1)
```

![CV across years by region](output/pov-mhf-cv-years-regions.png){#fig-reg-cv}

![Poverty across years by region](output/pov-mhf-pov-years-regions.png){#fig-reg-pov}

![SE across years by region](output/pov-mhf-se-years-regions.png){#fig-reg-se}

## Poverty maps

```{r}
pov_plots <-
  pov_cv_year_dta |>
  select(-name) |> 
  left_join(shp_dt) |> 
  st_as_sf() |> 
  group_by(year2 = year, type2 = estimate) |>
  nest() |> 
  ungroup()  |>  
  (\(x) {
    expand_grid(year2 = x$year2, type2 = x$type2) |> 
      distinct() |> 
      left_join(x, by = join_by(year2, type2))
  })() |> 
  arrange(year2, type2) |> 
  mutate(plot_poverty =
           map(data, 
               ~ {
                 # browser()
                 
                 fct_map_chreplot(.x, var_fill = "pov",
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "Poverty",
                                  force_breaks = c(0, .1, .15, .2, .25, .3, .35, .4, 0.55)
                                  )}
               )) |> 
  mutate(plot_SE =
           map(data, 
               ~ fct_map_chreplot(.x, var_fill = "SD", 
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "SE",
                                  force_breaks = c(.005, .02, .03, .035, 
                                                   .04, 0.055, 0.08, .15 ),
                                  label_form = label_number(0.001) 
                                  )))|> 
  mutate(plot_CV =
           map(data, 
               ~ fct_map_chreplot(.x, var_fill = "CV", 
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "CV",
                                  force_breaks = c(.1, .2, .25, .3, .35, 
                                                   0.4, .45, .5, 0.65),
                                  label_form = label_number(0.01) 
                                        )))

```

```{r}
plt1 <-
  pov_plots |> 
  filter(type2 == "MFH-2") |> 
  pull(plot_poverty) |>
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect") 

ggsave(
    filename = "output/povmap_mfh2_pov.png",
    width = 15, height = 8, dpi = 300,
    plot = plt1)


plt2 <-
  pov_plots |> 
  filter(type2 == "MFH-2") |> 
  pull(plot_CV) |>
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect") 

ggsave(
    filename = "output/povmap_mfh2_cv.png",
    width = 15, height = 8, dpi = 300,
    plot = plt2)
```

![MHF2 Poverty map](output/povmap_mfh2_pov.png){#fig-mfh2-pov}

![MHF2 CV](output/povmap_mfh2_pov.png){#fig-mfh2-cv}

## Spatio-temporal trends in poverty by region

```{r figcalc-tends}
library(trend)
library(broom)

pov_trend_dta <- 
  pov_cv_year_dta |> 
  # st_drop_geometry() |> 
  filter(year != 2011) |> 
  group_by(id, type, estimate) |> 
  nest() |> 
  # mutate(mkt = map(data, ~{mk.test(.x$pov) |> glance()})) |> 
  mutate(sen = map(data, ~{
    # browser()
    test_stat <- sens.slope(.x$pov) 
    tibble(sen_slope = test_stat$estimates,
           sen_pvalue = test_stat$p.value)
    
    })) |> 
  ungroup() |> 
  unnest(c(sen)) |> 
  left_join(shp_dt |> 
              select(id, name) |> 
              group_by(id) |> 
              filter(row_number() == 1) |> 
              ungroup()) |> 
  mutate(name = str_remove(name, "PODREGION") |> str_trim()) |>  
  st_as_sf()  |>
  group_by(type2 = estimate) |>
  nest() |>
  mutate(
    trend_p = map(data, ~ {
      .x |>
        fct_map_chreplot(
          var_fill = "sen_pvalue",
          subtitle_glue = "P-value: {estimate}",
          legend_glue = "P-value",
          label_form = label_number(0.01),
          force_breaks = c(0, 0.01, 0.05, 0.1, 1)
        )
    }),
    trend_slope = map(data, ~ {
      .x |>
        fct_map_chreplot(
          var_fill = "sen_slope",
          subtitle_glue = "Slope: {estimate}",
          legend_glue = "Slope",
          label_form = label_number(0.0001),
          force_breaks = c(-0.03, -0.015, -.01, -.005, 0, 0.005, 0.01, 0.025, 0.04)
        )
    })) 


on_dta <- 
  pov_trend_dta |> 
  filter(type2 == "MFH-2") 
  
plt <- 
  pov_trend_dta$trend_p |> 
  append(pov_trend_dta$trend_slope) |> 
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect")

ggsave(
    filename = "output/pov_mhf-trends.png",
    width = 15, height = 13, dpi = 300,
    plot = plt)
```

![SE across years by region](output/pov_mhf-trends.png){#fig-reg-time}
