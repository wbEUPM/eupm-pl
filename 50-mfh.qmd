---
title: "The Multivariate Fay Herriot"
author: "Ifeanyi Edochie"
format: html
editor: source
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false
#| error: false

# Chunk setup
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

pacman::p_load(
  sf, 
  data.table, 
  car, 
  msae,
  sae, 
  survey, 
  spdep,
  knitr, 
  MASS, 
  caret,
  purrr,
  pins,
  gt,
  lmtest, 
  scales,
  viridis, 
  patchwork,
  dplyr, 
  tictoc,
  gt,
  matrixcalc,
  tidyverse,
  modelsummary,
  flextable
  )

# Loading locally-developed
list.files("R", pattern = "*.R$", full.names = TRUE, ignore.case = TRUE) |>
  walk(~ suppressMessages(source(.x)))

# Raw data root
root_raw <- "./data/raw"
root_temp <- "./data/temp"
root_clean <- "./data/clean"

# Data-storage boards
bd_raw <- root_raw |> file.path("api") |> board_folder(versioned = T)
bd_aux <- root_temp |> board_folder(versioned = T)
bd_clean <- root_clean |> board_folder(versioned = T)
bd_out <- "output/res-50-mfh" |> board_folder(versioned = T)
```

# Stable FH Estimators over T time periods: The Multivariate Fay Herriot Modelling Approach

This section describes procedures that yield stable small area estimators for each of $D$ areas over $T$ subsequent time instants. Area populations, the samples and the data might change between time periods. Accordingly, we denote $U_t$ the overall population at time $t$, which is partitioned into $D$ areas $U_{1t}, ... ,U_{Dt}$, of respective population sizes $N_{1t}$

An overview of the MFH model estimation process is as follows:

-   Step 0: The data preparation phase in which we prepare the 3 data objects needed for the small area estimation under the Multivariate Fay Herriot Model

-   Step 1: Compute the selected direct area estimators for each target area $d = 1, ..., D$ for each time $t = 1, ..., T$ and estimators of their corresponding sampling variances and covariances.

-   Step 2: Prepare variables (from administrative data or other sources of data representative at the target area level) at the level of the target area for each time instant in the MFH model. We present a simple approach which performs model selection in a pooled linear regression model without time effects.

-   Step 3: Fit the MFH models to test for homoskedastic area-time effects $({\mu_d}_1, ... , {\mu_d}_T)$ are homoskedastic or not. If we reject the homoskedasticity of variances, implement the MFH3 model. Otherwise, we proceed with the MFH2 model. For the purposes of this training, we assume a homoskedastic model for simplicity.

-   Step 4: Check the selected model assumptions, including linearity, normality of predicted area effects and standardized model residuals, and the presence of the outlying areas.

-   Step 5: In case of systematic model departures such as isolated departures because of outlying areas, some adjustments might need to be implemented before returning to Step 2 to recompute the MFH model.

-   Step 6: If model assumptions hold, using the above direct estimates and estimated sampling variances and covariances, and the selected auxiliary variables, compute MFH estimators $\hat{\delta}_{dt}^{MFH},\quad d = 1,...,D$ and $t = 1, ..., T$ and their corresponding estimated MSEs.

We will show below the use of the `eblupMFH2()` and `eblupMFH3()` from the R package msae [@permatasari2022package] compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are:

`eblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

`eblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

## MFH Estimation of Poverty Rates for T time periods

In this example, we use a synthetic data set adapted from R package `sae` called `incomedata`. The original data contains information for $n = 17,119$ fictitious individuals residing across $D = 52$ Spanish provinces. The variables include the name of the province of residence (`provlab`), province code (`prov`), as well as several correlates of income. We have added two additional income vectors corresponding to two additional years of data.

We will show how to estimate a poverty map for each year by using the Multivariate Fay Herriot modelling approach. This approach allows us to take advantage of the temporal correlation between poverty rates i.e. an individuals income in year $t$ is likely correlated with their income in year $t+1$.

The rest of this tutorial shows how to prepare MFH models using a random 10% sample of the `incomedata` to estimate the poverty rates.

## Step 0: The Data Preparation Phase

The MFH estimation process relies on 3 types of data:

(1) Direct estimates of poverty and its variance: `direct_dt`

(2) Right hand side (RHS) variables i.e. indicators representative at the level of the target area for each year. This is often obtained from administrative data sources or geospatial data such as remotely sensed high resolution data. The final RHS dataset ought to include an area id, year and variables of interest. In this example, we call this `rhs_dt`

(3) Finally, geospatial boundaries of each target area: `shp_dt`

```{r dta-load}
#| echo: false

# Direct estimates load 
direct_dt <- bd_clean |> pin_read("pov_direct") |> 
  rename(id = subcode) |> 
  filter(type == "arop")

# RHS variables
widerhs_dt <-
  paste0(root_raw, "/data-other", "/auxiliary-variables.xlsx") |>
  readxl::read_excel() |> 
  clean_rhs() |>
  rename(id = ID) |> 
  select(-No., -Subregion)

rhs_vars <- paste0(root_raw, "/data-other", "/auxiliary-variables.xlsx") |>
  readxl::read_excel(sheet = 2) |> 
  select(var = Variable, category = `Category of variable` , name = `Variable (in English)`)

# Geometries
shp_dt <- bd_clean |> pin_read("geometries")
shp_dt <- shp_dt$level4
geoms <-  bd_clean |> pin_read("geometries")
geom_nuts3 <- geoms$level4 |> rename(subcode = id)
geom_nuts2 <- geoms$level2

# Key variable names
area_vars <- c("id") 
year_var <- "year"
directpov <- "pov"
directvar <- "vardir"
year_set <- c(2019:2023)
```

Data structure:

```{r}
#| eval: false
direct_dt |> glimpse()
```

`direct_dt` are the direct poverty estimates.

```{r}
#| eval: false
widerhs_dt |> glimpse()
```

`rhs_dt` is an object of class `data.frame` created at the level of the target area. It should contain the same target area identifiers as in `survey_dt` and the year variable `year`.

```{r}
#| eval: false
# shp_dt |> glimpse()
```

`shp_dt` is an object of class `sf`, `data.frame` created at the level of the target area. This is the shapefile for the area of interest for which the poverty map will be estimated. It should contain the same target area ID found in `survey_dt` as well as `rhs_dt`.

## Step 2: Variable Preparation and Model Selection

### Data Preparation for Model Selection & MFH estimation

Thus far, we have careful set up the types of data we require for the MFH model. One final step of variable preparation is necessary to use the `eblupUFH` and `eblupMFH` functions. This requires that we reshape the set of candidate variables dataset i.e. the `rhs_dt` object into wide format. We will also have to reshape our direct estimates and merge this into `rhs_dt` as well as `var_dt`. All the data we have created thus far needs to be placed together ultimately for the EBLUP estimation.

```{r}
#| echo: false
mfh_dt0 <- 
  direct_dt |>
  select(id, year, all_of(c(year_var, directpov, directvar))) |> 
  pivot_wider(
    id_cols = area_vars,
    names_from = all_of(year_var),
    values_from = all_of(c(directpov, directvar)),
    names_glue = "{.value}{year}",
    values_fn = ~ dplyr::first(na.omit(.x))
  ) |>
  left_join(widerhs_dt, by = area_vars)
```

Here is what the data looks like:

```{r}
#| echo: false
#| eval: false
mfh_dt0 |> glimpse()
```

### Variable Preparation

```{r var-preparation}
# Identify the columns of interest
x_vars <- mfh_dt0 |> select(matches("^X\\d+y\\d+")) |> names()
x_vars_numeric <- x_vars[sapply(mfh_dt0[x_vars], is.numeric)]

# Function to min-max scale
min_max_scale <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

# Mutate log and scaled versions
mfh_dt <-
  mfh_dt0 %>%
  mutate(
    # Add log-transformed columns
    across(all_of(x_vars_numeric), .fns = ~ log1p(.), .names = "log{.col}"),
    
    # Add min-max scaled columns
    across(all_of(x_vars_numeric), .fns = ~ min_max_scale(.), .names = "scaled{.col}")
  )
```

### Variable Selection

Next, we apply a simple variable selection process which employs the stepwise regression algorithm using the AIC selection criteria as in described by [@yamashita2007stepwise]. The function `step_wrapper()` implemented below is a wrapper to the `stepAIC()` function carries all the perfunctory cleaning necessary use the `stepAIC()` function. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations and remove perfectly or near collinear variables and combinations using the variance inflation method.

```{r var-selection}
#| echo: false
candidate_vars <- c(#x_vars_numeric,
                    paste0("log", x_vars_numeric)#,
                    # paste0("scaled", x_vars_numeric)
                    )

bad_vars <- candidate_vars[sapply(mfh_dt[candidate_vars], function(col) any(!is.finite(col)))]
candidate_vars <- candidate_vars[!candidate_vars %in% bad_vars]


fh_step <-
  year_set |>
  map( ~ {
    year_local <- .x |> as.character()
    year_local_lag <- as.character(as.numeric(year_local) - 1)
    candidate_vars_local <- candidate_vars[str_detect(candidate_vars, year_local_lag)]
    dta_local <- mfh_dt |> as_tibble() |> 
      dplyr::select(contains("pov"), contains(year_local_lag))
    y_local <- mfh_dt |> dplyr::select(contains("pov")) |> 
      dplyr::select(contains(year_local)) |> names()
    # browser()
    model_obj <-
      step_wrapper(
        dt = dta_local,
        xvars = candidate_vars_local,
        y = y_local,
        cor_thresh = 0.8,
        k = 2,
        trace = 0
      ) ### using log(n) to force BIC selection
    
    xx <- names(model_obj$coefficients)[!grepl("(Intercept)", names(model_obj$coefficients))]
    return(xx)
  })


# Set of formulas for the Multivariate FH
mfh_formula <-
  mapply(FUN = function(x, rhs){
    y <- as.formula(paste0(x, " ~ ", paste(rhs, collapse = " + ")))
    return(y)
  },
  x = paste0(directpov, year_set),
  rhs = fh_step,
  SIMPLIFY = FALSE)

### here is what the 3 equations look like
mfh_formula |> 
  map(~lm(.x, data = mfh_dt)) |> 
  modelsummary(
    output = "flextable", 
    estimate = "{estimate}{stars} ({std.error})", 
    statistic = NULL,
    coef_rename = function(x) {
      x |>
        str_remove_all("y\\d{4}") |> 
        map_chr( ~ {
          match_coef <- rhs_vars |> filter(var == str_replace(.x, "log", "")) |> pull(name)
          # browser()
          if (is.null(match_coef) | length(match_coef) == 0) {
            return(.x)
          } else {
            return(match_coef)
          }
        })
      
    }
  ) |> 
  autofit() |> 
  width(j = 1, width = 3)
```

Notes: Significance levels are: `***` p-value < 0.001, `***` p-value < 0.01, `*` p-value < 0.05, `+` p-value < 0.1 

## Step 3: Fitting the Multivariate Fay Herriot Model

Next, we show how to use the `msae` R package to estimate the Empirical Best Linear Unbiased Predictor (EBLUP) for the poverty map using the `eblupMFH2()` which allow for time series fay herriot estimation under homoskedastic assumptions. For completeness, we also briefly perform the previous described direct estimation in step 1, using the `eblupUFH()` function as well as the `eblupMFH1()` for the fay herriot model.

```{r mvfh-est}
#| echo: false
#| eval: false
prepare_vcov <- function(){
  vc <- paste0(directvar, year_set)
  
  # Generate all unique pairs (upper triangle only, including diagonal)
  all_pairs <- t(combn(vc, 2))
  colnames(all_pairs) <- c("var1", "var2")
  
  # Convert to data.frame (this contains only the covariances)
  covariances <- as.data.frame(all_pairs, 
                               stringsAsFactors = FALSE)  
  
  covariances <- 
    covariances |>
    mutate(fn = paste0(var1, var2))
  
  return(covariances$fn)
  
}

mfh_dt[, prepare_vcov()] <- 0
varcols <- c(paste0(directvar, year_set), prepare_vcov())
yvars <- paste0(directpov, year_set)

# Univariate FH
model0_obj <- eblupUFH(mfh_formula[yvars], vardir = varcols, data = mfh_dt)
bd_out |> pin_write(model0_obj, type = "rds")

# multivariate FH 1
tic(msg = "eblupMFH1")
model1_obj <- eblupMFH1(mfh_formula[yvars], 
                        vardir = varcols, 
                        data = mfh_dt, 
                        MAXITER = 10000, 
                        PRECISION = 0.001)
bd_out |> pin_write(model1_obj, type = "rds")
toc()

# multivariate FH 2
tic(msg = "eblupMFH2")
model2_obj <- eblupMFH2(mfh_formula[yvars], 
                        vardir = varcols, 
                        data = mfh_dt, 
                        MAXITER = 10000, 
                        PRECISION = 0.001)
bd_out |> pin_write(model2_obj, type = "rds")
toc()

```

```{r mvfh-load}
model0_obj <- bd_out |> pin_read("model0_obj")
model1_obj <- bd_out |> pin_read("model1_obj")
model2_obj <- bd_out |> pin_read("model2_obj")
```


Saving results of the models

```{r res-save}
# model0_obj
ext_mfh_dta <- function(fit, ids, type = "arop", estimate = "UFH") {
  # ids <- mfh_dt |> select(id)
  eblup_dt <- fit$eblup |> as_tibble()
  mse_dt <- fit$MSE |> as_tibble() |> rename_with( ~ str_replace(., "pov", "vardir"))
  
  bind_cols(ids, eblup_dt, mse_dt) |>
    pivot_longer(c(contains("pov"), contains("vardir"))) |>
    mutate(year = str_extract(name, "\\d{4}") |> as.integer(),
           name = str_remove(name, "\\d{4}")) |>
    pivot_wider(values_from = value, names_from = name) |>
    mutate(
      CV  = sqrt(vardir) / pov,
      SD = sqrt(vardir),
      type = type,
      estimate = estimate
    )
}

pov_mfh <- 
  # ext_mfh_dta(model0_obj,  select(mfh_dt, id), type = "arop", estimate = "UFH (MFH)") |> 
  bind_rows(
    ext_mfh_dta(model1_obj,  select(mfh_dt, id), type = "arop", estimate = "MFH-1")
  ) |> 
  bind_rows(
    ext_mfh_dta(model2_obj,  select(mfh_dt, id), type = "arop", estimate = "MFH-2")
  )
  
bd_clean |> pin_write(pov_mfh, name = "pov_mfh", type = "rds")
```


## Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers

We now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas.

### SE vs Poverty

```{r}
pov_mfh |> 
  bind_rows(direct_dt |> mutate(estimate = "Direct")) |> 
  ggplot() + 
  aes(y = SD, x = pov, colour = estimate ) + 
  geom_point() + 
  geom_smooth() +
  facet_wrap(. ~ year, scales = "free") + 
  scale_y_continuous("SE") + 
  theme_bw()
```

### CV vs Poverty

```{r}
pov_mfh |> 
  bind_rows(direct_dt |> mutate(estimate = "Direct")) |> 
  ggplot() + 
  aes(y = CV, x = pov, colour = estimate ) + 
  geom_point() + 
  geom_smooth() +
  facet_wrap(. ~ year, scales = "free") + 
  scale_y_continuous("CV") + 
  theme_bw()
```

### Direct vs MFH

#### Poverty rate

```{r}
library(glue)
pov_mfh_dir <- 
  pov_mfh |> 
  left_join(direct_dt |>
              select(
                id,
                year,
                pov_direct = pov,
                SD_direct = SD,
                CV_direct = CV
              ),
            by = join_by(id, year)
            ) |> 
  left_join(shp_dt) |> 
  st_as_sf()

pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "pov_direct",
    y_var = "pov",
    colour_var = "estimate",
    x_title_glue = "Poverty rate in direct estimates",
    y_title_glue = "MFH poverty rate",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```

#### CV

```{r}
pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "CV_direct",
    y_var = "CV",
    colour_var = "estimate",
    x_title_glue = "CV in direct estimates",
    y_title_glue = "MFH CV",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```


#### SE

```{r}
pov_mfh_dir |>
  fct_plot_scatter(
    x_var = "SD_direct",
    y_var = "SD",
    colour_var = "estimate",
    x_title_glue = "SE in direct estimates",
    y_title_glue = "MFH SE",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_percent()
  ) +
  facet_wrap(. ~ year, scales = "free")
```


### Linearity Test

To assess whether a linear regression model may be incorrectly specified — for instance, due to omitted variables or incorrect functional form — we can use Ramsey’s Regression Equation Specification Error Test (RESET). This test examines whether adding nonlinear combinations (typically powers) of the model’s fitted values significantly improves the model. However, the outcome of interest now is the model MSEs. A significant test result (low p-value) suggests the model is mis-specified and may benefit from additional or transformed predictors.

We implement Ramsey’s RESET test in R using the resettest() function from the lmtest package:

```{r lin-test}
#| echo: false
### lets create a dataframe with the errors and estimated poverty rates

pov_mfh_dir |> 
  mutate(error = pov_direct - pov) |>
  fct_plot_scatter(
    x_var = "pov_direct",
    y_var = "error",
    colour_var = "estimate",
    x_title_glue = "Predicted poverty rate (MFH)",
    y_title_glue = "Error terms",
    title_glue = "",
    subtitile_glue = "",
    legendtitle_glue = "Estimate",
    scale_x_label = label_number(0.0001)
  ) +
  facet_wrap(. ~ year, scales = "free")
  
eblup_dt <- model2_obj$eblup
mse_dt <- model2_obj$MSE

colnames(eblup_dt) <- paste0("eblup_", colnames(eblup_dt))
colnames(mse_dt) <- paste0("mse_", colnames(mse_dt))

reset_dt <- bind_cols(eblup_dt, mse_dt) |> as_tibble()

### lets perform the reset test on the pairs of variables as appropriate i.e. poverty = B0 + B1*MSE

test_list <-
  lapply(year_set, function(x) {

    # Subset only the columns for year x
    dt <- reset_dt |>
      dplyr::select(matches(paste0(x, "$")))  # Select columns ending with current year

    yvar <- colnames(dt)[grepl("^eblup_", colnames(dt))]
    xvar <- colnames(dt)[grepl("^mse_", colnames(dt))]

    # Create formula with squared and cubed terms using I()
    form <- as.formula(paste0(
      xvar, " ~ ",
      yvar, " + I(", yvar, "^2)"
    ))

    model_obj <- lm(form, data = dt)

    return(model_obj)

  })
# 
# reset_dt2 <- 
#   reset_dt |> 
#   mutate(id = row_number()) |> 
#   pivot_longer(c(contains("eblup"), contains("mse"))) |> 
#   mutate(
#     year = str_extract(name, "\\d{4}") |> as.numeric(),
#     var =  str_extract(name, "eblup|mse")
#   ) |> 
#   dplyr::select(-name) |> 
#   pivot_wider(names_from = var, values_from = value )
#   
# reset_dt2 |> 
#   ggplot() + 
#   aes(x = eblup, y = mse) + 
#   geom_point() + 
#   facet_wrap(. ~ year) + 
#   geom_smooth() + 
#   theme_bw()

```

Here is what the results look like:

```{r}
#| echo: false
test_list %>% lapply(X = ., FUN = summary)
```

The results particularly for the final 2 years indicate that we might need to retransform the variables in the model as there are potentially non linear relationships which our model is not capturing. Perhaps creating higher order polynomials and other variable transformations of our right hand side variables reduce the error rates within the model.

### Evaluating the Normality Assumption

#### The Shapiro Wilks Test

We use the shapiro wilks test of normality using the `shapiro.test()` function in base R. The Shapiro-Wilk test assesses whether a sample of data is drawn from a normally distributed population. It does so by comparing the order statistics (i.e., sorted values) of the sample to the expected values under a normal distribution. Specifically, the test statistic $W$ is a ratio of the squared correlation between the observed sample quantiles and the corresponding normal quantiles.

First, we perform the shapiro wilks normality test on the model errors, $\varepsilon$. We show both the normality distribution histogram as well as the qqplots as below:

```{r sw-test}
#| echo: false


### first lets replace the negative values with 0
eblup_dt[eblup_dt < 0] <- 0

### evaluating the normality assumption

#### first lets create a residual table by looking at the difference between actual and predicted poverty rates
resid_dt <- mfh_dt[,paste0("pov", year_set)] - eblup_dt

### perform the shapiro test

shapiro_obj <- apply(resid_dt, 2, shapiro.test)

summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

resid_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "Residual") %>%
  ggplot(aes(x = Residual)) + 
  geom_histogram(bins = 10, fill = "steelblue", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Residual Histograms by Time Period")


### here's how to create qqplots
resid_dt %>%
  pivot_longer(cols = everything(),
               names_to = "Time",
               values_to = "Residual") %>%
  ggplot(aes(sample = Residual)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Time, scales = "free") +
  theme_minimal() +
  labs(title = "QQ Plots of Residuals by Time Period")

```

Likewise, we test the normality of the random effect variable

```{r}
#| echo: true

#### For the random effects
raneff_dt <- as.data.frame(model2_obj$randomEffect)

### lets run the shapiro wilks tests again
shapiro_obj <- apply(raneff_dt, 2, shapiro.test)


summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

raneff_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "RandEff") %>%
  ggplot(aes(x = RandEff)) + 
  geom_histogram(bins = 10, fill = "darkorange", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Random Effects Histograms by Time Period")
```

In both cases, we compare the p-value to the 0.05 level of significance. The results suggest that in most cases we have to reject the null hypothesis of normally distributed model errors and random effects. This doesn't affect the validity of our poverty estimates for the Multivariate Fay Herriot model. However, subsequent analysis that will assume a normal distribution of the model errors cannot be performed. One good example of this is the statistical significance test for changes in poverty rates over time. For the purposes of this tutorial, we will carry on to show how to perform this under the assumption of normally distributed model errors. However, if the test for normality fails, this test cannot be carried out.

Next, we will show the benefits of small area estimation over direct estimation

### Comparing Direct Estimation to Multivariate Model Outputs

```{r, warning=FALSE, message = FALSE, error = FALSE}
#| echo: true

pov_mfh_dir_long <- 
  pov_mfh |> 
  bind_rows(direct_dt |>mutate(estimate = "Direct")) |> 
  left_join(shp_dt) |> 
  st_as_sf()

pov_cv_year_dta <-
  pov_mfh_dir_long |> 
  mutate(name = str_remove(name, "PODREGION") |> str_trim()) |>  
  st_drop_geometry() |> 
  arrange(year) |> 
  mutate(
    group_id = ifelse(year == 2011, 1, 2) |> str_c(estimate),
    year = as_factor(year)) 

pov_cv_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "CV",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "Coefficient of variance",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_number(0.001),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-cv-years-regions.png", 
       plot = pov_cv_year, width = 15, height = 12, dpi = 200, scale = 1)


pov_pov_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "pov",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "Poverty rate",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_percent(),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-pov-years-regions.png", 
       plot = pov_pov_year, width = 15, height = 12, dpi = 200, scale = 1)

pov_se_year <-
  pov_cv_year_dta |>  
  fct_plot_scatter(
               x_var = "year",
               y_var = "SD",
               colour_var = "estimate",
               group_var = "group_id",
               x_title_glue = "Year",
               y_title_glue = "SE",
               title_glue = "",
               subtitile_glue = "",
               legendtitle_glue = "Estimate",
               scale_y_label = label_number(0.001),
               scale_x_label = function(x) x,
               expand_x = NULL, 
               expand_y = NULL, 
               add_smooth = F, 
               add_line = F
             ) + 
  facet_wrap(. ~ name) + 
  scale_x_discrete() +
  geom_line()

ggsave(filename = "output/pov-mhf-se-years-regions.png", 
       plot = pov_se_year, width = 15, height = 12, dpi = 200, scale = 1)
```

![CV across years by region](output/pov-mhf-cv-years-regions.png){#fig-reg-cv}

![Poverty across years by region](output/pov-mhf-pov-years-regions.png){#fig-reg-pov}

![SE across years by region](output/pov-mhf-se-years-regions.png){#fig-reg-se}

## Poverty maps

```{r}
pov_plots <-
  pov_cv_year_dta |>
  select(-name) |> 
  left_join(shp_dt) |> 
  st_as_sf() |> 
  group_by(year2 = year, type2 = estimate) |>
  nest() |> 
  ungroup()  |>  
  (\(x) {
    expand_grid(year2 = x$year2, type2 = x$type2) |> 
      distinct() |> 
      left_join(x, by = join_by(year2, type2))
  })() |> 
  arrange(year2, type2) |> 
  mutate(plot_poverty =
           map(data, 
               ~ {
                 # browser()
                 
                 fct_map_chreplot(.x, var_fill = "pov",
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "Poverty",
                                  force_breaks = c(0, .1, .15, .2, .25, .3, .35, .4, 0.55)
                                  )}
               )) |> 
  mutate(plot_SE =
           map(data, 
               ~ fct_map_chreplot(.x, var_fill = "SD", 
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "SE",
                                  force_breaks = c(.005, .02, .03, .035, 
                                                   .04, 0.055, 0.08, .15 ),
                                  label_form = label_number(0.001) 
                                  )))|> 
  mutate(plot_CV =
           map(data, 
               ~ fct_map_chreplot(.x, var_fill = "CV", 
                                  subtitle_glue = "{estimate}: {year}", 
                                  legend_glue = "CV",
                                  force_breaks = c(.1, .2, .25, .3, .35, 
                                                   0.4, .45, .5, 0.65),
                                  label_form = label_number(0.01) 
                                        )))

```

```{r}
plt1 <-
  pov_plots |> 
  filter(type2 == "MFH-2") |> 
  pull(plot_poverty) |>
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect") 

ggsave(
    filename = "output/povmap_mfh2_pov.png",
    width = 15, height = 8, dpi = 300,
    plot = plt1)


plt2 <-
  pov_plots |> 
  filter(type2 == "MFH-2") |> 
  pull(plot_CV) |>
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect") 

ggsave(
    filename = "output/povmap_mfh2_cv.png",
    width = 15, height = 8, dpi = 300,
    plot = plt2)
```

![MHF2 Poverty map](output/povmap_mfh2_pov.png){#fig-mfh2-pov}

![MHF2 CV](output/povmap_mfh2_pov.png){#fig-mfh2-cv}


## Spatio-temporal trends in poverty by region

```{r figcalc-tends}
library(trend)
library(broom)

pov_trend_dta <- 
  pov_cv_year_dta |> 
  # st_drop_geometry() |> 
  filter(year != 2011) |> 
  group_by(id, type, estimate) |> 
  nest() |> 
  # mutate(mkt = map(data, ~{mk.test(.x$pov) |> glance()})) |> 
  mutate(sen = map(data, ~{
    # browser()
    test_stat <- sens.slope(.x$pov) 
    tibble(sen_slope = test_stat$estimates,
           sen_pvalue = test_stat$p.value)
    
    })) |> 
  ungroup() |> 
  unnest(c(sen)) |> 
  left_join(shp_dt |> 
              select(id, name) |> 
              group_by(id) |> 
              filter(row_number() == 1) |> 
              ungroup()) |> 
  mutate(name = str_remove(name, "PODREGION") |> str_trim()) |>  
  st_as_sf()  |>
  group_by(type2 = estimate) |>
  nest() |>
  mutate(
    trend_p = map(data, ~ {
      .x |>
        fct_map_chreplot(
          var_fill = "sen_pvalue",
          subtitle_glue = "P-value: {estimate}",
          legend_glue = "P-value",
          label_form = label_number(0.01),
          force_breaks = c(0, 0.01, 0.05, 0.1, 1)
        )
    }),
    trend_slope = map(data, ~ {
      .x |>
        fct_map_chreplot(
          var_fill = "sen_slope",
          subtitle_glue = "Slope: {estimate}",
          legend_glue = "Slope",
          label_form = label_number(0.0001),
          force_breaks = c(-0.03, -0.015, -.01, -.005, 0, 0.005, 0.01, 0.025, 0.04)
        )
    })) 


on_dta <- 
  pov_trend_dta |> 
  filter(type2 == "MFH-2") 
  
plt <- 
  pov_trend_dta$trend_p |> 
  append(pov_trend_dta$trend_slope) |> 
  reduce( ~ .x + .y)  +
  patchwork::plot_layout(guides = "collect")

ggsave(
    filename = "output/pov_mhf-trends.png",
    width = 15, height = 13, dpi = 300,
    plot = plt)
```

![SE across years by region](output/pov_mhf-trends.png){#fig-reg-time}
