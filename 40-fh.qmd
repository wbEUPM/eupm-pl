---
title: "The Univariate Fay-Herriot (UFH) model"
author: "Sylvia Harmening"
format: html
editor: visual
bibliography: references.bib
---

```{r, echo = FALSE, include = FALSE, warning = FALSE, message=FALSE, error =  FALSE}
#| label: setup
#| echo: true
#| warning: false
#| message: false
#| error: false

# Chunk setup
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

pacman::p_load(
  sf, 
  data.table, 
  car, 
  msae,
  sae, 
  survey, 
  spdep,
  knitr, 
  MASS, 
  caret,
  purrr,
  pins,
  rlang,
  emdi,
  gt,
  lmtest, 
  scales,
  viridis, 
  patchwork,
  dplyr, 
  tictoc,
  gt,
  matrixcalc,
  tidyverse,
  modelsummary,
  flextable,
  correlation,
  datawizard
  )

# Loading locally-developed
list.files("R", pattern="*.R$", full.names=TRUE, ignore.case=TRUE) |> 
  walk(source)

# Raw data root
root_raw <- "./data/raw"
root_temp <- "./data/temp"
root_clean <- "./data/clean"

# Data-storage boards
bd_raw <- root_raw |> file.path("api") |> board_folder(versioned = T)
bd_aux <- root_temp |> board_folder(versioned = T)
bd_clean <- root_clean |> board_folder(versioned = T)
bd_out <- "output/res-40-fh" |> board_folder(versioned = T)

```

## Introduction

In this section, we will present a whole estimation procedure of the univariate area-level model introduced by @Fay1979 in R. As with the disclaimer in the preceding section, this practical manual is not intended to serve as a theoretical introduction to area-level models. Instead, it offers a set of straightforward and practical R scripts, accompanied by clear explanations, to demonstrate how these tasks can be carried out in R. For a theoretical foundation please refer to @Fay1979 and @RaoMolina2015. In addition to theoretical information, the vignette "A framework for producing small area estimates based on area-level models in R" of the R package `emdi` [@Harmening2023] provides further code examples for the FH model.

The estimation procedure is explained step by step.

Step 1: Data preparation. Compute the direct estimates and their corresponding variances on the area-level and eventually perform variance smoothing. Aggregate the available auxiliary variables to the same area level and combine both input data.

Step 2: Model selection. Select the aggregated auxiliary variables at the area level for the FH model using a stepwise selection based on information criteria like the Akaike, Bayesian or Kullback information criteria.

Step 3: Model estimation of FH point estimates and their mean squared error (MSE) estimates as uncertainty measure. Eventually apply a transformation.

Step 4: Assessment of the estimated model. Check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals. When violations of the model assumptions are detected, the application of a transformation might help. Repeat Step 3 including a transformation and check again the model assumptions.

Step 5: Comparison of the FH results with the direct estimates.

Step 6: Benchmark the FH point estimates for consistency with higher results.

Step 7: Preparation of the results. Create tables and maps of results.

Step 8: Saving the results. One option is to export the results to known formats like Excel or OpenDocument Spreadsheets.

We will show below the use of the `fh()` function of the R package `emdi` [@Harmening2023] which computes the EBLUPs and their MSE estimates of the standard FH model and several extensions of it, among others it allows for the application of transformations. Because the poverty rate is a ratio, it might be helpful to apply the arcsin transformation to guarantee that the results lie between 0 and 1. The function call is:

`fh(fixed, vardir, combined_data, domains = NULL, method = "reml", interval = NULL,   k = 1.345, mult_constant = 1, transformation = "no", backtransformation = NULL,   eff_smpsize = NULL, correlation = "no", corMatrix = NULL, Ci = NULL, tol = 1e-04,   maxit = 100, MSE = FALSE, mse_type = "analytical", B = c(50, 0), seed = 123)`

## Data and preparation

### Load the dataset

Usually, SAE combines multiple data sources: a survey data set and a census or administrative/register dataset. For the estimation of area-level models, we need area-level aggregates of the same area-level (e.g. NUTS3) of both datasets. The target variable (typically household welfare/income for poverty mapping) is available in the survey but not in the census data.

To ensure transparent data organization, we adopt a standard structure of the poverty estimates data produced in our analysis. Specifically, each estimation method produces data set of the following structure (@tbl-data-str).

```{r echo=FALSE}
#| label: tbl-data-str
#| tbl-cap: Structure of the data resulting form UFH and other poverty estimates
tribble(
  ~Variable, ~Description,
  "id", "Region ID",
  "year", "Year",
  "estimate", "Estimate type, e.g. UFH, MFH1, MHF2, MFH3, direct, etc.",
  "type", "Poverty measure type (dependent variabel), e.g. AROP, AROPE, AROP anchored, AROP real.",
  "pov", "Estimates of poverty",
  "SE", "Standard errors of the direct estimates",
  "vardir", "Variance (SE^2)",
  "CV", "Coefficient of variation (SE/pov)",
   "var_SRS", "Variance under simple random sampling",
   "deff", "Desgin effect: ratio of the variance considering the sampling design (vardir) to var_SRS",
   "n_eff", "Sample size of each area divided by the design effect (deff)",
  "(v_pov20XX", "Covariance between poverty in current year versus year 20XX (multiple columns))"
) |> 
  flextable() |> FitFlextableToPage()
```

For this example, we use actual data for Poland for 2019.

-   Direct estimates are prepared based on the confidential SILC data in the script `10-direct-estimates.qmd`.

-   Right-hand-side variables, are prepared by the NSO using the [API](https://api.stat.gov.pl/Home/BdlApi) to their sub-national data base accessible through the R package [`bdl`](https://cran.r-project.org/package=bdl), see the script `20-other-variables.qmd`.

-   Sub-national geometries are available with the [`bdl`](https://cran.r-project.org/package=bdl) package and are prepared in `05-geometries.qmd`.

**Note:** since we have multiple different types of poverty estimates, e.g. AROP, AROPE, AROP anchored, etc. we recommend creating separate scripts for preparing variable-specific poverty maps. This tutorial focuses exclusively on AROP poverty estimate.


The FH estimation process relies on 3 types of data: direct estimates, right hand side variables and subnational geometries.


### Direct estimates

Direct estimates of poverty and its variance, `pov_direct`. Its is structure as outlined in @tbl-data-str. Below we presents the structure of this data set.

```{r dta-load-direct}
pov_direct <- bd_clean |> pin_read("pov_direct") |> 
  rename(id = subcode) |> filter(type == "arop") |> 
  mutate(estimate = "Direct")
```

```{r dta-str-pov-direct}
pov_direct |> glimpse()
```

### Right hand side variables

Right hand side (RHS) variables i.e. indicators representative at the level of the target area for each area consist of two components:

-   Data present in the wide format, where each column represents an individual variable and row and observation for one area and one year. Stecifically columns are:

    -   `id` regional identifier used to match RHS variables with regions and poverty estimates

    -   `year` year for which observations are used.

    -   `x1`, `x2`, `x3`, ..., `xn` are the RHS variables described in the metadata.

-   Metadata table describes briefly each variable and provides a short, self-explaining name, used in regression tables.

```{r dta-load-rhs, echo = FALSE}
rhs_dta_0 <- 
  suppressMessages(file.path(root_raw, "data-other", "Auxiliary (potential) variables.xlsx") |>
      readxl::read_excel()) 

new_names <- 
  rhs_dta_0 |> slice(1) |> t() |> as.data.frame() |>
  rownames_to_column() |> as_tibble() |> 
  mutate(rowname = ifelse(str_detect(rowname, "\\.{2,}"), NA_character_, rowname)) |> 
  fill(rowname) |> 
  mutate(rowname = str_to_lower(rowname),
         rowname = ifelse(!is.na(V1), str_c(rowname,"__", V1), rowname)) |> 
  pull(rowname)

rhs_dta <- 
  rhs_dta_0 |> 
  slice(-1) |> 
  (\(x){colnames(x) <- new_names; x})() |> 
  select(-no., -subregion) |> 
  pivot_longer(
    cols = matches("\\d{4}"), 
    values_transform = \(x) suppressWarnings(as.numeric(x))
    ) |> 
  filter(!is.na(value)) |> 
  separate(name, into = c("var", "year")) |> 
  mutate(year = as.numeric(year)) |> 
  pivot_wider(names_from = "var", values_from = "value")


rhs_meta <-
  file.path(root_raw, "data-other", "Auxiliary (potential) variables.xlsx") |>
  readxl::read_excel(sheet = 2) |> 
  select(var = Variable,
         category = `Category of variable`,
         name = `Variable (in English)`) |> 
  mutate(var = str_to_lower(var))
```

```{r dta-str-rhs}
#| echo: true
rhs_dta |> select(1:10) |> glimpse()
rhs_meta |> glimpse()
```

### Geo-spatial boundaries

Geo-spatial boundaries of each target area are loaded. We use NUTS2 and NUTS3 boundaries for plotting. Column `id` is used to map polygons to areas.

```{r dta-load-gis}
#| echo: false
geoms <-  bd_clean |> pin_read("geometries")
geom_nuts3 <- geoms$level4 |> rename(subcode = id)
geom_nuts2 <- geoms$level2
```

Lastly, we define a set of key variables names used in the code below. 

```{r set-var-names}
#| echo: true
area_var <- c("id") 
year_var <- "year"
directpov <- "pov"
directvar <- "vardir"
# year_set <- c(2019:2023)
## For the univariate model, we only use a single year
singleyear <- "2019"

```


## Model selection

### Variables preparation

At this step we prepared in the exact form that enables its use in the `fh` function. This includes:

-   Dependent variable preparation in the wide format with year-specific poverty rates as columns and regions as rows. `id` column is used as a regional identifier.

-   RHS variables preparation and reshaping:

    -   Computing economically sound ratios, constructing complex variables and performing linear transformation

    -   Reshaping data to the wide format with variables names and years in columns and regions identified by `id` variable in rows.

-   RHS variables selection that maximize the predictive power.


#### RHS variables preparation

While computing the new variables, we need to make sure that we:

-   name them uniquely and appropriately;

-   record relevant metadata for them.

@tbl-rhs-descr-stats provides the names for the RHS variables after we computed additional ones.

```{r var-preparation, include=FALSE}
rhs_dta0 <- 
  rhs_dta |> 
  mutate(
    x101 = x4 / x3, # Share of Working Age pop in the region,
    x102 = x6 / (x3 / x42),
    x103 = x7 / (x3 / x42),
    x104 = x8 / (x3 / x42),
    x105 = x9 / (x3/10000),
    x106 = x12 / (x3 / x42),
    x118 = x12 / (x3 / x42),
    x107 = x14 / (x3 / x42),
    x108 = x23 / (x3/10000),
    x109 = x41 /  (x3/10000),
    x110 = x40 /  (x3/x42),
    x111 = x22 /  (x3/x42),
    x112 = x17 /  (x12),
    x113 = x18 /  (x12),
    x114 = x19 /  (x12),
    x115 = x13 /  (x12),
    x116 = x1 /  (x3 / 1000),
    x117 = x2 /  (x3 / 1000),
    x119 = x29 /  (x4 / 1000),
    x120 = x30 /  (x4 / 1000)
  ) |> 
  select(#-x1, -x2, -x3, 
         -x6, -x7, -x8, -x12, -x13,
         -x17, -x18, -x19, -x31, -x33)

rhs_meta_extra <- 
  tribble(
    ~var, ~name,
    "x101", "Workage pop share",
    "x102", "Sahre fam. poverty ben.",
    "x103", "Sahre fam. homelessness ben.",
    "x104", "Sahre fam. unemployment ben.",
    "x105", "Market places per 10000 person",
    "x106", "Dwelling appartment per family",
    "x107", "Dwelling alowance per family",
    "x108", "Pharmacies per 10000 person",
    "x109", "Care facil. resid. per 10000 person",
    "x110", "Childrent 0-3 per family",
    "x111", "Share of families with Large Fam Card",
    "x112", "Water, share dwel.",
    "x113", "Heating, share dwel.",
    "x114", "Gas, share dwel.",
    "x115", "Rooms per apt.",
    "x116", "Birth rate per 1000",
    "x117", "Deth rate per 1000",
    "x118", "Dwelling rooms per family",
    "x119", "Immigrants per 1000 working pop",
    "x120", "Emmigrants per 1000 working pop"
  )

rhs_meta_0 <- rhs_meta |> bind_rows(rhs_meta_extra)

# # Identify the columns of interest
# x_vars <- mfh_dt0 |> select(matches("^X\\d+y\\d+")) |> names()
# x_vars_numeric <- x_vars[sapply(mfh_dt0[x_vars], is.numeric)]
# 
# # Function to min-max scale
# min_max_scale <- function(x) {
#   rng <- range(x, na.rm = TRUE)
#   (x - rng[1]) / (rng[2] - rng[1])
# }
# 
# # Mutate log and scaled versions
# mfh_dt <-
#   mfh_dt0 %>%
#   mutate(
#     # Add log-transformed columns
#     across(all_of(x_vars_numeric), .fns = ~ log1p(.), .names = "log{.col}"),
#     
#     # Add min-max scaled columns
#     across(all_of(x_vars_numeric), .fns = ~ min_max_scale(.), .names = "scaled{.col}")
#   )
```

```{r tbl-rhs-descr-stats, echo = FALSE}
#| tbl-cap: Descriptive statistics of the RHS variables
rhs_dta0 |>
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  mutate(var_order = str_extract(var, "\\d{1,4}") |> as.numeric()) |>
  arrange(var_order) |>
  left_join(rhs_meta_0, by = join_by(var)) |>
  mutate(name = str_c(var, " ", name) |> as_factor()) |>
  distinct(name) |>
  flextable() |>
  set_header_labels(name = "Label") |>
  autofit() |>
  fit_to_width(max_width = 6)
```

After all variables are computed, we perform logarithm transformation of all continuous variables, where maximum is above 10, or difference between min and maximum exceeds 10 times.

```{r}
var_to_log <- 
  rhs_dta0 |>
  pivot_longer(contains("x"), names_to = "var", cols_vary = "slowest") |>
  group_by(year, var) |> 
  summarise(min = min(value), max = max(value)) |> 
  filter(min * 10 < max & max > 10 | max > 100) |> pull(var) |> unique()

rhs_dta1 <- 
  rhs_dta0 |>
  group_by(year) |>
  mutate(across(all_of(var_to_log), ~ log(.))) |> 
  ungroup()
```


### Variable selection


With the help of the `step()` function of package `emdi`, we perform a variable selection based on the chosen variable selection criterion and directly get the model with fewer variables. The function `step_wrapper()` implemented below is a wrapper to the `emdi::step()` function and performs all the perfunctory cleaning necessary to use `step()`. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations (for the model selection only the in-sample domains are used) and remove perfectly or near collinear variables and combinations.

We apply the function to select the variables. Required inputs: data set, character vector containing the set of auxiliary variables, name of y variable, a correlation threshold between 0 and 1, name of information criterion and name of direct variance variable. In case, a transformation should be applied, "arcsin" as transformation and name of variable that contains the effective sample size.

```{r remove_multicol, message = FALSE, warning = FALSE, results = "hide"}
## For the univariate model, we only use a single year
pov_direct_year <- pov_direct |>
  filter(year == singleyear)

rhs_dta1_year <- rhs_dta1 |>
  filter(year == as.character(as.numeric(singleyear) - 1))

## We merge the direct data to the rhs data. FH does not run if there is any missing value in the auxiliary variables, and therefore, any variable with missing value should be removed in advance.
dta_ufh <-
  rhs_dta1_year |> 
  left_join(pov_direct_year, by = join_by(!!area_var)) |> 
  select(-where(~any(is.na(.))))

candidate_vars <- dta_ufh |> select(contains("x")) |> names()

fh_step <- step_wrapper_fh(dt = dta_ufh,
                           xvars = candidate_vars,
                           y = "pov",
                           cor_thresh = 0.7,
                           criteria = "BIC",
                           vardir = "vardir", 
                           transformation = "arcsin", 
                           eff_smpsize = "n_eff")

```

```{r remove_multicol1, message = FALSE, warning = FALSE}
# Resulting model formula
print(fh_step$fixed)
```

## Model estimation of FH point and their MSE estimates.

In this example, we use the function `fh` to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1 [@Casas2016, @Schmid2017]. The arcsin transformation works variance stabilizing itself. When applying the arcsin transformation, the direct variances are automatically set to 1/(4\*effective sampling size) when using the `fh` function of package `emdi`. (If the variance stabilizing effect is not enough, the design effect of a higher area level could also be used here.) We choose "arcsin" as `transformation`, and a bias-corrected `backtransformation` ("bc"). Additionally, the effective sample size, which equals the sample size of each area divided by the design effect, is needed for the arcsin transformation. We set the `MSE` estimation to `TRUE`, the `mse_type` to "boot" (necessary for the type of transformation) and determine the number of bootstrap iterations. For practical applications, values larger than 200 are recommended. In case, no transformation is desired, the `transformation` argument must be set to "no" and the inputs `backtransformation` and `eff_smpsize` are no longer needed.


```{r model_est, message = FALSE, warning = FALSE}
set.seed(123)
fh_model <- fh(fixed = formula(fh_step$fixed),
               vardir = "vardir", 
               combined_data = as.data.frame(dta_ufh), 
               domains = "id",
               method = "ml", 
               transformation = "arcsin", 
               backtransformation = "bc",
               eff_smpsize = "n_eff", 
               MSE = TRUE, 
              mse_type = "boot", B = c(200, 0)) 

bd_out |> pin_write(fh_model, type = "rds")

## In case, no transformation is desired, the call would like this:
#fh_model <- fh(
#  fixed = formula(fh_step$fixed),
#  vardir = "vardir", 
#  combined_data = as.data.frame(dta_ufh), 
#  domains = "id",
#  method = "ml", 
#  MSE = TRUE) 
```

## Assessment of the estimated model.

With the help of the `summary` method of `emdi`, we gain detailed insights into the data and model components. It includes information on the estimation methods used, the number of domains, the log-likelihood, and information criteria as proposed by @Marhuenda2014. It also reports the adjusted $R^2$ from a standard linear model and the adjusted $R^2$ specific to FH models, as introduced by @Lahiri2015. It also offers diagnostic measures to assess model assumptions regarding the standardized realized residuals and random effects. These include skewness and kurtosis (based on the `moments` package by @Komsta2015), as well as Shapiro-Wilk test statistics and corresponding p-values to evaluate the normality of both error components.

```{r summary, message = FALSE, warning = FALSE}
summary(fh_model)
```

We can see, that all domains are in-sample domains. The variance of the random effects equals 0.0008613493. All of the included auxiliary variables are significant and their explanatory power is large with an adjusted $R^2$ (for FH models) of around 0.73. The results of the Shapiro-Wilk-test indicate that normality is not rejected for both error terms.

### Diagnostic plots

We produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms by the `plot` method of `emdi`.

```{r plot, warning = FALSE}
plot(fh_model)
```

The plots show slight deviations of the distributions from normality. Together with the results of the Shapiro-Wilk test, we can state that normality is not rejected for both error terms.

## Comparison of the FH results with the direct estimates.

The FH estimates are expected to align closely with the direct estimates in domains with small direct MSEs and/or large sample sizes. Moreover, incorporating auxiliary information should enhance the precision of the direct estimates. We produce a scatter plot proposed by @Brown2001 and a line plot. The fitted regression and the identity line of the scatter plot should not differ too much. The FH estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. Furthermore, we compare the MSE and CV estimates for the direct and FH estimators using boxplots and ordered scatter plots (by setting the input arguments `MSE` and `CV` to `TRUE`).

Additionally, we compute a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model [@Chandra2015] and a goodness of fit diagnostic [@Brown2001].

```{r compare, warning = FALSE}
compare_plot(fh_model, MSE = TRUE, CV = TRUE)
compare(fh_model)

```

The direct estimates are tracked by most of the FH estimates within the line plot. The precision of the direct estimates could be improved by the usage of the FH model in terms of MSEs and CVs. The null hypothesis of the Brown test is not rejected and the correlation coefficient indicates a positive correlation (0.72) between the direct and FH estimates.

If the result of the model assessment is not satisfactory, the following should be checked again: Can the direct estimation including variance estimation be improved? Are there further auxiliary variables and/or must possible interaction effects be taken into account? Does a (different) transformation need to be used?

## Benchmark the FH point estimates for consistency with higher results.

Benchmarking is based on the principle that aggregated FH estimates should sum up to the estimates at a higher regional level. For the benchmark function, a benchmark value and a vector containing the shares of the population size per area ($N_d/N$) is required. Please note, that only the FH estimates are benchmarked and not their MSE estimates. As benchmark types "raking", "ratio" and "MSE_adj" can be chosen. For further details about using the function, please refer to the `emdi` vignette and for general information about the benchmarking options to @Datta2011.

```{r benchmarking, message = FALSE, warning = FALSE}
## compute the benchmark value (mean of poverty indicator for the whole country)
benchmark_value <- mean(dta_ufh$pov)

## compute the share of population size in the total population size (N_d/N) per area
dta_ufh <- dta_ufh |>
  mutate(ratio_n = N_pop/sum(N_pop))

fh_bench <- benchmark(fh_model,
                      benchmark = benchmark_value,
                      share = dta_ufh$ratio_n, 
                      type = "ratio",
                      overwrite = TRUE)
head(fh_bench$ind)
```

## Preparation of the results.

Create one dataframe that contains the direct and FH estimation results including MSE and CV results.

```{r res_prep, message = FALSE, warning = FALSE}
pov_fh <- as.data.frame(estimators(fh_model, MSE = TRUE, CV = TRUE))
head(pov_fh)

pov_fh <- pov_fh |>
  rename(!!area_var := "Domain") |>
  mutate(year = singleyear)

bd_out |>
  pin_write(x = pov_fh,
            name = "pov_fh",
            type = "rds")

write.csv(pov_fh, "data/clean/pov_fh.csv")
```

### Poverty map

With the help of geographical maps, the results can be presented in a user-friendly way and differences among the areas can be detected more easily. For the map, a shape file is reqired. The domain identifiers in the results object (`fh_model`) need to match to the respective identifiers of the shape file. Therefore, we create a mapping table first and then produce the map by `emdi::map_plot`.

```{r fig-povmap, message = FALSE}
## Create a suitable mapping table
## Find the right order
domain_ord <- match(geom_nuts3$subcode, fh_model$ind$Domain)

## Create the mapping table based on the order obtained above
map_tab <- data.frame(pop_data_id = fh_model$ind$Domain[domain_ord],
                      shape_id = geom_nuts3$subcode)

## Create map
map_plot(object = fh_model, MSE = TRUE, map_obj = geom_nuts3,
 map_dom_id = "subcode", map_tab = map_tab)

```

## Saving the results.

Either by using `save.image("fh_estimation.RData")` or export of the model output and estimation results to Excel or OpenDocument spreadsheet (ODS) `write.excel(fh_model, file = "fh_model_output.xlsx", MSE = TRUE, CV = TRUE`).

## References
