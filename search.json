[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EUPM: Poland",
    "section": "",
    "text": "0.1 Overview\nRStudio project for preparing data and performing the poverty mapping analysis for Poland. It collects data from various sources, cleans and merges it, and prepares data sets for analysis. Then, poverty mapping is performed.\nThis project relies on the renv R package to ensure that the same version of all supporting packages is being used by the users. To control for the version change in the scripts, git is used locally with a corresponding github account. Any data in the data folder and sub-folders are ignored, therefore, will not appear in the GitHub history to preserve privacy.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "EUPM: Poland",
    "section": "0.2 Project structure",
    "text": "0.2 Project structure\n.\n├── 00-master.R                   - Executes all code\n├── 05-geometries.qmd             - Documentation of the geometries preparation\n├── 10-SILC-variables.R           - Prepared variables based on the SILC data\n├── 20-other-variables.qmd        - Collection and normalization of other variables\n├── 30-data-mege.qmd              - Selecting relevant variables and merging data\n├── 40-fh.qmd                     - The Univariate Fay-Herriot (UFH) model\n├── 50-mfh.qmd                    - The Multivariate Fay Herriot Modelling\n├── data                          - data inventory\n│   ├── clean                     - Place for clean and processed data sets, e.g. poverty maps.\n│   ├── raw                       - Repository for saving raw data\n│   │   ├── api                   - Area level data downloaded from the NSO api\n│   │   ├── ntl                   - Night time light data\n│   │   ├── geometries            - Geometrical boundaries at different levels\n│   │   ├── pl-metadata.xlsx      - Metadata for area-level data\n│   │   └── silk-sample           - Sample/dummy data for setting up the SILC code\n│   └── temp                      - Temporary processed raw data\n├── R                             - Custom R functions used in the analysis\n├── reports                       - Generated reports\n│   ├── 05-area-geometries.html\n│   └── 20-area-level-variables.html\n├── scripts                       - Folder with the axiliary R scripts.\n├── eupm-pl.Rproj                 - RStudio project file\n├── README.md                     - Read me\n└── _quarto.yml\n\n0.2.1 Principals of data storage\nTo save data, we rely on R (pins.rstudio.com)[https://pins.rstudio.com], which enable keeping a history of the data changes on the local folder structure.\n\n\n0.2.2 Data reload from the API\nTo reload the data from the API, follow the instructions in the script 20-other-variables.qmd. Please note that by default, all data is reloaded at once. It may take a while.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "40-fh.html",
    "href": "40-fh.html",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "",
    "text": "2.1 Introduction\nIn this section, we will present a whole estimation procedure of the univariate area-level model introduced by Fay and Herriot (1979) in R. As with the disclaimer in the preceding section, this practical manual is not intended to serve as a theoretical introduction to area-level models. Instead, it offers a set of straightforward and practical R scripts, accompanied by clear explanations, to demonstrate how these tasks can be carried out in R. For a theoretical foundation please refer to Fay and Herriot (1979) and Rao and Molina (2015). In addition to theoretical information, the vignette “A framework for producing small area estimates based on area-level models in R” of the R package emdi (Harmening et al. 2023) provides further code examples for the FH model.\nIn this chapter, we will describe how to run the univariate Fay-Herriot (FH) using simulated income data from Spain. The estimation procedure is explained step by step.\nStep 1: Data preparation. Compute the direct estimates and their corresponding variances on the area-level and eventually perform variance smoothing. Aggregate the available auxiliary variables to the same area level and combine both input data.\nStep 2: Model selection. Select the aggregated auxiliary variables at the area level for the FH model using a stepwise selection based on information criteria like the Akaike, Bayesian or Kullback information criteria.\nStep 3: Model estimation of FH point estimates and their mean squared error (MSE) estimates as uncertainty measure. Eventually apply a transformation.\nStep 4: Assessment of the estimated model. Check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals. When violations of the model assumptions are detected, the application of a transformation might help. Repeat Step 3 including a transformation and check again the model assumptions.\nStep 5: Comparison of the FH results with the direct estimates.\nStep 6: Benchmark the FH point estimates for consistency with higher results.\nStep 7: Preparation of the results. Create tables and maps of results.\nStep 8: Saving the results. One option is to export the results to known formats like Excel or OpenDocument Spreadsheets.\nWe will show below the use of the fh() function of the R package emdi (Harmening et al. 2023) which computes the EBLUPs and their MSE estimates of the standard FH model and several extensions of it, among others it allows for the application of transformations. Because the poverty rate is a ratio, it might be helpful to apply the arcsin transformation to guarantee that the results lie between 0 and 1. The function call is:\nfh(fixed, vardir, combined_data, domains = NULL, method = \"reml\", interval = NULL,   k = 1.345, mult_constant = 1, transformation = \"no\", backtransformation = NULL,   eff_smpsize = NULL, correlation = \"no\", corMatrix = NULL, Ci = NULL, tol = 1e-04,   maxit = 100, MSE = FALSE, mse_type = \"analytical\", B = c(50, 0), seed = 123)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#data-and-preparation",
    "href": "40-fh.html#data-and-preparation",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.2 Data and preparation",
    "text": "2.2 Data and preparation\n\n2.2.1 Load the dataset\nUsually, SAE combines multiple data sources: a survey data set and a census or administrative/register dataset. For the estimation of area-level models, we need area-level aggregates of the same area-level (e.g. NUTS3) of both datasets. The target variable (typically household welfare/income for poverty mapping) is available in the survey but not in the census data.\nIn this example, we use a synthetic data set adapted from R package sae called incomedata. The original data contains information for \\(n = 17,119\\) fictitious individuals residing across \\(D = 52\\) Spanish provinces. The variables include the name of the province of residence (provlab), province code (prov), as well as several correlates of income.\nFor this exercise, we use a random 10% sample of the incomedata to estimate the poverty rates. For the univariate case, we use the income variable from 2012.\nThe FH estimation process relies on 3 types of data:\n\nThe data containing the outcome variable from which direct estimates will be computed. This is a usually a survey dataset for each year of data including outcome variable (such as income or welfare aggregates), weights, cluster identifiers (i.e. if available, psu or enumeration areas) at the unit level i.e. individual or household. In this example, we call this: survey_dt.\nA dataset containing for the right hand side (RHS) variables i.e. indicator estimates representative at the level of the target area. This is often obtained from administrative data sources or geospatial data such as remotely sensed high resolution data. The final RHS dataset ought to include an area id and variables of interest. In this example, we call this rhs_dt.\nFinally, a shapefile which spatially links each target area to their boundaries on a map. This will contain the area id as well as the geometry object which when visualized will show the shape of the areas in which poverty rates will be estimated. In this example, we call this shp_dt.\n\n\n### lets read in our 3 aforementioned datasets \n\nsurvey_dt &lt;- \n  bd_clean |&gt;\n  pin_read(\"pov_direct\")\n\n\nrhs_dt &lt;- \n  bd_clean |&gt;\n  pin_read(\"sae_data\")\n  \n\nshp_dt &lt;- \n  bd_clean |&gt;\n  pin_read(\"geometries\")\n\nBelow is what our datasets look like. These are generally the variables\n\nsurvey_dt |&gt;\n  glimpse()\n\nRows: 12,894\nColumns: 7\n$ provlab &lt;fct&gt; Alava, Alava, Alava, Alava, Alava, Alava, Alava, Alava, Alava,…\n$ prov    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ weight  &lt;dbl&gt; 25977.699, 25977.699, 25977.699, 19471.566, 19471.566, 19471.5…\n$ ea_id   &lt;int&gt; 101, 101, 101, 102, 102, 102, 103, 103, 103, 103, 103, 103, 10…\n$ year    &lt;int&gt; 2012, 2013, 2014, 2012, 2013, 2014, 2012, 2012, 2012, 2012, 20…\n$ income  &lt;dbl&gt; 5150.399, 5204.872, 5480.116, 27920.737, 28508.978, 31512.153,…\n$ povline &lt;dbl&gt; 6477.484, 6515.865, 6515.865, 6477.484, 6515.865, 6515.865, 64…\n\n\nsurvey_dt contains our target area identifiers (provlab, prov), the weight variable weight, the cluster id i.e. enumeration area or psu, ea_id, the year, year, the income variable income and the poverty line poverty. In this example, survey_dt of class data.frame is at the level of the individual. It could also be at the level of the household if poverty lines are evaluated at that level.\n\nrhs_dt |&gt;\n  glimpse()\n\nRows: 156\nColumns: 39\nGroups: prov [52]\n$ prov           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ provlab        &lt;fct&gt; Alava, Albacete, Alicante, Almeria, Avila, Badajoz, Bal…\n$ gen            &lt;dbl&gt; 1.591661, 1.512574, 1.531217, 1.379440, 1.469752, 1.412…\n$ age2           &lt;dbl&gt; 0.10662920, 0.15809756, 0.09547348, 0.16773730, 0.00000…\n$ age3           &lt;dbl&gt; 0.3874807, 0.4944282, 0.3547117, 0.4673372, 0.4989752, …\n$ age4           &lt;dbl&gt; 0.245845529, 0.059850454, 0.174966688, 0.005694235, 0.1…\n$ age5           &lt;dbl&gt; 0.19980439, 0.18429053, 0.23276281, 0.18630876, 0.27121…\n$ educ1          &lt;dbl&gt; 0.21756579, 0.22430208, 0.38687301, 0.34254113, 0.40534…\n$ educ2          &lt;dbl&gt; 0.2353839, 0.4386757, 0.3915053, 0.3840035, 0.3181650, …\n$ educ3          &lt;dbl&gt; 0.32395951, 0.10794670, 0.07260912, 0.10053284, 0.20833…\n$ nat1           &lt;dbl&gt; 0.9653883, 0.9837955, 0.9436051, 0.8789277, 1.0000000, …\n$ labor1         &lt;dbl&gt; 0.3692168, 0.4513452, 0.4054253, 0.5191851, 0.4992434, …\n$ labor2         &lt;dbl&gt; 0.000000000, 0.036094381, 0.047285012, 0.040264017, 0.0…\n$ labor3         &lt;dbl&gt; 0.4076923, 0.2834849, 0.3982771, 0.2676284, 0.4326099, …\n$ abs            &lt;dbl&gt; 0.84933922, -0.97816135, 0.98863150, 0.31350535, -1.419…\n$ ntl            &lt;dbl&gt; 0.2091905, 1.1589425, -0.4210840, 1.5472493, -2.1440414…\n$ aec            &lt;dbl&gt; 1.16518757, -0.55990121, -0.66408247, -0.70557145, 4.99…\n$ schyrs         &lt;dbl&gt; 1.70993214, -0.96018920, -0.18004563, 0.22504016, -1.64…\n$ mkt            &lt;dbl&gt; 2.5782541, -1.0468569, -0.2044435, -0.9811182, -1.08118…\n$ age2_X_gen     &lt;dbl&gt; 0.15555592, 0.26628138, 0.15033171, 0.21079949, 0.00000…\n$ age3_X_gen     &lt;dbl&gt; 0.6647560, 0.7222008, 0.5516074, 0.6262462, 0.7645003, …\n$ age4_X_gen     &lt;dbl&gt; 0.36008687, 0.10298379, 0.26866795, 0.01138847, 0.26583…\n$ age5_X_gen     &lt;dbl&gt; 0.35102165, 0.30664168, 0.35115346, 0.28723752, 0.37126…\n$ educ1_X_gen    &lt;dbl&gt; 0.3687830, 0.3682332, 0.5879137, 0.5256249, 0.6863470, …\n$ educ2_X_gen    &lt;dbl&gt; 0.4130652, 0.7220561, 0.6010236, 0.4517964, 0.4494350, …\n$ educ3_X_gen    &lt;dbl&gt; 0.53779476, 0.16167700, 0.12157537, 0.15825036, 0.26582…\n$ nat1_X_gen     &lt;dbl&gt; 1.557049, 1.480165, 1.453477, 1.206699, 1.469752, 1.376…\n$ labor1_X_gen   &lt;dbl&gt; 0.6529375, 0.6858805, 0.5812196, 0.6455309, 0.7541068, …\n$ labor2_X_gen   &lt;dbl&gt; 0.000000000, 0.072188761, 0.086971286, 0.040264017, 0.0…\n$ labor3_X_gen   &lt;dbl&gt; 0.6667055, 0.4938971, 0.6423218, 0.4498768, 0.6474986, …\n$ age2_X_educ3   &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.0…\n$ age3_X_educ3   &lt;dbl&gt; 0.18883381, 0.05231836, 0.02345535, 0.10053284, 0.00000…\n$ age4_X_educ3   &lt;dbl&gt; 0.110497526, 0.036933029, 0.025056921, 0.000000000, 0.1…\n$ age5_X_educ3   &lt;dbl&gt; 0.024628173, 0.018695319, 0.024096847, 0.000000000, 0.0…\n$ nat1_X_educ3   &lt;dbl&gt; 0.32395951, 0.10794670, 0.06702616, 0.06418623, 0.20833…\n$ labor1_X_educ3 &lt;dbl&gt; 0.274032897, 0.052318355, 0.036012387, 0.100532844, 0.1…\n$ labor2_X_educ3 &lt;dbl&gt; 0.000000000, 0.000000000, 0.006859625, 0.000000000, 0.0…\n$ labor3_X_educ3 &lt;dbl&gt; 0.049926612, 0.055628348, 0.029737105, 0.000000000, 0.0…\n$ year           &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2…\n\n\n`rhs_dt is an object of class data.frame created at the level of the target area. It should contain the same target area identifiers as in survey_dt and the year variable year.\n\nshp_dt |&gt;\n  glimpse()\n\nRows: 52\nColumns: 3\n$ prov     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ provlab  &lt;fct&gt; Alava, Albacete, Alicante, Almeria, Avila, Badajoz, Baleares,…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-2.858067 4..., MULTIPOLYGON (((…\n\n\nshp_dt is an object of class sf, data.frame created at the level of the target area. This is the shapefile for the area of interest for which the poverty map will be estimated. It should contain the same target area ID found in survey_dt as well as rhs_dt.\nA quick summary table on the data needs for the UFH model:\n\n\n\n\n\n\n\n\nData Input Checklist for the Univariate Fay-Herriot Model\n\n\nDatasets, levels, and Required variables\n\n\nDataset Name\nUnit of Observation\nRequired Variables\n\n\n\n\nsurvey_dt\nIndividual (or Household)\ntarget area identifiers, weights, cluster identifier, income/welfare variable, poverty line\n\n\nrhs_dt\nTarget Area (e.g. Province)\ntarget area identifiers, covariates (e.g. gen, educ1, schyrs, etc.)\n\n\nshp_dt\nTarget Area (Spatial)\ntarget area identifiers, geometries (e.g. geometry column from sf)\n\n\n\n\n\n\n\n\n\n2.2.2 Direct estimation\nWe will use the direct Horvitz-Thompons estimators that use the survey weights in weight variable. We calculate the sample sizes for each provinces and compute the direct estimates and their variances. We use the direct function of the emdi package here. Other options are e.g. the direct function of package sae (Molina and Marhuenda 2015) or the svyby command of package survey (Lumley 2024). Then, we create a dataframe containing the direct estimate, the standard errors, the variances, the coefficient of variation and the design effects, that are needed for the arcsin transformation. The design effect is the ratio of the variance considering the sampling design to the variance estimated under simple random sampling. For detailled information about the arcsin transformation please refer to Casas-Cordero, Encina, and Lahiri (2016) and Schmid et al. (2017). In some areas with a very small sample size, it may occur, that the individual data only consists of zeros and ones, resulting in a direct estimate of zero or one and a direct variance of zero. We set those areas to out-of-sample and for the final estimation results only the synthetic part of the FH model is used.\n\n### a little bit of housekeeping to ensure ease of access \narea_vars &lt;- c(\"prov\", \"provlab\") ### both variables are at the same level. If the levels vary, you would need to combine both variables for effect use\n\ncluster_var &lt;- \"ea_id\"\n\nweight_var &lt;- \"weight\"\n\nyear_var &lt;- \"year\"\n\noutcome_var &lt;- \"income\"\n\npovline_var &lt;- \"povline\"\n\ncandidate_vars &lt;- colnames(rhs_dt)[!colnames(rhs_dt) %in% c(area_vars, year_var)]\n\n## For the univariate model, we only use a single year (here 2012)\nsingleyear &lt;- \"2012\"\n\nsurvey_dt &lt;- survey_dt |&gt;\n  filter(year == singleyear)\n\nrhs_dt &lt;- rhs_dt |&gt;\n  filter(year == singleyear)\n\n\n## calculate sample size for each province\nsampsize_dt &lt;- \nsurvey_dt |&gt;\n  group_by(!!!syms(area_vars[1])) |&gt;\n  summarize(N = n())\n\n## computation of direct estimates and their variances (the poverty line is already included within the data)\n## creating the poverty indicator\nsurvey_dt &lt;- \n  survey_dt |&gt;\n  mutate(pov_indicator = ifelse(income &lt; povline, 1, 0))\n\n\n### creating a survey object\ndesign_obj &lt;- survey::svydesign(ids = eval(expr(~!!sym(cluster_var))), \n                                weights = eval(expr(~!!sym(weight_var))), \n                                data = survey_dt)\n\nvar_dt &lt;- survey::svyby(~pov_indicator, by=eval(expr(~!!sym(area_vars[1]))), design = design_obj, FUN = survey::svymean)\n\n\ndirect_dt &lt;- \n  var_dt |&gt;\n  rename(direct_povrate = \"pov_indicator\") |&gt;\n  rename(SD = \"se\") |&gt;\n  mutate(vardir = SD^2) |&gt;\n  mutate(CV = SD / direct_povrate) |&gt;\n  merge(sampsize_dt, \n        by = area_vars[[1]]) |&gt;\n  mutate(var_SRS = direct_povrate * (1 - direct_povrate) / N) |&gt;\n  mutate(deff = vardir / var_SRS) |&gt;\n  mutate(n_eff = N/deff)\n\n## set zero variance to OOS\ndirect_dt &lt;- direct_dt[complete.cases(direct_dt), ]\n\nHere is what the results look like:\n\nhead(direct_dt)\n\n  prov direct_povrate         SD       vardir        CV   N     var_SRS\n1    1     0.24394059 0.09047786 0.0081862422 0.3709012  24 0.007684732\n2    2     0.15913215 0.08113377 0.0065826888 0.5098515  43 0.003111840\n3    3     0.16294054 0.02727481 0.0007439151 0.1673912 135 0.001010303\n4    4     0.29656922 0.04988701 0.0024887140 0.1682137  50 0.004172318\n5    5     0.09564252 0.05928951 0.0035152460 0.6199074  14 0.006178217\n6    6     0.22733200 0.03849937 0.0014822014 0.1693531 124 0.001416550\n       deff     n_eff\n1 1.0652605  22.52970\n2 2.1153688  20.32742\n3 0.7363286 183.34206\n4 0.5964823  83.82478\n5 0.5689742  24.60568\n6 1.0463462 118.50762\n\n\n\n\n2.2.3 Variance smoothing\nA quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by You and Hidiroglou (2023). Please see the code and Roxygen comments below explaining the use of the varsmoothie_king() function which computes smoothed variances. In case, the arcsin transformation will be applied, the variance smoothing described here is not necessary, since the arcsin transformation works variance stabilizing itself. When applying the arcsin transformation, the direct variances are automatically set to 1/(4*effective sampling size) when using the fh function of package emdi. The effective sample size equals the sample size of each area divided by the design effect. If the variance stabilizing effect is not enough, the design effect of a higher area level could also be used here (in this example the regions ac).\nThe goal now is to use the varsmoothie_king() function to add an additional column of smoothed variances into our direct_dt dataframe. Required inputs: a vector of unique domains, the raw variances estimated from sample data and the sample size for each domain.\n\nvar_smooth &lt;- varsmoothie_king(domain = direct_dt[[area_vars[1]]],\n                               direct_var = direct_dt$vardir,\n                               sampsize = direct_dt$N)\n\ndirect_dt &lt;- var_smooth |&gt; merge(direct_dt, by.x = \"Domain\",\n        by.y = area_vars[1])\n\n# Replace the variances that are zero with their smoothed counterparts\ndirect_dt &lt;- \n  direct_dt |&gt;\n  mutate(across(\n    starts_with(\"v_\"),\n    ~ if_else(abs(.x) &lt;= 1e-4, get(paste0(\"vsv\", str_remove(cur_column(), \"^v\"))), .x),\n    .names = \"{.col}\"\n  ))\n\nThus far, we have careful set up the types of data we require for the FH model. We only need to combine the dataframe containing the direct estimates and their variances with the auxiliary variables.\n\nfh_dt &lt;- merge(direct_dt, rhs_dt,\n    by.x = \"Domain\", by.y = area_vars[[1]],\n    all = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#model-selection",
    "href": "40-fh.html#model-selection",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.3 Model selection",
    "text": "2.3 Model selection\n\n2.3.1 Model preparation\nFH does not run if there is any missing value in the auxiliary variables, and therefore, any variable with missing value should be removed in advance.\n\nrowsNAcovariates &lt;- rowSums(sapply(fh_dt[,..candidate_vars], is.na))\nfh_dt &lt;- fh_dt[rowsNAcovariates == 0, ]\n\n\n\n2.3.2 Check multicollinearity\nWith the help of the step() function of package emdi, we perform a variable selection based on the chosen variable selection criterion and directly get the model with fewer variables. The function step_wrapper() implemented below is a wrapper to the emdi::step() function and performs all the perfunctory cleaning necessary to use step(). This includes dropping columns that are entirely missing (NA) and keep only complete cases/observations (for the model selection only the in-sample domains are used) and remove perfectly or near collinear variables and combinations.\nWe apply the function to select the variables. Required inputs: data set, character vector containing the set of auxiliary variables, name of y variable, a correlation threshold between 0 and 1, name of information criterion and name of direct variance variable. In case, a transformation should be applied, “arcsin” as transformation and name of variable that contains the effective sample size.\n\nfh_step &lt;- step_wrapper_fh(dt = fh_dt,\n                           xvars = candidate_vars,\n                           y = \"direct_povrate\",\n                           cor_thresh = 0.7,\n                           criteria = \"BIC\",\n                           vardir = \"vardir\", \n                           transformation = \"arcsin\", \n                           eff_smpsize = \"n_eff\")\n\n\n# Resulting model formula\nprint(fh_step$fixed)\n\ndirect_povrate ~ gen + age5 + educ1 + mkt\n&lt;environment: 0x000002cd146067e8&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#model-estimation-of-fh-point-and-their-mse-estimates.",
    "href": "40-fh.html#model-estimation-of-fh-point-and-their-mse-estimates.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.4 Model estimation of FH point and their MSE estimates.",
    "text": "2.4 Model estimation of FH point and their MSE estimates.\nIn this example, we use the function fh to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1. For that, we choose “arcsin” as transformation, and a bias-corrected backtransformation (“bc”). Additionally, the effective sample size, which equals the sample size of each area divided by the design effect, is needed for the arcsin transformation. We set the MSE estimation to TRUE, the mse_type to “boot” (necessary for the type of transformation) and determine the number of bootstrap iterations. For practical applications, values larger than 200 are recommended. In case, no transformation is desired, the transformation argument must be set to “no” and the inputs backtransformation and eff_smpsize are no longer needed.\n\nset.seed(123)\nfh_model &lt;- fh(fixed = formula(fh_step$fixed),\n               vardir = \"vardir\", \n               combined_data = fh_dt, \n               domains = \"Domain\",\n               method = \"ml\", \n               transformation = \"arcsin\", \n               backtransformation = \"bc\",\n               eff_smpsize = \"n_eff\", \n               MSE = TRUE, \n               mse_type = \"boot\", B = c(200, 0)) \n\n## In case, no transformation is desired, the call would like this:\n# fh_model &lt;- fh(\n#   fixed = Direct ~ age2 + age3 + age4 + age5 + educ1 + ntl + schyrs, #formula(fh_step$fixed),\n#   vardir = \"vardir\", combined_data = comb_Data, domains = \"Domain\",\n#   method = \"ml\", MSE = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#assessment-of-the-estimated-model.",
    "href": "40-fh.html#assessment-of-the-estimated-model.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.5 Assessment of the estimated model.",
    "text": "2.5 Assessment of the estimated model.\nWith the help of the summary method of emdi, we gain detailed insights into the data and model components. It includes information on the estimation methods used, the number of domains, the log-likelihood, and information criteria as proposed by Marhuenda, Morales, and Camen Pardo (2014). It also reports the adjusted \\(R^2\\) from a standard linear model and the adjusted \\(R^2\\) specific to FH models, as introduced by Lahiri and Suntornchost (2015). It also offers diagnostic measures to assess model assumptions regarding the standardized realized residuals and random effects. These include skewness and kurtosis (based on the moments package by Komsta and Novomestky (2015)), as well as Shapiro-Wilk test statistics and corresponding p-values to evaluate the normality of both error components.\n\nsummary(fh_model)\n\nCall:\n fh(fixed = formula(fh_step$fixed), vardir = \"vardir\", combined_data = fh_dt, \n    domains = \"Domain\", method = \"ml\", transformation = \"arcsin\", \n    backtransformation = \"bc\", eff_smpsize = \"n_eff\", MSE = TRUE, \n    mse_type = \"boot\", B = c(200, 0))\n\nOut-of-sample domains:  1 \nIn-sample domains:  51 \n\nVariance and MSE estimation:\nVariance estimation method:  ml \nEstimated variance component(s):  0.002254948 \nMSE method:  bootstrap \n\nCoefficients:\n            coefficients std.error t.value   p.value    \n(Intercept)     1.248326  0.311379  4.0090 6.097e-05 ***\ngen            -0.525756  0.206284 -2.5487  0.010813 *  \nage5            0.560927  0.208204  2.6941  0.007057 ** \neduc1          -0.271896  0.132949 -2.0451  0.040843 *  \nmkt             0.052423  0.011775  4.4518 8.513e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nExplanatory measures:\n   loglike       AIC      BIC     AdjR2     FH_R2\n1 59.21822 -106.4364 -94.8455 0.4606883 0.7681853\n\nResidual diagnostics:\n                         Skewness Kurtosis Shapiro_W Shapiro_p\nStandardized_Residuals -0.0704399 2.085588 0.9773667 0.4338271\nRandom_effects         -0.2587831 3.422467 0.9896043 0.9324833\n\nTransformation:\n Transformation Back_transformation\n         arcsin                  bc\n\n\nWe can see, that 49 domains are in-sample domains. The 3 out-of-sample domains belong to the domains with 0 direct and variance estimates that we set to NA in the beginning. The variance of the random effects equals 0.001516632. All of the included auxiliary variables are significant on a 0.05 significance level and their explanatory power is large with an adjusted \\(R^2\\) (for FH models) of around 0.74. The results of the Shapiro-Wilk-test indicate that normality is not rejected for the standardized residuals.\n\n2.5.1 Diagnostic plots\nWe produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms by the plot method of emdi.\n\nplot(fh_model)\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nThe plots show slight deviations of the distributions from normality. The normality assumption is not required for the computation of the FH estimates, but at the obtained MSE estimates we have to look with some care when the normality assumption does not hold.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#comparison-of-the-fh-results-with-the-direct-estimates.",
    "href": "40-fh.html#comparison-of-the-fh-results-with-the-direct-estimates.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.6 Comparison of the FH results with the direct estimates.",
    "text": "2.6 Comparison of the FH results with the direct estimates.\nThe FH estimates are expected to align closely with the direct estimates in domains with small direct MSEs and/or large sample sizes. Moreover, incorporating auxiliary information should enhance the precision of the direct estimates. We produce a scatter plot proposed by Brown et al. (2001) and a line plot. The fitted regression and the identity line of the scatter plot should not differ too much. The FH estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. Furthermore, we compare the MSE and CV estimates for the direct and FH estimators using boxplots and ordered scatter plots (by setting the input arguments MSE and CV to TRUE).\nAdditionally, we compute a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model (Chandra, Salvati, and Chambers 2015) and a goodness of fit diagnostic (Brown et al. 2001).\n\ncompare_plot(fh_model, MSE = TRUE, CV = TRUE)\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\ncompare(fh_model)\n\nBrown test \n\nNull hypothesis: EBLUP estimates do not differ significantly from the\n      direct estimates \n\n  W.value Df   p.value\n 15.08585 51 0.9999998\n\nCorrelation between synthetic part and direct estimator:  0.72 \n\n\nThe direct estimates are tracked by most of the FH estimates within the line plot. The precision of the direct estimates could be improved by the usage of the FH model in terms of MSEs and CVs. The null hypothesis of the Brown test is not rejected and the correlation coefficient indicates a positive correlation (0.66) between the direct and FH estimates.\nIf the result of the model assessment is not satisfactory, the following should be checked again: Can the direct estimation including variance estimation be improved? Are there further auxiliary variables and/or must possible interaction effects be taken into account? Does a (different) transformation need to be used?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#benchmark-the-fh-point-estimates-for-consistency-with-higher-results.",
    "href": "40-fh.html#benchmark-the-fh-point-estimates-for-consistency-with-higher-results.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.7 Benchmark the FH point estimates for consistency with higher results.",
    "text": "2.7 Benchmark the FH point estimates for consistency with higher results.\nBenchmarking is based on the principle that aggregated FH estimates should sum up to the estimates at a higher regional level. For the benchmark function, a benchmark value and a vector containing the shares of the population size per area (\\(N_d/N\\)) is required. Please note, that only the FH estimates are benchmarked and not their MSE estimates. As benchmark types “raking”, “ratio” and “MSE_adj” can be chosen. For further details about using the function, please refer to the emdi vignette and for general information about the benchmarking options to Datta et al. (2011).\n\n## compute the benchmark value (mean of poverty indicator for the whole country)\nbenchmark_value &lt;- weighted.mean(survey_dt$pov_indicator, survey_dt[[weight_var]])\n\n## compute the share of population size in the total population size (N_d/N) per area\ndata(\"sizeprov\")\nfh_dt &lt;- fh_dt |&gt;\n  left_join(sizeprov |&gt;\n  mutate(ratio_n = Nd/sum(Nd)), by = c(\"Domain\" = area_vars[1]))\n\nfh_bench &lt;- benchmark(fh_model,\n                      benchmark = benchmark_value,\n                      share = fh_dt$ratio_n, \n                      type = \"ratio\",\n                      overwrite = TRUE)\nhead(fh_bench$ind)\n\n  Domain     Direct        FH  FH_Bench Out\n1      1 0.24394059 0.3061689 0.3140667   0\n2      2 0.15913215 0.1794998 0.1841301   0\n3      3 0.16294054 0.1754708 0.1799972   0\n4      4 0.29656922 0.2501263 0.2565784   0\n5      5 0.09564252 0.1781981 0.1827949   0\n6      6 0.22733200 0.2485626 0.2549744   0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#preparation-of-the-results.",
    "href": "40-fh.html#preparation-of-the-results.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.8 Preparation of the results.",
    "text": "2.8 Preparation of the results.\nCreate one dataframe that contains the direct and FH estimation results including MSE and CV results.\n\npov_fh &lt;- as.data.frame(estimators(fh_model, MSE = TRUE, CV = TRUE))\nhead(pov_fh)\n\n  Domain     Direct   Direct_MSE Direct_CV        FH       FH_MSE     FH_CV\n1      1 0.24394059 0.0081862422 0.3709012 0.3061689 0.0026749083 0.1689248\n2      2 0.15913215 0.0065826888 0.5098515 0.1794998 0.0015381875 0.2184946\n3      3 0.16294054 0.0007439151 0.1673912 0.1754708 0.0007341814 0.1544176\n4      4 0.29656922 0.0024887140 0.1682137 0.2501263 0.0009681514 0.1243977\n5      5 0.09564252 0.0035152460 0.6199074 0.1781981 0.0017924112 0.2375832\n6      6 0.22733200 0.0014822014 0.1693531 0.2485626 0.0010867894 0.1326285\n\npov_fh &lt;- pov_fh |&gt;\n  rename(!!area_vars[1] := \"Domain\") |&gt;\n  mutate(year = singleyear)\n\nbd_out |&gt;\n  pin_write(x = pov_fh,\n            name = \"pov_fh\",\n            type = \"rds\")\n\nwrite.csv(pov_fh, \"data/clean-example/pov_fh.csv\")\n\n\n2.8.1 Poverty map\nWith the help of geographical maps, the results can be presented in a user-friendly way and differences among the areas can be detected more easily. For the map, a shape file is reqired. The domain identifiers in the results object (fh_model) need to match to the respective identifiers of the shape file. Therefore, we create a mapping table first and then produce the map by emdi::map_plot.\n\n## Create a suitable mapping table\n## Find the right order\ndomain_ord &lt;- match(shp_dt[[area_vars[1]]], fh_model$ind$Domain)\n\n## Create the mapping table based on the order obtained above\nmap_tab &lt;- data.frame(pop_data_id = fh_model$ind$Domain[domain_ord],\n                      shape_id = shp_dt[[area_vars[1]]])\n\n## Create map\nmap_plot(object = fh_model, MSE = TRUE, map_obj = shp_dt,\n map_dom_id = area_vars[1], map_tab = map_tab)\n\nPress [enter] to continue\n\n\nPress [enter] to continue\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\nFigure 2.1\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#saving-the-results.",
    "href": "40-fh.html#saving-the-results.",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.9 Saving the results.",
    "text": "2.9 Saving the results.\nEither by using save.image(\"fh_estimation.RData\") or export of the model output and estimation results to Excel or OpenDocument spreadsheet (ODS) write.excel(fh_model, file = \"fh_model_output.xlsx\", MSE = TRUE, CV = TRUE.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "40-fh.html#references",
    "href": "40-fh.html#references",
    "title": "2  The Univariate Fay-Herriot (UFH) model",
    "section": "2.10 References",
    "text": "2.10 References\n\n\n\n\n\n\nBrown, G., R. Chambers, P. Heady, and D. Heasman. 2001. “Evaluation of Small Area Estimation Methods - an Application to Unemployment Estimates from the UK LFS.” In Proceedings of Statistics Canada Symposium.\n\n\nCasas-Cordero, C., J. Encina, and P. Lahiri. 2016. “Poverty Mapping for the Chilean Comunas.” In Analysis of Poverty by Small Area Estimation, by M. Pratesi, 379–403. John Wiley & Sons. https://doi.org/10.1002/9781118814963.ch20.\n\n\nChandra, H., N. Salvati, and R. Chambers. 2015. “A Spatially Nonstationary Fay-Herriot Model for Small Area Estimation.” Journal of the Survey Statistics and Methodology 3 (2): 109–35. https://doi.org/10.1093/jssam/smu026.\n\n\nDatta, G. S., M. Ghosh, R. Steorts, and J. Maples. 2011. “Bayesian Benchmarking with Applications to Small Area Estimation.” TEST 20 (3): 574–88. https://doi.org/10.1007/s11749-010-0218-y.\n\n\nFay, Robert E., and Roger A. Herriot. 1979. “Estimates of Income for Small Places: An Application of James-Stein Procedures to Census Data.” Journal of the American Statistical Association 74: 269–77.\n\n\nHarmening, Sylvia, Ann-Kristin Kreutzmann, Sören Schmidt, Nicola Salvati, and Timo Schmid. 2023. “A Framework for Producing Small Area Estimates Based on Area-Level Models in r.” The R Journal 15 (1): 316–41. https://doi.org/10.32614/RJ-2023-039.\n\n\nKomsta, Lukasz, and Frederick Novomestky. 2015. Moments: Moments, Cumulants, Skewness, Kurtosis and Related Tests. https://CRAN.R-project.org/package=moments.\n\n\nLahiri, P., and J. Suntornchost. 2015. “Variable Selection for Linear Mixed Models with Applications in Small Area Estimation.” The Indian Journal of Statistics 77-B (2): 312–20. https://www.jstor.org/stable/43694416.\n\n\nLumley, Thomas. 2024. “Survey: Analysis of Complex Survey Samples.”\n\n\nMarhuenda, Y., D. Morales, and M. del Camen Pardo. 2014. “Information Criteria for Fay-Herriot Model Selection.” Computational Statistics and Data Analysis 70: 268–80. https://doi.org/10.1016/j.csda.2013.09.016.\n\n\nMolina, Isabel, and Yolanda Marhuenda. 2015. “sae: An R Package for Small Area Estimation.” The R Journal 7 (1): 81–98. https://journal.r-project.org/archive/2015/RJ-2015-007/RJ-2015-007.pdf.\n\n\nRao, J. N. K., and Isabel Molina. 2015. Small Area Estimation. John Wiley; Sons, Inc, Hoboken, NJ, USA.\n\n\nSchmid, T., F. Bruckschen, N. Salvati, and T. Zbiranski. 2017. “Constructing Sociodemographic Indicators for National Statistical Institutes Using Mobile Phone Data: Estimating Literacy Rates in Senegal.” Journal of the Royal Statistical Society A 180 (4): 1163–90. https://doi.org/10.1111/rssa.12305.\n\n\nYou, Yong, and Mike Hidiroglou. 2023. “Application of Sampling Variance Smoothing Methods for Small Area Proportion Estimation.” Journal of Official Statistics 39 (4): 571–90.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Univariate Fay-Herriot (UFH) model</span>"
    ]
  },
  {
    "objectID": "50-mfh.html",
    "href": "50-mfh.html",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "",
    "text": "4 Stable FH Estimators over T time periods: The Multivariate Fay Herriot Modelling Approach\nThis section describes procedures that yield stable small area estimators for each of \\(D\\) areas over \\(T\\) subsequent time instants. Area populations, the samples and the data might change between time periods. Accordingly, we denote \\(U_t\\) the overall population at time \\(t\\), which is partitioned into \\(D\\) areas \\(U_{1t}, ... ,U_{Dt}\\), of respective population sizes \\(N_{1t}\\)\nAn overview of the MFH model estimation process is as follows:\nWe will show below the use of the eblupMFH2() and eblupMFH3() from the R package msae (Permatasari, Ubaidillah, and Permatasari 2022) compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are:\neblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)\neblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#mfh-estimation-of-poverty-rates-for-t-time-periods",
    "href": "50-mfh.html#mfh-estimation-of-poverty-rates-for-t-time-periods",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.1 MFH Estimation of Poverty Rates for T time periods",
    "text": "4.1 MFH Estimation of Poverty Rates for T time periods\nIn this example, we use a synthetic data set adapted from R package sae called incomedata. The original data contains information for \\(n = 17,119\\) fictitious individuals residing across \\(D = 52\\) Spanish provinces. The variables include the name of the province of residence (provlab), province code (prov), as well as several correlates of income. We have added two additional income vectors corresponding to two additional years of data.\nWe will show how to estimate a poverty map for each year by using the Multivariate Fay Herriot modelling approach. This approach allows us to take advantage of the temporal correlation between poverty rates i.e. an individuals income in year \\(t\\) is likely correlated with their income in year \\(t+1\\).\nThe rest of this tutorial shows how to prepare MFH models using a random 10% sample of the incomedata to estimate the poverty rates.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#step-0-the-data-preparation-phase",
    "href": "50-mfh.html#step-0-the-data-preparation-phase",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.2 Step 0: The Data Preparation Phase",
    "text": "4.2 Step 0: The Data Preparation Phase\nThe MFH estimation process relies on 3 types of data:\n\nThe data containing the outcome variable from which direct estimates will be computed. This is a usually a survey dataset for each year of data including outcome variable (such as income or welfare aggregates), weights, cluster identifiers (i.e. if available, psu or enumeration areas) at the unit level i.e. individual or household. In this example, we call this: survey_dt\nA dataset containing for the right hand side (RHS) variables i.e. indicator estimates representative at the level of the target area for each year. This is often obtained from administrative data sources or geospatial data such as remotely sensed high resolution data. The final RHS dataset ought to include an area id, year and variables of interest. In this example, we call this rhs_dt\nFinally, a shapefile which spatially links each target area to their boundaries on a map. This will contain the area id as well as the geometry object which when visualized will show the shape of the areas in which poverty rates will be estimated. In this example, we call this shp_dt\n\n\nsurvey_dt &lt;- bd_clean |&gt; pin_read(\"pov_direct\")\nrhs_dt &lt;- bd_clean |&gt; pin_read(\"sae_data\")\nshp_dt &lt;- bd_clean |&gt; pin_read(\"geometries\")\n\nBelow is what our datasets look like. These are generally the variables\n\n\nRows: 12,894\nColumns: 7\n$ provlab &lt;fct&gt; Alava, Alava, Alava, Alava, Alava, Alava, Alava, Alava, Alava,…\n$ prov    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ weight  &lt;dbl&gt; 25977.699, 25977.699, 25977.699, 19471.566, 19471.566, 19471.5…\n$ ea_id   &lt;int&gt; 101, 101, 101, 102, 102, 102, 103, 103, 103, 103, 103, 103, 10…\n$ year    &lt;int&gt; 2012, 2013, 2014, 2012, 2013, 2014, 2012, 2012, 2012, 2012, 20…\n$ income  &lt;dbl&gt; 5150.399, 5204.872, 5480.116, 27920.737, 28508.978, 31512.153,…\n$ povline &lt;dbl&gt; 6477.484, 6515.865, 6515.865, 6477.484, 6515.865, 6515.865, 64…\n\n\nsurvey_dt contains our target area identifiers (provlab, prov), the weight variable weight, the cluster id i.e. enumeration area or psu, ea_id, the year, year, the income variable income and the poverty line poverty. In this example, survey_dt of class data.frame is at the level of the individual. It could also be at the level of the household if poverty lines are evaluated at that level.\n\n\nRows: 156\nColumns: 39\nGroups: prov [52]\n$ prov           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ provlab        &lt;fct&gt; Alava, Albacete, Alicante, Almeria, Avila, Badajoz, Bal…\n$ gen            &lt;dbl&gt; 1.591661, 1.512574, 1.531217, 1.379440, 1.469752, 1.412…\n$ age2           &lt;dbl&gt; 0.10662920, 0.15809756, 0.09547348, 0.16773730, 0.00000…\n$ age3           &lt;dbl&gt; 0.3874807, 0.4944282, 0.3547117, 0.4673372, 0.4989752, …\n$ age4           &lt;dbl&gt; 0.245845529, 0.059850454, 0.174966688, 0.005694235, 0.1…\n$ age5           &lt;dbl&gt; 0.19980439, 0.18429053, 0.23276281, 0.18630876, 0.27121…\n$ educ1          &lt;dbl&gt; 0.21756579, 0.22430208, 0.38687301, 0.34254113, 0.40534…\n$ educ2          &lt;dbl&gt; 0.2353839, 0.4386757, 0.3915053, 0.3840035, 0.3181650, …\n$ educ3          &lt;dbl&gt; 0.32395951, 0.10794670, 0.07260912, 0.10053284, 0.20833…\n$ nat1           &lt;dbl&gt; 0.9653883, 0.9837955, 0.9436051, 0.8789277, 1.0000000, …\n$ labor1         &lt;dbl&gt; 0.3692168, 0.4513452, 0.4054253, 0.5191851, 0.4992434, …\n$ labor2         &lt;dbl&gt; 0.000000000, 0.036094381, 0.047285012, 0.040264017, 0.0…\n$ labor3         &lt;dbl&gt; 0.4076923, 0.2834849, 0.3982771, 0.2676284, 0.4326099, …\n$ abs            &lt;dbl&gt; 0.84933922, -0.97816135, 0.98863150, 0.31350535, -1.419…\n$ ntl            &lt;dbl&gt; 0.2091905, 1.1589425, -0.4210840, 1.5472493, -2.1440414…\n$ aec            &lt;dbl&gt; 1.16518757, -0.55990121, -0.66408247, -0.70557145, 4.99…\n$ schyrs         &lt;dbl&gt; 1.70993214, -0.96018920, -0.18004563, 0.22504016, -1.64…\n$ mkt            &lt;dbl&gt; 2.5782541, -1.0468569, -0.2044435, -0.9811182, -1.08118…\n$ age2_X_gen     &lt;dbl&gt; 0.15555592, 0.26628138, 0.15033171, 0.21079949, 0.00000…\n$ age3_X_gen     &lt;dbl&gt; 0.6647560, 0.7222008, 0.5516074, 0.6262462, 0.7645003, …\n$ age4_X_gen     &lt;dbl&gt; 0.36008687, 0.10298379, 0.26866795, 0.01138847, 0.26583…\n$ age5_X_gen     &lt;dbl&gt; 0.35102165, 0.30664168, 0.35115346, 0.28723752, 0.37126…\n$ educ1_X_gen    &lt;dbl&gt; 0.3687830, 0.3682332, 0.5879137, 0.5256249, 0.6863470, …\n$ educ2_X_gen    &lt;dbl&gt; 0.4130652, 0.7220561, 0.6010236, 0.4517964, 0.4494350, …\n$ educ3_X_gen    &lt;dbl&gt; 0.53779476, 0.16167700, 0.12157537, 0.15825036, 0.26582…\n$ nat1_X_gen     &lt;dbl&gt; 1.557049, 1.480165, 1.453477, 1.206699, 1.469752, 1.376…\n$ labor1_X_gen   &lt;dbl&gt; 0.6529375, 0.6858805, 0.5812196, 0.6455309, 0.7541068, …\n$ labor2_X_gen   &lt;dbl&gt; 0.000000000, 0.072188761, 0.086971286, 0.040264017, 0.0…\n$ labor3_X_gen   &lt;dbl&gt; 0.6667055, 0.4938971, 0.6423218, 0.4498768, 0.6474986, …\n$ age2_X_educ3   &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.0…\n$ age3_X_educ3   &lt;dbl&gt; 0.18883381, 0.05231836, 0.02345535, 0.10053284, 0.00000…\n$ age4_X_educ3   &lt;dbl&gt; 0.110497526, 0.036933029, 0.025056921, 0.000000000, 0.1…\n$ age5_X_educ3   &lt;dbl&gt; 0.024628173, 0.018695319, 0.024096847, 0.000000000, 0.0…\n$ nat1_X_educ3   &lt;dbl&gt; 0.32395951, 0.10794670, 0.06702616, 0.06418623, 0.20833…\n$ labor1_X_educ3 &lt;dbl&gt; 0.274032897, 0.052318355, 0.036012387, 0.100532844, 0.1…\n$ labor2_X_educ3 &lt;dbl&gt; 0.000000000, 0.000000000, 0.006859625, 0.000000000, 0.0…\n$ labor3_X_educ3 &lt;dbl&gt; 0.049926612, 0.055628348, 0.029737105, 0.000000000, 0.0…\n$ year           &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2…\n\n\n`rhs_dt is an object of class data.frame created at the level of the target area. It should contain the same target area identifiers as in survey_dt and the year variable year.\n\n\nRows: 52\nColumns: 3\n$ prov     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ provlab  &lt;fct&gt; Alava, Albacete, Alicante, Almeria, Avila, Badajoz, Baleares,…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-2.858067 4..., MULTIPOLYGON (((…\n\n\nshp_dt is an object of class sf, data.frame created at the level of the target area. This is the shapefile for the area of interest for which the poverty map will be estimated. It should contain the same target area ID found in survey_dt as well as rhs_dt.\nA quick summary table on the data needs for the MFH model:\n\n\n\n\n\n\n\n\nData Input Checklist for the Multivariate Fay-Herriot Model\n\n\nDatasets, levels, and Required variables\n\n\nDataset Name\nUnit of Observation\nRequired Variables\n\n\n\n\nsurvey_dt\nIndividual (or Household)\ntarget area identifiers, weights, cluster identifier, year, income/welfare variable, poverty line\n\n\nrhs_dt\nTarget Area (e.g. Province)\ntarget area identifiers, year, covariates (e.g. gen, educ1, schyrs, etc.)\n\n\nshp_dt\nTarget Area (Spatial)\ntarget area identifiers, geometries (e.g. geometry column from sf)\n\n\n\n\n\n\n\nFor ease of use of this tutorial, make sure the variable names match as described across the datasets i.e. use the same target area variables and year variable name across all 3 datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#step-1-direct-estimation-of-poverty-variance-covarance-matrix",
    "href": "50-mfh.html#step-1-direct-estimation-of-poverty-variance-covarance-matrix",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.3 Step 1: Direct Estimation of Poverty & Variance-Covarance Matrix",
    "text": "4.3 Step 1: Direct Estimation of Poverty & Variance-Covarance Matrix\nWe will use the direct HT estimators that use the survey weights in weight variable. First, we calculate the total sample size, the number of provinces, the sample sizes for each province and extract the population sizes for each province/target area from the sizeprov file. For those using the household/individual level survey data, this may be obtained from the sum of the household or individual weights as appropriate.\n\n4.3.1 Simple Direct Estimation\nThe goal is simply to apply the poverty line to the\n\n### a little bit of housekeeping to ensure ease of access \narea_vars &lt;- c(\"prov\", \"provlab\") ### both variables are at the same level. if the levels vary, you would need to combine both variables for effect use\n\ncluster_var &lt;- \"ea_id\"\nweight_var &lt;- \"weight\"\nyear_var &lt;- \"year\"\noutcome_var &lt;- \"income\"\npovline_var &lt;- \"povline\"\n\ncandidate_vars &lt;- colnames(rhs_dt)[!colnames(rhs_dt) %in% c(area_vars, year_var)]\n\n### quickly compute the sample size for each province\nsampsize_dt &lt;- survey_dt |&gt;\n  group_by(!!!syms(area_vars), !!sym(year_var)) |&gt;\n  summarize(N = n(), .groups = \"drop\")\n\n## the poverty line for each is already included within the data. \n## Lets compute the direct estimates for each year of data and create a list of data.frames (equal in length to the number of years) \n## containing the direct estimate, the standard errors and the coefficient of variation. \n\ndirect_dt &lt;- \n  survey_dt |&gt;\n  mutate(pov_indicator = ifelse(income &lt; povline, 1, 0)) |&gt;\n  group_by(!!!syms(area_vars), !!sym(year_var)) |&gt;\n  summarise(direct_povrate = weighted.mean(x = pov_indicator, \n                                           w = !!sym(weight_var),\n                                           na.rm = TRUE),\n            .groups = \"drop\")\n\nHere is what the results look like:\n\ndirect_dt\n\n# A tibble: 156 × 4\n    prov provlab   year direct_povrate\n   &lt;int&gt; &lt;fct&gt;    &lt;int&gt;          &lt;dbl&gt;\n 1     1 Alava     2012          0.244\n 2     1 Alava     2013          0.312\n 3     1 Alava     2014          0.414\n 4     2 Albacete  2012          0.159\n 5     2 Albacete  2013          0.173\n 6     2 Albacete  2014          0.211\n 7     3 Alicante  2012          0.163\n 8     3 Alicante  2013          0.163\n 9     3 Alicante  2014          0.227\n10     4 Almeria   2012          0.297\n# ℹ 146 more rows\n\n\n\n\n4.3.2 Computing the Variance-Covariance Matrix for the Sampling Error of the Direct Estimate\nNext, we quickly estimate the sample variance and covariance for the direct estimator using the survey R package as below. We apply the compute_vcov() function which we have created as a wrapper on the survey::svymean() and survey::svyvar() functions to estimate the variance covariance matrix.\n\nsurvey_dt &lt;- \n  survey_dt |&gt;\n  mutate(pov_indicator = ifelse(!!sym(outcome_var) &lt; !!sym(povline_var), 1, 0))\n\n### we need to reconstruct the variables from long to wide for this\n### this wide format will also be useful for estimating the MFH model as well \n### later on\n\nwidesurvey_dt &lt;- \n  survey_dt |&gt;\n  pivot_wider(\n    id_cols = c(!!!syms(area_vars), !!sym(cluster_var), !!sym(weight_var)),\n    names_from = !!sym(year_var),\n    values_from = c(!!sym(outcome_var), !!sym(povline_var), \"pov_indicator\"),\n    names_glue = \"{.value}{year}\",\n    values_fn = first\n  )\n\n#### now we are ready to compute the variance covariance matrix\n### notice that we have to specify the new column names for the pov_indicator variables as they recreated in widesurvey_dt\nvar_dt &lt;- \n  compute_vcov(dt = widesurvey_dt,\n               domain = area_vars[[1]],\n               ids = cluster_var,\n               weights = weight_var,\n               yvars = paste0(\"pov_indicator\", unique(survey_dt[[year_var]]))) \n\nvar_dt &lt;- \n  var_dt |&gt;\n  rename(!!sym(area_vars[[1]]) := \"domain\") ### quick rename the domain variable to match our area_vars\n\n### lets merge in our sample sizes\nvar_dt &lt;-\n  var_dt |&gt;\n  merge(sampsize_dt %&gt;%\n          dplyr::select(!!sym(area_vars[1]), \"N\") |&gt;\n          unique(), \n        by = area_vars[[1]])\n\nIt is noteworthy that the var_dt object is estimated at the level of the target area. Hence, for reasons that will be made self-evident later, the var_dt is not a matrix for each area rather the matrix is stored rowwise.\n\n4.3.2.1 Handling Low Sample Sizes: Variance Smoothing\nA quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by (You and Hidiroglou 2023). Please see the code and Roxygen comments below explaining the use of the varsmoothie_king() function which computes smoothed variances.\nThe goal now is to use the above varsmoothie_king() function to add additional columns of smoothed variances into our var_dt object.\n\nvarcols &lt;- grep(\"^v_\", names(var_dt), value = TRUE) ## the column names for the variance covariance matrix\n\nvar_dt &lt;-\n  lapply(X = varcols,\n         FUN = function(x){\n\n           z &lt;- varsmoothie_king(domain = var_dt[[area_vars[1]]],\n                                 direct_var = var_dt[[x]],\n                                 sampsize = var_dt[[\"N\"]]) |&gt;\n             as.data.table() |&gt;\n             setnames(old = \"var_smooth\", new = paste0(\"vs\", x)) |&gt;\n             as_tibble()\n           \n\n           return(z)\n\n         }) %&gt;%\n  Reduce(f = \"merge\",\n         x = .) %&gt;%\n  merge(x = var_dt,\n        y = .,\n        by.x = area_vars[[1]],\n        by.y = \"Domain\") |&gt;\n  as_tibble()\n\nNow, you can replace the zero/near zero sample size area MSEs with their smoothed variances.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#step-2-variable-preparation-and-model-selection",
    "href": "50-mfh.html#step-2-variable-preparation-and-model-selection",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.4 Step 2: Variable Preparation and Model Selection",
    "text": "4.4 Step 2: Variable Preparation and Model Selection\n\n4.4.1 Data Preparation for Model Selection & MFH estimation\nThus far, we have careful set up the types of data we require for the MFH model. One final step of variable preparation is necessary to use the eblupUFH and eblupMFH functions. This requires that we reshape the set of candidate variables dataset i.e. the rhs_dt object into wide format. We will also have to reshape our direct estimates and merge this into rhs_dt as well as var_dt. All the data we have created thus far needs to be placed together ultimately for the EBLUP estimation.\n\n## reshaping the rhs_dt from long to wide\nwiderhs_dt &lt;- \n  rhs_dt |&gt;\n  pivot_wider(\n    id_cols = dplyr::all_of(area_vars),\n    names_from = dplyr::all_of(year_var),\n    values_from = dplyr::all_of(candidate_vars),\n    names_glue = \"{.value}{year}\",\n    values_fn = \\(x) dplyr::first(x)  # or `first`, or something more explicit if needed\n  )\n\n## now let us reshape and merge in the poverty rates\n\nmfh_dt &lt;- \n  direct_dt |&gt;\n  pivot_wider(\n    id_cols = dplyr::all_of(area_vars),\n    names_from = dplyr::all_of(year_var),\n    values_from = \"direct_povrate\",\n    names_glue = \"{.value}{year}\",\n    values_fn = \\(x) dplyr::first(x)  # or `first`, or something more explicit if neede\n  ) |&gt;\n  merge(y = widerhs_dt,\n        by = area_vars[[1]]) |&gt;\n  merge(y = var_dt,\n        by = area_vars[[1]])\n\nHere is what the data looks like:\n\nmfh_dt |&gt; glimpse()\n\nRows: 52\nColumns: 127\n$ prov                                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ provlab.x                              &lt;fct&gt; Alava, Albacete, Alicante, Alme…\n$ direct_povrate2012                     &lt;dbl&gt; 0.24394059, 0.15913215, 0.16294…\n$ direct_povrate2013                     &lt;dbl&gt; 0.31179047, 0.17325814, 0.16268…\n$ direct_povrate2014                     &lt;dbl&gt; 0.41422988, 0.21133345, 0.22727…\n$ provlab.y                              &lt;fct&gt; Alava, Albacete, Alicante, Alme…\n$ gen2012                                &lt;dbl&gt; 1.591661, 1.512574, 1.531217, 1…\n$ gen2013                                &lt;dbl&gt; 1.591661, 1.512574, 1.531217, 1…\n$ gen2014                                &lt;dbl&gt; 1.591661, 1.512574, 1.531217, 1…\n$ age22012                               &lt;dbl&gt; 0.10662920, 0.15809756, 0.09547…\n$ age22013                               &lt;dbl&gt; 0.10662920, 0.15809756, 0.09547…\n$ age22014                               &lt;dbl&gt; 0.10662920, 0.15809756, 0.09547…\n$ age32012                               &lt;dbl&gt; 0.3874807, 0.4944282, 0.3547117…\n$ age32013                               &lt;dbl&gt; 0.3874807, 0.4944282, 0.3547117…\n$ age32014                               &lt;dbl&gt; 0.3874807, 0.4944282, 0.3547117…\n$ age42012                               &lt;dbl&gt; 0.245845529, 0.059850454, 0.174…\n$ age42013                               &lt;dbl&gt; 0.245845529, 0.059850454, 0.174…\n$ age42014                               &lt;dbl&gt; 0.245845529, 0.059850454, 0.174…\n$ age52012                               &lt;dbl&gt; 0.19980439, 0.18429053, 0.23276…\n$ age52013                               &lt;dbl&gt; 0.19980439, 0.18429053, 0.23276…\n$ age52014                               &lt;dbl&gt; 0.19980439, 0.18429053, 0.23276…\n$ educ12012                              &lt;dbl&gt; 0.21756579, 0.22430208, 0.38687…\n$ educ12013                              &lt;dbl&gt; 0.21756579, 0.22430208, 0.38687…\n$ educ12014                              &lt;dbl&gt; 0.21756579, 0.22430208, 0.38687…\n$ educ22012                              &lt;dbl&gt; 0.2353839, 0.4386757, 0.3915053…\n$ educ22013                              &lt;dbl&gt; 0.2353839, 0.4386757, 0.3915053…\n$ educ22014                              &lt;dbl&gt; 0.2353839, 0.4386757, 0.3915053…\n$ educ32012                              &lt;dbl&gt; 0.32395951, 0.10794670, 0.07260…\n$ educ32013                              &lt;dbl&gt; 0.32395951, 0.10794670, 0.07260…\n$ educ32014                              &lt;dbl&gt; 0.32395951, 0.10794670, 0.07260…\n$ nat12012                               &lt;dbl&gt; 0.9653883, 0.9837955, 0.9436051…\n$ nat12013                               &lt;dbl&gt; 0.9653883, 0.9837955, 0.9436051…\n$ nat12014                               &lt;dbl&gt; 0.9653883, 0.9837955, 0.9436051…\n$ labor12012                             &lt;dbl&gt; 0.3692168, 0.4513452, 0.4054253…\n$ labor12013                             &lt;dbl&gt; 0.3692168, 0.4513452, 0.4054253…\n$ labor12014                             &lt;dbl&gt; 0.3692168, 0.4513452, 0.4054253…\n$ labor22012                             &lt;dbl&gt; 0.000000000, 0.036094381, 0.047…\n$ labor22013                             &lt;dbl&gt; 0.000000000, 0.036094381, 0.047…\n$ labor22014                             &lt;dbl&gt; 0.000000000, 0.036094381, 0.047…\n$ labor32012                             &lt;dbl&gt; 0.4076923, 0.2834849, 0.3982771…\n$ labor32013                             &lt;dbl&gt; 0.4076923, 0.2834849, 0.3982771…\n$ labor32014                             &lt;dbl&gt; 0.4076923, 0.2834849, 0.3982771…\n$ abs2012                                &lt;dbl&gt; 0.84933922, -0.97816135, 0.9886…\n$ abs2013                                &lt;dbl&gt; 0.84933922, -0.97816135, 0.9886…\n$ abs2014                                &lt;dbl&gt; 0.84933922, -0.97816135, 0.9886…\n$ ntl2012                                &lt;dbl&gt; 0.2091905, 1.1589425, -0.421084…\n$ ntl2013                                &lt;dbl&gt; 0.2091905, 1.1589425, -0.421084…\n$ ntl2014                                &lt;dbl&gt; 0.2091905, 1.1589425, -0.421084…\n$ aec2012                                &lt;dbl&gt; 1.16518757, -0.55990121, -0.664…\n$ aec2013                                &lt;dbl&gt; 1.16518757, -0.55990121, -0.664…\n$ aec2014                                &lt;dbl&gt; 1.16518757, -0.55990121, -0.664…\n$ schyrs2012                             &lt;dbl&gt; 1.70993214, -0.96018920, -0.180…\n$ schyrs2013                             &lt;dbl&gt; 1.70993214, -0.96018920, -0.180…\n$ schyrs2014                             &lt;dbl&gt; 1.70993214, -0.96018920, -0.180…\n$ mkt2012                                &lt;dbl&gt; 2.5782541, -1.0468569, -0.20444…\n$ mkt2013                                &lt;dbl&gt; 2.5782541, -1.0468569, -0.20444…\n$ mkt2014                                &lt;dbl&gt; 2.5782541, -1.0468569, -0.20444…\n$ age2_X_gen2012                         &lt;dbl&gt; 0.15555592, 0.26628138, 0.15033…\n$ age2_X_gen2013                         &lt;dbl&gt; 0.15555592, 0.26628138, 0.15033…\n$ age2_X_gen2014                         &lt;dbl&gt; 0.15555592, 0.26628138, 0.15033…\n$ age3_X_gen2012                         &lt;dbl&gt; 0.6647560, 0.7222008, 0.5516074…\n$ age3_X_gen2013                         &lt;dbl&gt; 0.6647560, 0.7222008, 0.5516074…\n$ age3_X_gen2014                         &lt;dbl&gt; 0.6647560, 0.7222008, 0.5516074…\n$ age4_X_gen2012                         &lt;dbl&gt; 0.36008687, 0.10298379, 0.26866…\n$ age4_X_gen2013                         &lt;dbl&gt; 0.36008687, 0.10298379, 0.26866…\n$ age4_X_gen2014                         &lt;dbl&gt; 0.36008687, 0.10298379, 0.26866…\n$ age5_X_gen2012                         &lt;dbl&gt; 0.35102165, 0.30664168, 0.35115…\n$ age5_X_gen2013                         &lt;dbl&gt; 0.35102165, 0.30664168, 0.35115…\n$ age5_X_gen2014                         &lt;dbl&gt; 0.35102165, 0.30664168, 0.35115…\n$ educ1_X_gen2012                        &lt;dbl&gt; 0.3687830, 0.3682332, 0.5879137…\n$ educ1_X_gen2013                        &lt;dbl&gt; 0.3687830, 0.3682332, 0.5879137…\n$ educ1_X_gen2014                        &lt;dbl&gt; 0.3687830, 0.3682332, 0.5879137…\n$ educ2_X_gen2012                        &lt;dbl&gt; 0.4130652, 0.7220561, 0.6010236…\n$ educ2_X_gen2013                        &lt;dbl&gt; 0.4130652, 0.7220561, 0.6010236…\n$ educ2_X_gen2014                        &lt;dbl&gt; 0.4130652, 0.7220561, 0.6010236…\n$ educ3_X_gen2012                        &lt;dbl&gt; 0.53779476, 0.16167700, 0.12157…\n$ educ3_X_gen2013                        &lt;dbl&gt; 0.53779476, 0.16167700, 0.12157…\n$ educ3_X_gen2014                        &lt;dbl&gt; 0.53779476, 0.16167700, 0.12157…\n$ nat1_X_gen2012                         &lt;dbl&gt; 1.557049, 1.480165, 1.453477, 1…\n$ nat1_X_gen2013                         &lt;dbl&gt; 1.557049, 1.480165, 1.453477, 1…\n$ nat1_X_gen2014                         &lt;dbl&gt; 1.557049, 1.480165, 1.453477, 1…\n$ labor1_X_gen2012                       &lt;dbl&gt; 0.6529375, 0.6858805, 0.5812196…\n$ labor1_X_gen2013                       &lt;dbl&gt; 0.6529375, 0.6858805, 0.5812196…\n$ labor1_X_gen2014                       &lt;dbl&gt; 0.6529375, 0.6858805, 0.5812196…\n$ labor2_X_gen2012                       &lt;dbl&gt; 0.000000000, 0.072188761, 0.086…\n$ labor2_X_gen2013                       &lt;dbl&gt; 0.000000000, 0.072188761, 0.086…\n$ labor2_X_gen2014                       &lt;dbl&gt; 0.000000000, 0.072188761, 0.086…\n$ labor3_X_gen2012                       &lt;dbl&gt; 0.6667055, 0.4938971, 0.6423218…\n$ labor3_X_gen2013                       &lt;dbl&gt; 0.6667055, 0.4938971, 0.6423218…\n$ labor3_X_gen2014                       &lt;dbl&gt; 0.6667055, 0.4938971, 0.6423218…\n$ age2_X_educ32012                       &lt;dbl&gt; 0.000000000, 0.000000000, 0.000…\n$ age2_X_educ32013                       &lt;dbl&gt; 0.000000000, 0.000000000, 0.000…\n$ age2_X_educ32014                       &lt;dbl&gt; 0.000000000, 0.000000000, 0.000…\n$ age3_X_educ32012                       &lt;dbl&gt; 0.18883381, 0.05231836, 0.02345…\n$ age3_X_educ32013                       &lt;dbl&gt; 0.18883381, 0.05231836, 0.02345…\n$ age3_X_educ32014                       &lt;dbl&gt; 0.18883381, 0.05231836, 0.02345…\n$ age4_X_educ32012                       &lt;dbl&gt; 0.110497526, 0.036933029, 0.025…\n$ age4_X_educ32013                       &lt;dbl&gt; 0.110497526, 0.036933029, 0.025…\n$ age4_X_educ32014                       &lt;dbl&gt; 0.110497526, 0.036933029, 0.025…\n$ age5_X_educ32012                       &lt;dbl&gt; 0.024628173, 0.018695319, 0.024…\n$ age5_X_educ32013                       &lt;dbl&gt; 0.024628173, 0.018695319, 0.024…\n$ age5_X_educ32014                       &lt;dbl&gt; 0.024628173, 0.018695319, 0.024…\n$ nat1_X_educ32012                       &lt;dbl&gt; 0.32395951, 0.10794670, 0.06702…\n$ nat1_X_educ32013                       &lt;dbl&gt; 0.32395951, 0.10794670, 0.06702…\n$ nat1_X_educ32014                       &lt;dbl&gt; 0.32395951, 0.10794670, 0.06702…\n$ labor1_X_educ32012                     &lt;dbl&gt; 0.274032897, 0.052318355, 0.036…\n$ labor1_X_educ32013                     &lt;dbl&gt; 0.274032897, 0.052318355, 0.036…\n$ labor1_X_educ32014                     &lt;dbl&gt; 0.274032897, 0.052318355, 0.036…\n$ labor2_X_educ32012                     &lt;dbl&gt; 0.000000000, 0.000000000, 0.006…\n$ labor2_X_educ32013                     &lt;dbl&gt; 0.000000000, 0.000000000, 0.006…\n$ labor2_X_educ32014                     &lt;dbl&gt; 0.000000000, 0.000000000, 0.006…\n$ labor3_X_educ32012                     &lt;dbl&gt; 0.049926612, 0.055628348, 0.029…\n$ labor3_X_educ32013                     &lt;dbl&gt; 0.049926612, 0.055628348, 0.029…\n$ labor3_X_educ32014                     &lt;dbl&gt; 0.049926612, 0.055628348, 0.029…\n$ v_pov_indicator2012pov_indicator2012   &lt;dbl&gt; 0.0098654852, 0.0072993525, 0.0…\n$ v_pov_indicator2013pov_indicator2013   &lt;dbl&gt; 0.0053009986, 0.0067381016, 0.0…\n$ v_pov_indicator2014pov_indicator2014   &lt;dbl&gt; 0.0093328620, 0.0023188905, 0.0…\n$ v_pov_indicator2012pov_indicator2013   &lt;dbl&gt; 0.0058723217, 0.0069408865, 0.0…\n$ v_pov_indicator2012pov_indicator2014   &lt;dbl&gt; 0.0070770971, 0.0030800647, 0.0…\n$ v_pov_indicator2013pov_indicator2014   &lt;dbl&gt; 0.0048793230, 0.0030653366, 0.0…\n$ N                                      &lt;int&gt; 24, 43, 135, 50, 14, 124, 158, …\n$ vsv_pov_indicator2012pov_indicator2012 &lt;dbl&gt; 0.0097321234, 0.0053156454, 0.0…\n$ vsv_pov_indicator2013pov_indicator2013 &lt;dbl&gt; 0.009595262, 0.005426157, 0.001…\n$ vsv_pov_indicator2014pov_indicator2014 &lt;dbl&gt; 0.0090107365, 0.0055231048, 0.0…\n$ vsv_pov_indicator2012pov_indicator2013 &lt;dbl&gt; 0.0089483129, 0.0050001825, 0.0…\n$ vsv_pov_indicator2012pov_indicator2014 &lt;dbl&gt; 0.0073280234, 0.0040019268, 0.0…\n$ vsv_pov_indicator2013pov_indicator2014 &lt;dbl&gt; 0.0068160750, 0.0036177536, 0.0…\n\n\n\n\n4.4.2 Variable Selection Process\nNext, we apply a simple variable selection process which employs the stepwise regression algorithm using the AIC selection criteria as in described by (Yamashita, Yamashita, and Kamimura 2007). The function step_wrapper() implemented below is a wrapper to the stepAIC() function carries all the perfunctory cleaning necessary use the stepAIC() function. This includes dropping columns that are entirely missing (NA) and keep only complete cases/observations and remove perfectly or near collinear variables and combinations using the variance inflation method.\n\ncandidate_vars &lt;- \n  expand.grid(var = candidate_vars,\n              year = unique(survey_dt[[year_var]])) |&gt;\n  transform(name = paste0(var, year)) |&gt;\n  dplyr::select(name) |&gt;\n  unlist() |&gt;\n  unname()\n  \n  \n## extract the year identifiers in each variance covariance name\nvaryear_list &lt;- stringr::str_extract_all(varcols, \"\\\\d{4}\")\n\n## Keep only those where both years are the same i.e. the variances\nvariance_cols &lt;- varcols[lengths(varyear_list) == 2 & \n                           sapply(varyear_list, function(x) x[1] == x[2])]\n\n## replace the variances-covariances that are zero or near zero with their smoothed counterparts\nmfh_dt &lt;- \n  mfh_dt |&gt;\n  mutate(across(\n    starts_with(\"v_\"),\n    ~ if_else(abs(.x) &lt;= 1e-4, get(paste0(\"vsv\", str_remove(cur_column(), \"^v\"))), .x),\n    .names = \"{.col}\"\n  ))\n\nfh_step &lt;-\n  lapply(X = paste0(\"direct_povrate\", unique(survey_dt[[year_var]])),\n         FUN = function(x){\n\n           model_obj &lt;-\n           step_wrapper(dt = mfh_dt,\n                        xvars = candidate_vars,\n                        y = x,\n                        cor_thresh = 0.7,\n                        k = log(nrow(mfh_dt))) ### using log(n) to force BIC selection\n\n           xx &lt;- names(model_obj$coefficients)[!grepl(\"(Intercept)\",\n                                                      names(model_obj$coefficients))]\n           return(xx)\n\n         })\n\n\n# mfh_formula &lt;- \n#   mapply(x = paste0(\"direct_povrate\", unique(survey_dt[[year_var]])),\n#          y = variance_cols,\n#          FUN = function(x, y){\n#            \n#            fh_step &lt;- step_wrapper_fh(dt = mfh_dt,\n#                                       xvars = candidate_vars,\n#                                       y = x,\n#                                       cor_thresh = 0.8,\n#                                       criteria = \"BIC\",\n#                                       vardir = y,\n#                                       transformation = \"no\")\n#            \n#            return(fh_step$fixed)\n# \n#          },\n#          SIMPLIFY = FALSE)\n\n\nmfh_formula &lt;-\n  mapply(FUN = function(x, rhs){\n\n    y &lt;- as.formula(paste0(x, \" ~ \", paste(rhs, collapse = \" + \")))\n\n    return(y)\n\n  },\n  x = paste0(\"direct_povrate\", unique(survey_dt[[year_var]])),\n  rhs = fh_step |&gt; map(~.x[1:3]),\n  SIMPLIFY = FALSE)\n\n### here is what the 3 equations look like\nmfh_formula\n\n$direct_povrate2012\ndirect_povrate2012 ~ age52012 + labor12012 + labor22012\n&lt;environment: 0x0000022891eb0af8&gt;\n\n$direct_povrate2013\ndirect_povrate2013 ~ mkt2012 + age3_X_gen2012 + age3_X_educ32012\n&lt;environment: 0x0000022891e8ca88&gt;\n\n$direct_povrate2014\ndirect_povrate2014 ~ age42012 + nat12012 + labor12012\n&lt;environment: 0x0000022892214120&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#step-3-fitting-the-multivariate-fay-herriot-model",
    "href": "50-mfh.html#step-3-fitting-the-multivariate-fay-herriot-model",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.5 Step 3: Fitting the Multivariate Fay Herriot Model",
    "text": "4.5 Step 3: Fitting the Multivariate Fay Herriot Model\nNext, we show how to use the msae R package to estimate the Empirical Best Linear Unbiased Predictor (EBLUP) for the poverty map using the eblupMFH2() which allow for time series fay herriot estimation under homoskedastic assumptions. For completeness, we also briefly perform the previous described direct estimation in step 1, using the eblupUFH() function as well as the eblupMFH1() for the fay herriot model.\n\nmfh_dt1 &lt;- mfh_dt |&gt; mutate(across(contains(\"v_pov_\"), ~ 0.1)) |&gt; as_tibble()\n\n# univariate FH\nmodel0_obj &lt;- eblupUFH(mfh_formula, vardir = varcols, data = mfh_dt)\nbd_out |&gt; pin_write(model0_obj, type = \"rds\")\n\n# multivariate FH 1\ntic(msg = \"eblupMFH1\")\nmodel1_obj &lt;- eblupMFH1(mfh_formula, vardir = varcols, data = mfh_dt, MAXITER = 10000, PRECISION = 0.01)\nbd_out |&gt; pin_write(model1_obj, type = \"rds\")\ntoc()\n\n# multivariate FH 2\ntic(msg = \"eblupMFH2\")\nmodel2_obj &lt;- eblupMFH2(mfh_formula, vardir = varcols, data = mfh_dt, MAXITER = 1e10, PRECISION = 0.01)\nbd_out |&gt; pin_write(model2_obj, type = \"rds\")\ntoc()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  },
  {
    "objectID": "50-mfh.html#step-4-post-estimation-diagnostics-model-assumption-checks-for-linearity-normality-and-outliers",
    "href": "50-mfh.html#step-4-post-estimation-diagnostics-model-assumption-checks-for-linearity-normality-and-outliers",
    "title": "3  Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach",
    "section": "4.6 Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers",
    "text": "4.6 Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers\nWe now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas.\n\n4.6.1 Linearity Test\nTo assess whether a linear regression model may be incorrectly specified — for instance, due to omitted variables or incorrect functional form — we can use Ramsey’s Regression Equation Specification Error Test (RESET). This test examines whether adding nonlinear combinations (typically powers) of the model’s fitted values significantly improves the model. However, the outcome of interest now is the model MSEs. A significant test result (low p-value) suggests the model is mis-specified and may benefit from additional or transformed predictors.\nWe implement Ramsey’s RESET test in R using the resettest() function from the lmtest package:\n\n### lets create a dataframe with the errors and estimated poverty rates\n\neblup_dt &lt;- model2_obj$eblup\nmse_dt &lt;- model2_obj$MSE\n\ncolnames(eblup_dt) &lt;- paste0(\"eblup_\", colnames(eblup_dt))\ncolnames(mse_dt) &lt;- paste0(\"mse_\", colnames(mse_dt))\n\nreset_dt &lt;- bind_cols(eblup_dt, mse_dt) |&gt; as_tibble()\n  \n### lets perform the reset test on the pairs of variables as appropriate i.e. poverty = B0 + B1*MSE\n\ntest_list &lt;- \n  lapply(unique(survey_dt[[year_var]]), function(x) {\n    \n    # Subset only the columns for year x\n    dt &lt;- reset_dt |&gt;\n      dplyr::select(matches(paste0(x, \"$\")))  # Select columns ending with current year\n    \n    yvar &lt;- colnames(dt)[grepl(\"^eblup_\", colnames(dt))]\n    xvar &lt;- colnames(dt)[grepl(\"^mse_\", colnames(dt))]\n    \n    # Create formula with squared and cubed terms using I()\n    form &lt;- as.formula(paste0(\n      xvar, \" ~ \", \n      yvar, \" + I(\", yvar, \"^2)\"\n    ))\n    \n    model_obj &lt;- lm(form, data = dt)\n    \n    return(model_obj)\n    \n  })\n\nreset_dt2 &lt;- \n  reset_dt |&gt; \n  mutate(id = row_number()) |&gt; \n  pivot_longer(c(contains(\"eblup\"), contains(\"mse\"))) |&gt; \n  mutate(\n    year = str_extract(name, \"\\\\d{4}\") |&gt; as.numeric(),\n    var =  str_extract(name, \"eblup|mse\")\n  ) |&gt; \n  dplyr::select(-name) |&gt; \n  pivot_wider(names_from = var, values_from = value )\n  \nreset_dt2 |&gt; \n  ggplot() + \n  aes(x = eblup, y = mse) + \n  geom_point() + \n  facet_wrap(. ~ year) + \n  geom_smooth() + \n  theme_bw()\n\n\n\n\n\n\n\n\nHere is what the results look like:\n\ntest_list %&gt;% lapply(X = ., FUN = summary)\n\n[[1]]\n\nCall:\nlm(formula = form, data = dt)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.056927 -0.000966  0.000134  0.001378  0.015740 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                   -0.04001    0.01351  -2.961  0.00471 **\neblup_direct_povrate2012       0.39620    0.14987   2.644  0.01098 * \nI(eblup_direct_povrate2012^2) -0.91295    0.40010  -2.282  0.02688 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.009032 on 49 degrees of freedom\nMultiple R-squared:  0.1827,    Adjusted R-squared:  0.1494 \nF-statistic: 5.477 on 2 and 49 DF,  p-value: 0.007131\n\n\n[[2]]\n\nCall:\nlm(formula = form, data = dt)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.112311 -0.002595 -0.000842  0.002170  0.037487 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   -0.09305    0.02497  -3.726 0.000503 ***\neblup_direct_povrate2013       0.90590    0.27363   3.311 0.001752 ** \nI(eblup_direct_povrate2013^2) -2.08745    0.72264  -2.889 0.005746 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01846 on 49 degrees of freedom\nMultiple R-squared:  0.2396,    Adjusted R-squared:  0.2085 \nF-statistic: 7.718 on 2 and 49 DF,  p-value: 0.001219\n\n\n[[3]]\n\nCall:\nlm(formula = form, data = dt)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.145920 -0.003367 -0.001102  0.004768  0.051751 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   -0.16560    0.04583  -3.613 0.000712 ***\neblup_direct_povrate2014       1.14465    0.34956   3.275 0.001946 ** \nI(eblup_direct_povrate2014^2) -1.89832    0.64619  -2.938 0.005027 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02402 on 49 degrees of freedom\nMultiple R-squared:  0.2342,    Adjusted R-squared:  0.2029 \nF-statistic: 7.492 on 2 and 49 DF,  p-value: 0.001449\n\n\nThe results particularly for the final 2 years indicate that we might need to retransform the variables in the model as there are potentially non linear relationships which our model is not capturing. Perhaps creating higher order polynomials and other variable transformations of our right hand side variables reduce the error rates within the model.\n\n\n4.6.2 Evaluating the Normality Assumption\n\n4.6.2.1 The Shapiro Wilks Test\nWe use the shapiro wilks test of normality using the shapiro.test() function in base R. The Shapiro-Wilk test assesses whether a sample of data is drawn from a normally distributed population. It does so by comparing the order statistics (i.e., sorted values) of the sample to the expected values under a normal distribution. Specifically, the test statistic \\(W\\) is a ratio of the squared correlation between the observed sample quantiles and the corresponding normal quantiles.\nFirst, we perform the shapiro wilks normality test on the model errors, \\(\\varepsilon\\). We show both the normality distribution histogram as well as the qqplots as below:\n\n### first lets replace the negative values with 0\neblup_dt[eblup_dt &lt; 0] &lt;- 0\n\n### evaluating the normality assumption\n\n#### first lets create a residual table by looking at the difference between actual and predicted poverty rates\nresid_dt &lt;- mfh_dt[,paste0(\"direct_povrate\", unique(survey_dt[[year_var]]))] - eblup_dt\n\n### perform the shapiro test\n\nshapiro_obj &lt;- apply(resid_dt, 2, shapiro.test)\n\nsummary_dt &lt;- \n  data.frame(Time = names(shapiro_obj),\n             W = lapply(X = shapiro_obj,\n                        FUN = function(x){\n                          \n                          return(x$statistic[[1]])\n                          \n                        }) %&gt;%\n               as.numeric(),\n             p_value = lapply(X = shapiro_obj,\n                              FUN = function(x){\n                                \n                                return(x$p.value)\n                                \n                              }) %&gt;%\n               as.numeric())\n\n### plot the results\nsummary_dt &lt;- \n  summary_dt %&gt;%\n  mutate(label = paste0(\"W = \", round(W, 3), \"\\n\", \"p = \", signif(p_value, 3)))\n\nresid_dt %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"Time\", \n               values_to = \"Residual\") %&gt;%\n  ggplot(aes(x = Residual)) + \n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"white\") + \n  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),\n            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +\n  facet_wrap(~Time, scales = \"free\") + \n  theme_minimal() + \n  labs(title = \"Residual Histograms by Time Period\")\n\n\n\n\n\n\n\n### here's how to create qqplots\nresid_dt %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = \"Time\",\n               values_to = \"Residual\") %&gt;%\n  ggplot(aes(sample = Residual)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Time, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"QQ Plots of Residuals by Time Period\")\n\n\n\n\n\n\n\n\nLikewise, we test the normality of the random effect variable\n\n#### For the random effects\nraneff_dt &lt;- as.data.frame(model2_obj$randomEffect)\n\n### lets run the shapiro wilks tests again\nshapiro_obj &lt;- apply(raneff_dt, 2, shapiro.test)\n\n\nsummary_dt &lt;- \n  data.frame(Time = names(shapiro_obj),\n             W = lapply(X = shapiro_obj,\n                        FUN = function(x){\n                          \n                          return(x$statistic[[1]])\n                          \n                        }) %&gt;%\n               as.numeric(),\n             p_value = lapply(X = shapiro_obj,\n                              FUN = function(x){\n                                \n                                return(x$p.value)\n                                \n                              }) %&gt;%\n               as.numeric())\n\n### plot the results\nsummary_dt &lt;- \n  summary_dt %&gt;%\n  mutate(label = paste0(\"W = \", round(W, 3), \"\\n\", \"p = \", signif(p_value, 3)))\n\nraneff_dt %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"Time\", \n               values_to = \"RandEff\") %&gt;%\n  ggplot(aes(x = RandEff)) + \n  geom_histogram(bins = 10, fill = \"darkorange\", color = \"white\") + \n  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),\n            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +\n  facet_wrap(~Time, scales = \"free\") + \n  theme_minimal() + \n  labs(title = \"Random Effects Histograms by Time Period\")\n\n\n\n\n\n\n\n\nIn both cases, we compare the p-value to the 0.05 level of significance. The results suggest that in most cases we have to reject the null hypothesis of normally distributed model errors and random effects. This doesn’t affect the validity of our poverty estimates for the Multivariate Fay Herriot model. However, subsequent analysis that will assume a normal distribution of the model errors cannot be performed. One good example of this is the statistical significance test for changes in poverty rates over time. For the purposes of this tutorial, we will carry on to show how to perform this under the assumption of normally distributed model errors. However, if the test for normality fails, this test cannot be carried out.\nNext, we will show the benefits of small area estimation over direct estimation\n\n\n\n4.6.3 Comparing Direct Estimation to Multivariate Model Outputs\n\nmodel2mse_dt &lt;- \n  model2_obj$MSE |&gt;\n  mutate(!!sym(area_vars[[1]]) := 1:n()) |&gt;\n  pivot_longer(\n    cols = starts_with(\"direct_povrate\"),\n    names_to = \"year\",\n    names_pattern = \"direct_povrate(\\\\d+)\",  # Extract just the digits\n    values_to = \"modelMSE\"\n  ) |&gt;\n  mutate(year = as.integer(year))\n\nmodel2pov_dt &lt;- \n  model2_obj$eblup |&gt;\n  mutate(!!sym(area_vars[[1]]) := 1:n()) |&gt;\n  pivot_longer(\n    cols = starts_with(\"direct_povrate\"),\n    names_to = \"year\",\n    names_pattern = \"direct_povrate(\\\\d+)\",  # Extract just the digits\n    values_to = \"modelpov\"\n  ) |&gt;\n  mutate(year = as.integer(year))\n\nmodel2pov_dt &lt;- merge(model2mse_dt, model2pov_dt)\n\n\nmodel2pov_dt &lt;- \n  model2pov_dt |&gt;\n  mutate(modelCV = sqrt(modelMSE) / modelpov)\n\n\nmodel2pov_dt &lt;- merge(model2pov_dt, \n                      direct_dt, \n                      by = c(area_vars[[1]], \n                             year_var))\n\n### compute direct CVs and include in `model2pov_dt`\n\n\n## we have to reshape the direct estimates data in mfh_dt from wide to long\n## and then merge with model2pov_dt \nmodel2pov_dt &lt;- \nmfh_dt |&gt;\n  dplyr::select(starts_with(variance_cols),  # add this!\n                all_of(area_vars[[1]])) |&gt;\n  pivot_longer(\n    cols = -all_of(area_vars[[1]]),\n    names_to = \"indicator_year\",\n    values_to = \"value\"\n  ) |&gt;\n  mutate(\n    type = case_when(\n      str_starts(indicator_year, \"v_pov_indicator\") ~ \"variance\"\n    ),\n    year = str_extract(indicator_year, \"\\\\d{4}\")\n  ) |&gt;\n  dplyr::select(-indicator_year) |&gt;\n  pivot_wider(\n    names_from = type,\n    values_from = value\n  ) |&gt;\n  mutate(year = as.integer(year)) |&gt;\n  merge(model2pov_dt,\n        by = c(area_vars[[1]], year_var),\n        all = TRUE) |&gt;\n  mutate(direct_CV = sqrt(variance) / direct_povrate) |&gt;\n  merge(sampsize_dt %&gt;%\n          dplyr::select(!!sym(area_vars[1]), \"N\") |&gt;\n          unique(), \n        by = area_vars[[1]])\n\nNow lets plot the direct against the model CVs to get a sense for the gains made as a result of applying the MFH modelling approach over direct estimation. We will also show the sample sizes for each year to show that large sample size reduce the gains from small area estimation.\n\nnumeric_cols &lt;- sapply(model2pov_dt, is.numeric)\nmodel2pov_dt_clean &lt;- model2pov_dt[complete.cases(model2pov_dt[, numeric_cols]) &\n                                     apply(model2pov_dt[, numeric_cols], 1, function(row)\n                                       all(is.finite(row))), ]\n  \n\n# Make year a factor using year_var\nmodel2pov_dt_clean[[year_var]] &lt;- as.factor(model2pov_dt_clean[[year_var]])\n\n# Compute max values for scaling\nmax_n &lt;- max(model2pov_dt_clean$N, na.rm = TRUE)\nmax_cv &lt;- max(model2pov_dt_clean$direct_CV, na.rm = TRUE)\n\n# Create the plot\nggplot(model2pov_dt_clean, aes(x = .data[[year_var]])) +\n  geom_line(aes(y = direct_CV, group = 1, color = \"Direct CV\"), size = 1) +\n  geom_line(aes(y = modelCV, group = 1, color = \"Model CV\"), size = 1) +\n  geom_point(aes(y = N / max_n * max_cv, color = \"Sample Size (N)\"),\n             shape = 1, size = 1.5) +\n  facet_wrap(vars(.data[[area_vars[[1]]]]), scales = \"free_y\") +\n  scale_y_continuous(\n    name = \"Coefficient of Variation (CV)\",\n    sec.axis = sec_axis(~ . * max_n / max_cv, name = \"Sample Size (N)\")\n  ) +\n  scale_color_manual(values = c(\"Direct CV\" = \"darkorange\",\n                                \"Model CV\" = \"steelblue\",\n                                \"Sample Size (N)\" = \"grey40\")) +\n  labs(x = \"Year\", color = \"\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.title.y.right = element_text(color = \"grey40\"),\n    axis.title.y.left = element_text(color = \"black\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\nThe other advantage of MFH estimation is the smoothing of the model estimates over the unit variate model. We can show this below as well:\n\nmodel2pov_dt[[year_var]] &lt;- as.factor(model2pov_dt[[year_var]])\n\n## first lets merge in the results of the eblupUFH model\n\nufh_dt &lt;- \n  model0_obj$eblup |&gt;\n  mutate(!!sym(area_vars[[1]]) := 1:n()) |&gt;\n  pivot_longer(\n    cols = starts_with(\"direct_povrate\"),\n    names_to = \"year\",\n    names_pattern = \"direct_povrate(\\\\d+)\",  # Extract just the digits\n    values_to = \"ufh_povrate\"\n  )\n\nmodel2pov_dt &lt;- merge(model2pov_dt,\n                      ufh_dt,\n                      by = c(area_vars[[1]], year_var))\n\n## now we make a similar plot but comparing the UFH vs MFH model estimates to check for the smoothness of the estimation \n\n# Compute max values for scaling\nmax_n &lt;- max(model2pov_dt$N, na.rm = TRUE)\n\n\nggplot(model2pov_dt, aes(x = .data[[year_var]])) +\n  geom_line(aes(y = ufh_povrate, group = 1, color = \"UFH Poverty\"), size = 1) +\n  geom_line(aes(y = modelpov, group = 1, color = \"MFH Poverty\"), size = 1) +\n  geom_point(aes(y = N / max_n, color = \"Sample Size (N)\"),\n             shape = 1, size = 1.5) +\n  facet_wrap(vars(.data[[area_vars[[1]]]]), scales = \"free_y\") +\n  scale_y_continuous(\n    name = \"Poverty Rates\",\n    sec.axis = sec_axis(~ . * max_n, name = \"Sample Size (N)\")\n  ) +\n  scale_color_manual(values = c(\"UFH Poverty\" = \"darkorange\",\n                                \"MFH Poverty\" = \"steelblue\",\n                                \"Sample Size (N)\" = \"grey40\")) +\n  labs(x = \"Year\", color = \"\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.title.y.right = element_text(color = \"grey40\"),\n    axis.title.y.left = element_text(color = \"black\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\nIt can be seen from the plots that the provinces in with the highest variance in UFH poverty estimates tend to provide smoother MFH estimates particularly in the provinces with smaller samples. This shows the power of the MFH approach in taking advantage of the across year correlation of poverty.\nNext we will assume our model errors were normally distributed in order to show how to statistically test to see if poverty has changed between given years.\n\n\n4.6.4 Did the poverty rates change over time?\nNext we can call the pbmcpeMFH2() function, which returns the EBLUP, as well as, the MSEs of the EBLUPs for each time point, and the MCPEs for each pair of time points based on the MFH model 2 as follows:\n\nset.seed(123)\ntic(pbmcpeMFH2)\nmcpemfh2_obj &lt;- \n  pbmcpeMFH2(formula = mfh_formula,\n           vardir = varcols,\n           nB = 50,\n           data = mfh_dt, \n           MAXITER = 1e10, \n           PRECISION = 1e-2)\nbd_clean |&gt; pin_write(mcpemfh2_obj, type = \"rds\")\ntoc()\n\nThe above function takes the difference between any two time periods and prepares a table of differences for each area include the MCPE estimated error rates as well as lower and upper bounds given a specified confidence level. See the function implemented to compare periods 1 and 2 (years 2012 and 2013)\n\ncomp12_obj &lt;- compare_mfh2()\ncomp23_obj &lt;- compare_mfh2(period_list = c(2, 3))\ncomp13_obj &lt;- compare_mfh2(period_list = c(1, 3))\n\nSee the plots below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee the data created\n\nComparing the years 2012 and 2013\n\n\ncomp12_obj$df %&gt;% head() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiff\nmse\nalpha\nzq\nlb\nub\nsignificant\nindex\n\n\n\n\n0.02009\n0.0092704\n0.05\n1.959964\n-0.1686207\n0.2088007\nNot Significant\n1\n\n\n0.00417\n0.0019928\n0.05\n1.959964\n-0.0833244\n0.0916644\nNot Significant\n2\n\n\n0.00105\n0.0114769\n0.05\n1.959964\n-0.2089219\n0.2110219\nNot Significant\n3\n\n\n-0.00676\n0.0505814\n0.05\n1.959964\n-0.4475619\n0.4340419\nNot Significant\n4\n\n\n0.00000\n0.0000000\n0.05\n1.959964\n-0.0000241\n0.0000241\nNot Significant\n5\n\n\n0.00102\n0.0141145\n0.05\n1.959964\n-0.2318327\n0.2338727\nNot Significant\n6\n\n\n\n\n\n\nComparing the years 2013 and 2014\n\n\ncomp23_obj$df %&gt;% head() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiff\nmse\nalpha\nzq\nlb\nub\nsignificant\nindex\n\n\n\n\n0.094150\n0.1543261\n0.05\n1.959964\n-0.6758094\n0.8641094\nNot Significant\n1\n\n\n0.052870\n0.1868019\n0.05\n1.959964\n-0.7942380\n0.8999780\nNot Significant\n2\n\n\n0.088530\n1.3239427\n0.05\n1.959964\n-2.1666577\n2.3437177\nNot Significant\n3\n\n\n0.052230\n6.9538819\n0.05\n1.959964\n-5.1162370\n5.2206970\nNot Significant\n4\n\n\n0.048865\n0.1154835\n0.05\n1.959964\n-0.6171870\n0.7149170\nNot Significant\n5\n\n\n0.102160\n0.0055322\n0.05\n1.959964\n-0.0436195\n0.2479395\nNot Significant\n6\n\n\n\n\n\n\nComparing the years 2012 and 2014\n\n\ncomp13_obj$df %&gt;% head() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiff\nmse\nalpha\nzq\nlb\nub\nsignificant\nindex\n\n\n\n\n0.114240\n0.2379474\n0.05\n1.959964\n-0.8418275\n1.0703075\nNot Significant\n1\n\n\n0.057040\n0.2267551\n0.05\n1.959964\n-0.8762715\n0.9903515\nNot Significant\n2\n\n\n0.089580\n1.5809691\n0.05\n1.959964\n-2.3748120\n2.5539720\nNot Significant\n3\n\n\n0.045470\n8.1882867\n0.05\n1.959964\n-5.5630027\n5.6539427\nNot Significant\n4\n\n\n0.048865\n0.1154821\n0.05\n1.959964\n-0.6171828\n0.7149128\nNot Significant\n5\n\n\n0.103180\n0.0369259\n0.05\n1.959964\n-0.2734487\n0.4798087\nNot Significant\n6\n\n\n\n\n\n\n\n4.6.5 Presenting Results\nNow that we have estimated the MFH models and run some diagnostics, we might be interested in the following:\n\npresenting some poverty maps\n\n\nlapply(X = unique(survey_dt[[year_var]]),\n       FUN = function(x){\n         \n         shp_dt |&gt;\n           merge(model2pov_dt %&gt;% \n                   filter(!!sym(year_var) == x) |&gt;\n                   dplyr::select(!!sym(area_vars[[1]]), \"modelpov\"),\n                 by = area_vars[[1]]) |&gt;\n           ggplot() + \n           geom_sf(aes(fill = modelpov), color = NA) +\n            scale_fill_viridis(\n              name = \"Poverty Map\",\n              option = \"magma\",\n            ) +\n            theme_minimal() +\n            labs(\n              title = paste0(\"Spatial Distribution of Poverty \", x),\n              caption = \"Data source: Author Calculation\"\n            ) +\n            theme(\n              legend.position = \"bottom\",\n              axis.text = element_blank(),\n              axis.ticks = element_blank(),\n              panel.grid = element_blank()\n            )\n         \n       }) |&gt;\n  Reduce(f = \"+\")\n\n\n\n\n\n\n\n\n\nquantifying poverty change by mapping the growth rates of poverty rates for all the time periods\n\n\n### lets work with our shapefile\nlongpov_dt &lt;- \n  model2_obj$eblup |&gt;\n  mutate(!!sym(area_vars[[1]]) := 1:n()) |&gt;\n  pivot_longer(\n    cols = starts_with(\"direct_povrate\"),\n    names_to = \"year\",\n    names_pattern = \"direct_povrate(\\\\d+)\",  # Extract just the digits\n    values_to = \"modelpov\"\n  )\n\n### lets perform a regression for each area of poverty rates against year \n### and then we divide the slope variable by the average poverty rate\n\nshp_dt$growth_rate &lt;- \nlongpov_dt |&gt;\n  mutate(!!sym(year_var) := as.integer(!!sym(year_var))) |&gt;\n  group_split(!!sym(area_vars[[1]])) %&gt;%\n  lapply(X = .,\n         FUN = function(x){\n           \n           y &lt;- lm(paste0(\"modelpov ~ \", year_var[[1]]), data = x)\n           \n           y &lt;- coef(y)[2]\n           \n           delta &lt;- y / (mean(x$modelpov, na.rm = TRUE))\n           \n           return(delta)\n           \n         }) |&gt;\n  unlist() |&gt;\n  unname()\n\nLets plot this like with a heatmap on a shapefile:\n\n# Cap growth rates at 1 just to get rid of the outlier\nshp_dt$growth_rate_capped &lt;- pmin(shp_dt$growth_rate, 1)\n\nggplot(shp_dt) +\n  geom_sf(aes(fill = growth_rate_capped), color = NA) +\n  scale_fill_viridis(\n    name = \"Growth Rate (capped at 1)\",\n    limits = c(min(shp_dt$growth_rate_capped), 0.5),\n    oob = squish,\n    option = \"magma\",\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Spatial Distribution of Poverty Growth Rates\",\n    subtitle = \"Growth rates capped at 1 to reduce outlier effect\",\n    caption = \"Data source: Author Calculation\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPermatasari, Novia, Azka Ubaidillah, and Maintainer Novia Permatasari. 2022. “Package ‘Msae’.”\n\n\nYamashita, Toshie, Keizo Yamashita, and Ryotaro Kamimura. 2007. “A Stepwise AIC Method for Variable Selection in Linear Regression.” Communications in Statistics—Theory and Methods 36 (13): 2395–2403.\n\n\nYou, Yong, and Mike Hidiroglou. 2023. “Application of Sampling Variance Smoothing Methods for Small Area Proportion Estimation.” Journal of Official Statistics 39 (4): 571–90.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stable FH Estimates over T Time Periods: The Multivariate Fay Herriot Modelling Approach</span>"
    ]
  }
]